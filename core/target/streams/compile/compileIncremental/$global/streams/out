[0m[[0mdebug[0m] [0m[naha] [0m
[0m[[0mdebug[0m] [0m[naha] Initial source changes: [0m
[0m[[0mdebug[0m] [0m[naha] 	removed:Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	added: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/StatCounter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/BitSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CausedBy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingMode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ApplicationDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnvStoppedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/FileSegment.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FilterFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileRecordReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/PackedRecordPointer.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/SparkSubmitArgumentsParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadStackTrace.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingAlgorithm.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClientListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ParentClassLoader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/WorkerCommandBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryMode.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/InputFileBlockHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferInputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/SamplingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function0.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/Pseudorandom.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/HashMapGrowthStrategy.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Distribution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InternalAccumulator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkExitCode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/GroupMappingServiceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ReduceFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/TaskDetailsClassNames.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapGroupsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordPointerAndKeyPrefix.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SortDataFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocality.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/Optional.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/JVMObjectTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Command.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/EnumUtil.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/WorkerOffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorExitCode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorFailuresInTaskSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskKilledException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockNotFoundException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SplitInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/BoundedPriorityQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SerDe.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/VersionUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function4.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CollectionsUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/CoGroupFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IntParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskNotSerializableException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordComparator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Clock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutablePair.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SparkRDefaults.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/UninterruptibleThread.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function3.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachPartitionFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TooLargePageException.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/BoundedDouble.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapPartitionsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IdGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorLossReason.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ToolTips.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/PartialResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] 	modified: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated products: Set()[0m
[0m[[0mdebug[0m] [0m[naha] External API changes: API Changes: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Modified binary dependencies: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Initial directly invalidated sources: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/StatCounter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/BitSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CausedBy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingMode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ApplicationDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnvStoppedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/FileSegment.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FilterFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileRecordReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/PackedRecordPointer.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/SparkSubmitArgumentsParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadStackTrace.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingAlgorithm.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClientListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ParentClassLoader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/WorkerCommandBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryMode.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/InputFileBlockHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferInputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/SamplingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function0.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/Pseudorandom.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/HashMapGrowthStrategy.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Distribution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InternalAccumulator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkExitCode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/GroupMappingServiceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ReduceFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/TaskDetailsClassNames.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapGroupsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordPointerAndKeyPrefix.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SortDataFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocality.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/Optional.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/JVMObjectTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Command.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/EnumUtil.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/WorkerOffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorExitCode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorFailuresInTaskSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskKilledException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockNotFoundException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SplitInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/BoundedPriorityQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SerDe.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/VersionUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function4.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CollectionsUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/CoGroupFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IntParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskNotSerializableException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordComparator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Clock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutablePair.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SparkRDefaults.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/UninterruptibleThread.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function3.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachPartitionFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TooLargePageException.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/BoundedDouble.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapPartitionsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IdGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorLossReason.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ToolTips.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/PartialResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] [0m
[0m[[0mdebug[0m] [0m[naha] Sources indirectly invalidated by:[0m
[0m[[0mdebug[0m] [0m[naha] 	product: Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	binary dep: Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	external source: Set()[0m
[0m[[0mdebug[0m] [0mAll initially invalidated sources: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/StatCounter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/BitSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CausedBy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingMode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ApplicationDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnvStoppedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/FileSegment.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FilterFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileRecordReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/PackedRecordPointer.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/SparkSubmitArgumentsParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadStackTrace.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingAlgorithm.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClientListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ParentClassLoader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/WorkerCommandBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryMode.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/InputFileBlockHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferInputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/SamplingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function0.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/Pseudorandom.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/HashMapGrowthStrategy.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Distribution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InternalAccumulator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkExitCode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/GroupMappingServiceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ReduceFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/TaskDetailsClassNames.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapGroupsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordPointerAndKeyPrefix.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SortDataFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocality.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/Optional.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/JVMObjectTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Command.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/EnumUtil.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/WorkerOffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorExitCode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorFailuresInTaskSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskKilledException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockNotFoundException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SplitInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/BoundedPriorityQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SerDe.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/VersionUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function4.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CollectionsUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/CoGroupFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IntParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskNotSerializableException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordComparator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Clock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutablePair.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SparkRDefaults.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/UninterruptibleThread.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function3.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachPartitionFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TooLargePageException.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/BoundedDouble.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapPartitionsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IdGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorLossReason.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ToolTips.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/PartialResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/StatCounter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/BitSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CausedBy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingMode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ApplicationDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnvStoppedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/FileSegment.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FilterFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileRecordReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/PackedRecordPointer.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/SparkSubmitArgumentsParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadStackTrace.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingAlgorithm.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClientListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ParentClassLoader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/WorkerCommandBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryMode.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/InputFileBlockHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferInputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/SamplingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function0.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/Pseudorandom.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/HashMapGrowthStrategy.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Distribution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InternalAccumulator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkExitCode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/GroupMappingServiceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ReduceFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/TaskDetailsClassNames.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapGroupsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordPointerAndKeyPrefix.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SortDataFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocality.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/Optional.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/JVMObjectTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Command.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/EnumUtil.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/WorkerOffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorExitCode.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorFailuresInTaskSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskKilledException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockNotFoundException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SplitInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/BoundedPriorityQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SerDe.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/VersionUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function4.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CollectionsUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/CoGroupFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IntParam.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskNotSerializableException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordComparator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Clock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutablePair.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SparkRDefaults.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/UninterruptibleThread.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function3.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachPartitionFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TooLargePageException.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/BoundedDouble.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapPartitionsFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IdGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorLossReason.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorDescription.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ToolTips.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/PartialResult.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Recompiling all 567 sources: invalidated sources (567) exceeded 50.0% of all sources[0m
[0m[[0minfo[0m] [0mCompiling 490 Scala sources and 77 Java sources to /usr/local/spark-2.3.2-bin-hadoop2.7/core/target/scala-2.11/classes...[0m
[0m[[0mdebug[0m] [0mGetting org.scala-sbt:compiler-interface:0.13.16:component from component compiler for Scala 2.11.8[0m
[0m[[0mdebug[0m] [0mGetting org.scala-sbt:compiler-interface:0.13.16:component from component compiler for Scala 2.11.8[0m
[0m[[0mdebug[0m] [0mRunning cached compiler 7cd78b9f, interfacing (CompilerInterface) with Scala compiler version 2.11.8[0m
[0m[[0mdebug[0m] [0mCalling Scala compiler with arguments  (CompilerInterface):[0m
[0m[[0mdebug[0m] [0m	-unchecked[0m
[0m[[0mdebug[0m] [0m	-deprecation[0m
[0m[[0mdebug[0m] [0m	-feature[0m
[0m[[0mdebug[0m] [0m	-explaintypes[0m
[0m[[0mdebug[0m] [0m	-Yno-adapted-args[0m
[0m[[0mdebug[0m] [0m	-P:genjavadoc:out=/usr/local/spark-2.3.2-bin-hadoop2.7/core/target/java[0m
[0m[[0mdebug[0m] [0m	-P:genjavadoc:strictVisibility=true[0m
[0m[[0mdebug[0m] [0m	-Xplugin:/home/vm1/.ivy2/cache/com.typesafe.genjavadoc/genjavadoc-plugin_2.11.8/jars/genjavadoc-plugin_2.11.8-0.10.jar[0m
[0m[[0mdebug[0m] [0m	-target:jvm-1.8[0m
[0m[[0mdebug[0m] [0m	-sourcepath[0m
[0m[[0mdebug[0m] [0m	/usr/local/spark-2.3.2-bin-hadoop2.7[0m
[0m[[0mdebug[0m] [0m	-bootclasspath[0m
[0m[[0mdebug[0m] [0m	/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/classes:/home/vm1/.ivy2/cache/org.scala-lang/scala-library/jars/scala-library-2.11.8.jar[0m
[0m[[0mdebug[0m] [0m	-classpath[0m
[0m[[0mdebug[0m] [0m	/usr/local/spark-2.3.2-bin-hadoop2.7/core/target/scala-2.11/classes:/usr/local/spark-2.3.2-bin-hadoop2.7/launcher/target/scala-2.11/spark-launcher_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/tags/target/scala-2.11/spark-tags_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/kvstore/target/scala-2.11/spark-kvstore_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/network-common/target/scala-2.11/spark-network-common_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/network-shuffle/target/scala-2.11/spark-network-shuffle_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/unsafe/target/scala-2.11/spark-unsafe_2.11-2.3.2.jar:/home/vm1/.ivy2/cache/org.spark-project.spark/unused/jars/unused-1.0.0.jar:/home/vm1/.ivy2/cache/com.google.guava/guava/bundles/guava-14.0.1.jar:/home/vm1/.ivy2/cache/org.jpmml/pmml-model/jars/pmml-model-1.2.15.jar:/home/vm1/.ivy2/cache/org.jpmml/pmml-schema/jars/pmml-schema-1.2.15.jar:/home/vm1/.ivy2/cache/org.fusesource.leveldbjni/leveldbjni-all/bundles/leveldbjni-all-1.8.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.core/jackson-core/bundles/jackson-core-2.6.7.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.core/jackson-databind/bundles/jackson-databind-2.6.7.1.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.core/jackson-annotations/bundles/jackson-annotations-2.6.7.jar:/home/vm1/.ivy2/cache/io.netty/netty-all/jars/netty-all-4.1.17.Final.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-lang3/jars/commons-lang3-3.5.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-core/bundles/metrics-core-3.1.5.jar:/home/vm1/.ivy2/cache/org.slf4j/slf4j-api/jars/slf4j-api-1.7.16.jar:/home/vm1/.ivy2/cache/com.google.code.findbugs/jsr305/jars/jsr305-1.3.9.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-crypto/jars/commons-crypto-1.0.0.jar:/home/vm1/.ivy2/cache/com.twitter/chill_2.11/jars/chill_2.11-0.8.4.jar:/home/vm1/.ivy2/cache/com.twitter/chill-java/jars/chill-java-0.8.4.jar:/home/vm1/.ivy2/cache/com.esotericsoftware/kryo-shaded/bundles/kryo-shaded-3.0.3.jar:/home/vm1/.ivy2/cache/com.esotericsoftware/minlog/bundles/minlog-1.3.0.jar:/home/vm1/.ivy2/cache/org.objenesis/objenesis/jars/objenesis-2.1.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro/jars/avro-1.7.7.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-core-asl/jars/jackson-core-asl-1.9.13.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-mapper-asl/jars/jackson-mapper-asl-1.9.13.jar:/home/vm1/.ivy2/cache/org.xerial.snappy/snappy-java/bundles/snappy-java-1.1.2.6.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-compress/jars/commons-compress-1.4.1.jar:/home/vm1/.ivy2/cache/org.tukaani/xz/jars/xz-1.0.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro-mapred/jars/avro-mapred-1.7.7-hadoop2.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7-tests.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7.jar:/home/vm1/.ivy2/cache/org.apache.xbean/xbean-asm5-shaded/bundles/xbean-asm5-shaded-4.4.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-client/jars/hadoop-client-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-common/jars/hadoop-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-annotations/jars/hadoop-annotations-2.6.5.jar:/home/vm1/.ivy2/cache/commons-cli/commons-cli/jars/commons-cli-1.2.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-math3/jars/commons-math3-3.4.1.jar:/home/vm1/.ivy2/cache/xmlenc/xmlenc/jars/xmlenc-0.52.jar:/home/vm1/.ivy2/cache/commons-httpclient/commons-httpclient/jars/commons-httpclient-3.1.jar:/home/vm1/.ivy2/cache/commons-io/commons-io/jars/commons-io-2.4.jar:/home/vm1/.ivy2/cache/commons-net/commons-net/jars/commons-net-3.1.jar:/home/vm1/.ivy2/cache/commons-collections/commons-collections/jars/commons-collections-3.2.2.jar:/home/vm1/.ivy2/cache/log4j/log4j/bundles/log4j-1.2.17.jar:/home/vm1/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.6.jar:/home/vm1/.ivy2/cache/commons-configuration/commons-configuration/jars/commons-configuration-1.6.jar:/home/vm1/.ivy2/cache/commons-digester/commons-digester/jars/commons-digester-1.8.jar:/home/vm1/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:/home/vm1/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:/home/vm1/.ivy2/cache/com.google.protobuf/protobuf-java/bundles/protobuf-java-2.5.0.jar:/home/vm1/.ivy2/cache/com.google.code.gson/gson/jars/gson-2.2.4.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-auth/jars/hadoop-auth-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.directory.server/apacheds-kerberos-codec/bundles/apacheds-kerberos-codec-2.0.0-M15.jar:/home/vm1/.ivy2/cache/org.apache.directory.server/apacheds-i18n/bundles/apacheds-i18n-2.0.0-M15.jar:/home/vm1/.ivy2/cache/org.apache.directory.api/api-asn1-api/bundles/api-asn1-api-1.0.0-M20.jar:/home/vm1/.ivy2/cache/org.apache.directory.api/api-util/bundles/api-util-1.0.0-M20.jar:/home/vm1/.ivy2/cache/org.apache.curator/curator-framework/bundles/curator-framework-2.6.0.jar:/home/vm1/.ivy2/cache/org.apache.curator/curator-client/bundles/curator-client-2.6.0.jar:/home/vm1/.ivy2/cache/org.apache.zookeeper/zookeeper/jars/zookeeper-3.4.6.jar:/home/vm1/.ivy2/cache/jline/jline/jars/jline-0.9.94.jar:/home/vm1/.ivy2/cache/io.netty/netty/bundles/netty-3.9.9.Final.jar:/home/vm1/.ivy2/cache/org.apache.curator/curator-recipes/bundles/curator-recipes-2.6.0.jar:/home/vm1/.ivy2/cache/org.htrace/htrace-core/jars/htrace-core-3.0.4.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-hdfs/jars/hadoop-hdfs-2.6.5.jar:/home/vm1/.ivy2/cache/org.mortbay.jetty/jetty-util/jars/jetty-util-6.1.26.jar:/home/vm1/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.9.1.jar:/home/vm1/.ivy2/cache/xml-apis/xml-apis/jars/xml-apis-1.3.04.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-app/jars/hadoop-mapreduce-client-app-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-common/jars/hadoop-mapreduce-client-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-common/jars/hadoop-yarn-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-api/jars/hadoop-yarn-api-2.6.5.jar:/home/vm1/.ivy2/cache/javax.xml.bind/jaxb-api/jars/jaxb-api-2.2.2.jar:/home/vm1/.ivy2/cache/javax.xml.stream/stax-api/jars/stax-api-1.0-2.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-jaxrs/jars/jackson-jaxrs-1.9.13.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-xc/jars/jackson-xc-1.9.13.jar:/home/vm1/.ivy2/cache/com.google.inject/guice/jars/guice-3.0.jar:/home/vm1/.ivy2/cache/javax.inject/javax.inject/jars/javax.inject-1.jar:/home/vm1/.ivy2/cache/aopalliance/aopalliance/jars/aopalliance-1.0.jar:/home/vm1/.ivy2/cache/org.sonatype.sisu.inject/cglib/jars/cglib-2.2.1-v20090111.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-client/jars/hadoop-yarn-client-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-core/jars/hadoop-mapreduce-client-core-2.6.5.jar:/home/vm1/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.16.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-server-common/jars/hadoop-yarn-server-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-shuffle/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-jobclient/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/vm1/.ivy2/cache/org.codehaus.jettison/jettison/bundles/jettison-1.1.jar:/home/vm1/.ivy2/cache/net.java.dev.jets3t/jets3t/jars/jets3t-0.9.4.jar:/home/vm1/.ivy2/cache/org.apache.httpcomponents/httpcore/jars/httpcore-4.4.1.jar:/home/vm1/.ivy2/cache/org.apache.httpcomponents/httpclient/jars/httpclient-4.5.jar:/home/vm1/.ivy2/cache/commons-codec/commons-codec/jars/commons-codec-1.11.jar:/home/vm1/.ivy2/cache/javax.activation/activation/jars/activation-1.1.1.jar:/home/vm1/.ivy2/cache/org.bouncycastle/bcprov-jdk15on/jars/bcprov-jdk15on-1.52.jar:/home/vm1/.ivy2/cache/com.jamesmurty.utils/java-xmlbuilder/jars/java-xmlbuilder-1.1.jar:/home/vm1/.ivy2/cache/net.iharder/base64/jars/base64-2.3.8.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-plus/jars/jetty-plus-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-webapp/jars/jetty-webapp-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-xml/jars/jetty-xml-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-util/jars/jetty-util-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-servlet/jars/jetty-servlet-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-security/jars/jetty-security-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-server/jars/jetty-server-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/javax.servlet/javax.servlet-api/jars/javax.servlet-api-3.1.0.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-http/jars/jetty-http-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-io/jars/jetty-io-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-jndi/jars/jetty-jndi-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-continuation/jars/jetty-continuation-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-proxy/jars/jetty-proxy-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-client/jars/jetty-client-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-servlets/jars/jetty-servlets-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.slf4j/jul-to-slf4j/jars/jul-to-slf4j-1.7.16.jar:/home/vm1/.ivy2/cache/org.slf4j/jcl-over-slf4j/jars/jcl-over-slf4j-1.7.16.jar:/home/vm1/.ivy2/cache/com.ning/compress-lzf/bundles/compress-lzf-1.0.3.jar:/home/vm1/.ivy2/cache/org.lz4/lz4-java/jars/lz4-java-1.4.0.jar:/home/vm1/.ivy2/cache/com.github.luben/zstd-jni/bundles/zstd-jni-1.3.2-2.jar:/home/vm1/.ivy2/cache/org.roaringbitmap/RoaringBitmap/bundles/RoaringBitmap-0.5.11.jar:/home/vm1/.ivy2/cache/org.json4s/json4s-jackson_2.11/jars/json4s-jackson_2.11-3.2.11.jar:/home/vm1/.ivy2/cache/org.json4s/json4s-core_2.11/jars/json4s-core_2.11-3.2.11.jar:/home/vm1/.ivy2/cache/org.json4s/json4s-ast_2.11/jars/json4s-ast_2.11-3.2.11.jar:/home/vm1/.ivy2/cache/org.scala-lang/scalap/jars/scalap-2.11.8.jar:/home/vm1/.ivy2/cache/org.scala-lang/scala-compiler/jars/scala-compiler-2.11.8.jar:/home/vm1/.ivy2/cache/org.scala-lang/scala-reflect/jars/scala-reflect-2.11.8.jar:/home/vm1/.ivy2/cache/org.scala-lang.modules/scala-xml_2.11/bundles/scala-xml_2.11-1.0.4.jar:/home/vm1/.ivy2/cache/org.scala-lang.modules/scala-parser-combinators_2.11/bundles/scala-parser-combinators_2.11-1.0.4.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.core/jersey-client/jars/jersey-client-2.22.2.jar:/home/vm1/.ivy2/cache/javax.ws.rs/javax.ws.rs-api/jars/javax.ws.rs-api-2.0.1.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.core/jersey-common/jars/jersey-common-2.22.2.jar:/home/vm1/.ivy2/cache/javax.annotation/javax.annotation-api/jars/javax.annotation-api-1.2.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.bundles.repackaged/jersey-guava/bundles/jersey-guava-2.22.2.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/hk2-api/jars/hk2-api-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/hk2-utils/jars/hk2-utils-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2.external/aopalliance-repackaged/jars/aopalliance-repackaged-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2.external/javax.inject/jars/javax.inject-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/hk2-locator/jars/hk2-locator-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.javassist/javassist/bundles/javassist-3.18.1-GA.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/osgi-resource-locator/jars/osgi-resource-locator-1.0.1.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.core/jersey-server/jars/jersey-server-2.22.2.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.media/jersey-media-jaxb/jars/jersey-media-jaxb-2.22.2.jar:/home/vm1/.ivy2/cache/javax.validation/validation-api/jars/validation-api-1.1.0.Final.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet/jars/jersey-container-servlet-2.22.2.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet-core/jars/jersey-container-servlet-core-2.22.2.jar:/home/vm1/.ivy2/cache/com.clearspring.analytics/stream/jars/stream-2.7.0.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-jvm/bundles/metrics-jvm-3.1.5.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-json/bundles/metrics-json-3.1.5.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-graphite/bundles/metrics-graphite-3.1.5.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-scala_2.11/bundles/jackson-module-scala_2.11-2.6.7.1.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-paranamer/bundles/jackson-module-paranamer-2.7.9.jar:/home/vm1/.ivy2/cache/com.thoughtworks.paranamer/paranamer/bundles/paranamer-2.8.jar:/home/vm1/.ivy2/cache/org.apache.ivy/ivy/jars/ivy-2.4.0.jar:/home/vm1/.ivy2/cache/oro/oro/jars/oro-2.0.8.jar:/home/vm1/.ivy2/cache/net.razorvine/pyrolite/jars/pyrolite-4.13.jar:/home/vm1/.ivy2/cache/net.sf.py4j/py4j/jars/py4j-0.10.7.jar:/home/vm1/.ivy2/cache/org.spark-project.hive/hive-exec/jars/hive-exec-1.2.1.spark2.jar:/home/vm1/.ivy2/cache/com.twitter/parquet-hadoop-bundle/jars/parquet-hadoop-bundle-1.6.0.jar:/home/vm1/.ivy2/cache/javolution/javolution/bundles/javolution-5.5.1.jar:/home/vm1/.ivy2/cache/log4j/apache-log4j-extras/bundles/apache-log4j-extras-1.2.17.jar:/home/vm1/.ivy2/cache/org.antlr/antlr-runtime/jars/antlr-runtime-3.4.jar:/home/vm1/.ivy2/cache/org.antlr/stringtemplate/jars/stringtemplate-3.2.1.jar:/home/vm1/.ivy2/cache/antlr/antlr/jars/antlr-2.7.7.jar:/home/vm1/.ivy2/cache/org.antlr/ST4/jars/ST4-4.0.4.jar:/home/vm1/.ivy2/cache/org.jodd/jodd-core/jars/jodd-core-3.5.2.jar:/home/vm1/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar:/home/vm1/.ivy2/cache/org.apache.calcite/calcite-avatica/jars/calcite-avatica-1.2.0-incubating.jar:/home/vm1/.ivy2/cache/com.googlecode.javaewah/JavaEWAH/jars/JavaEWAH-0.3.2.jar:/home/vm1/.ivy2/cache/org.iq80.snappy/snappy/jars/snappy-0.2.jar:/home/vm1/.ivy2/cache/stax/stax-api/jars/stax-api-1.0.1.jar:/home/vm1/.ivy2/cache/net.sf.opencsv/opencsv/jars/opencsv-2.3.jar:/home/vm1/.ivy2/cache/org.spark-project.hive/hive-metastore/jars/hive-metastore-1.2.1.spark2.jar:/home/vm1/.ivy2/cache/com.jolbox/bonecp/bundles/bonecp-0.8.0.RELEASE.jar:/home/vm1/.ivy2/cache/commons-logging/commons-logging/jars/commons-logging-1.1.3.jar:/home/vm1/.ivy2/cache/org.apache.derby/derby/jars/derby-10.10.2.0.jar:/home/vm1/.ivy2/cache/org.datanucleus/datanucleus-api-jdo/jars/datanucleus-api-jdo-3.2.6.jar:/home/vm1/.ivy2/cache/org.datanucleus/datanucleus-rdbms/jars/datanucleus-rdbms-3.2.9.jar:/home/vm1/.ivy2/cache/commons-pool/commons-pool/jars/commons-pool-1.5.4.jar:/home/vm1/.ivy2/cache/commons-dbcp/commons-dbcp/jars/commons-dbcp-1.4.jar:/home/vm1/.ivy2/cache/javax.jdo/jdo-api/jars/jdo-api-3.0.1.jar:/home/vm1/.ivy2/cache/javax.transaction/jta/jars/jta-1.1.jar:/home/vm1/.ivy2/cache/org.apache.thrift/libthrift/jars/libthrift-0.9.3.jar:/home/vm1/.ivy2/cache/org.apache.thrift/libfb303/jars/libfb303-0.9.3.jar[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala:1725: method getExecutorStorageStatus in class SparkContext is deprecated: This method may change or be removed in a future release.[0m
[0m[[33mwarn[0m] [0m    StorageUtils.updateRddInfo(rddInfos, getExecutorStorageStatus)[0m
[0m[[33mwarn[0m] [0m                                         ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala:99: method group in class AbstractBootstrap is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    if (bootstrap != null && bootstrap.group() != null) {[0m
[0m[[33mwarn[0m] [0m                                       ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala:100: method group in class AbstractBootstrap is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      bootstrap.group().shutdownGracefully()[0m
[0m[[33mwarn[0m] [0m                ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala:102: method childGroup in class ServerBootstrap is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m    if (bootstrap != null && bootstrap.childGroup() != null) {[0m
[0m[[33mwarn[0m] [0m                                       ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala:103: method childGroup in class ServerBootstrap is deprecated: see corresponding Javadoc for more information.[0m
[0m[[33mwarn[0m] [0m      bootstrap.childGroup().shutdownGracefully()[0m
[0m[[33mwarn[0m] [0m                ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala:59: value attemptId in class StageInfo is deprecated: Use attemptNumber instead[0m
[0m[[33mwarn[0m] [0m  def attemptNumber(): Int = attemptId[0m
[0m[[33mwarn[0m] [0m                             ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala:169: class StorageStatus in package storage is deprecated: This class may be removed or made private in a future release.[0m
[0m[[33mwarn[0m] [0m  def getStorageStatus: Array[StorageStatus] = {[0m
[0m[[33mwarn[0m] [0m                        ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala:170: class StorageStatus in package storage is deprecated: This class may be removed or made private in a future release.[0m
[0m[[33mwarn[0m] [0m    driverEndpoint.askSync[Array[StorageStatus]](GetStorageStatus)[0m
[0m[[33mwarn[0m] [0m                           ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala:292: class StorageStatus in package storage is deprecated: This class may be removed or made private in a future release.[0m
[0m[[33mwarn[0m] [0m  private def storageStatus: Array[StorageStatus] = {[0m
[0m[[33mwarn[0m] [0m                             ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala:294: class StorageStatus in package storage is deprecated: This class may be removed or made private in a future release.[0m
[0m[[33mwarn[0m] [0m      new StorageStatus(blockManagerId, info.maxMem, Some(info.maxOnHeapMem),[0m
[0m[[33mwarn[0m] [0m          ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:303: class StorageStatus in package storage is deprecated: This class may be removed or made private in a future release.[0m
[0m[[33mwarn[0m] [0m  def updateRddInfo(rddInfos: Seq[RDDInfo], statuses: Seq[StorageStatus]): Unit = {[0m
[0m[[33mwarn[0m] [0m                                                      ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala:323: class StorageStatus in package storage is deprecated: This class may be removed or made private in a future release.[0m
[0m[[33mwarn[0m] [0m  def getRddBlockLocations(rddId: Int, statuses: Seq[StorageStatus]): Map[BlockId, Seq[String]] = {[0m
[0m[[33mwarn[0m] [0m                                                 ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala:492: trait AccumulableParam in package spark is deprecated: use AccumulatorV2[0m
[0m[[33mwarn[0m] [0m    param: org.apache.spark.AccumulableParam[R, T]) extends AccumulatorV2[T, R] {[0m
[0m[[33mwarn[0m] [0m                            ^[0m
[0m[[33mwarn[0m] [0m13 warnings found[0m
[0m[[0mdebug[0m] [0mScala compilation took 81.835362465 s[0m
[0m[[0mdebug[0m] [0mJava compilation took 2.283686081 s[0m
[0m[[0mdebug[0m] [0mJava analysis took 0.331109466 s[0m
[0m[[0mdebug[0m] [0mJava compile + analysis took 2.680407192 s[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, execId, wait, $asInstanceOf, state, equals, cores, ExecutorRunner, asInstanceOf, initializeLogIfNecessary, host, publicAddress, synchronized, $isInstanceOf, webUiPort, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, substituteVariables, <init>, ==, executorDir, clone, $init$, workerId, toString, logError, !=, getClass, appLocalDirs, sparkHome, logWarning, appId, start, kill, ne, eq, workerUrl, log, ##, finalize, appDesc, hashCode, logDebug, logInfo, worker, memory.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(execId, state, cores, ExecutorRunner, asInstanceOf, host, isInstanceOf, <init>, ==, workerId, toString, appId, eq, appDesc, worker, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(execId, state, cores, ExecutorRunner, asInstanceOf, host, publicAddress, synchronized, webUiPort, isInstanceOf, <init>, ==, executorDir, workerId, toString, logError, !=, appLocalDirs, sparkHome, logWarning, appId, start, kill, ne, log, appDesc, logDebug, logInfo, worker, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(execId, state, cores, ExecutorRunner, host, <init>, ==, workerId, toString, appId, appDesc, worker, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala: Set(execId, state, cores, ExecutorRunner, host, <init>, workerId, toString, appId, appDesc, worker, memory)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, delegationTokensRequired, wait, $asInstanceOf, equals, HBaseDelegationTokenProvider, asInstanceOf, initializeLogIfNecessary, serviceName, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, obtainDelegationTokens, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(delegationTokensRequired, HBaseDelegationTokenProvider, serviceName, <init>, obtainDelegationTokens, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, SparkFiles, equals, asInstanceOf, synchronized, getRootDirectory, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, get, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(SparkFiles, asInstanceOf, synchronized, getRootDirectory, isInstanceOf, ==, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, SparkFiles, asInstanceOf, synchronized, getRootDirectory, notifyAll, isInstanceOf, ==, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(SparkFiles, asInstanceOf, getRootDirectory, isInstanceOf, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(SparkFiles, asInstanceOf, synchronized, getRootDirectory, isInstanceOf, ==, clone, toString, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, BlockReplicationPolicy, wait, $asInstanceOf, RandomBlockReplicationPolicy, equals, asInstanceOf, initializeLogIfNecessary, prioritize, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, BasicBlockReplicationPolicy, BlockReplicationUtils, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo, getRandomSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(BlockReplicationPolicy, asInstanceOf, prioritize, synchronized, logTrace, isInstanceOf, <init>, ==, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, used, notify, find, span, flatten, toBuffer, count, reduceOption, sliceWithKnownDelta, wait, <init>$default$5, foldRight, takeWhile, stringPrefix, $asInstanceOf, splitAt, <init>$default$6, minBy, size, inits, zip, toSet, :\, insert, map, takeRight, dropWhile, toMap, filterNot, equals, taskMemoryManager, par, unzip3, repr, toList, isTraversableAgain, head, asInstanceOf, sameElements, initializeLogIfNecessary, unzip, reduceLeftOption, synchronized, sliding, diskBytesSpilled, Self, allocateArray, partition, aggregate, <init>$default$7, $isInstanceOf, forall, newBuilder, <init>$default$4, mkString, min, scanRight, fold, logTrace, scan, nonEmpty, canEqual, tail, isTraceEnabled, initializeLogIfNecessary$default$2, lastOption, dropRight, iterator, last, maybeSpill, logName, notifyAll, /:, toIterator, allocatePage, addString, releaseMemory, to, insertAll, collectFirst, drop, isInstanceOf, filter, freePage, ++:, <init>, toStream, companion, max, tails, ++, grouped, flatMap, take, elementsRead, parCombiner, reduceRight, groupBy, ==, maxBy, sliceWithKnownBound, clone, memoryBytesSpilled, slice, foreach, destructiveIterator, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, ExternalAppendOnlyMap, reduce, acquireMemory, $init$, toSeq, zipWithIndex, freeArray, _spillCount, toString, genericBuilder, copyToArray, seq, logError, !=, freeMemory, transpose, spill, collect, headOption, getClass, addElementsRead, logWarning, WithFilter, hasDefiniteSize, foldLeft, toCollection, isEmpty, getMode, ne, protected$getUsed, init, reversed, reduceLeft, eq, forceSpill, sum, log, peakMemoryUsedBytes, thisCollection, ##, scanLeft, finalize, hashCode, getUsed, zipAll, logDebug, numSpills, product, view, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(map, asInstanceOf, diskBytesSpilled, iterator, insertAll, isInstanceOf, <init>, ==, memoryBytesSpilled, foreach, ExternalAppendOnlyMap, toString, eq, peakMemoryUsedBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(withFilter, map, equals, head, asInstanceOf, diskBytesSpilled, iterator, insertAll, isInstanceOf, <init>, ==, memoryBytesSpilled, foreach, toArray, ExternalAppendOnlyMap, zipWithIndex, toString, ne, eq, peakMemoryUsedBytes, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	NUM_PARTITIONS, notify, wait, $asInstanceOf, INTER_JOB_WAIT_MS, equals, asInstanceOf, synchronized, $isInstanceOf, main, UIWorkloadGenerator, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, prefix, asInstanceOf, synchronized, $isInstanceOf, HistoryPage, notifyAll, isInstanceOf, <init>, ==, clone, renderJson, toString, !=, getClass, render, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, HistoryPage, isInstanceOf, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, getBlockData, equals, asInstanceOf, synchronized, $isInstanceOf, BlockDataManager, notifyAll, isInstanceOf, ==, clone, toString, !=, putBlockData, getClass, releaseLock, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(BlockDataManager, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(getBlockData, asInstanceOf, synchronized, BlockDataManager, isInstanceOf, ==, toString, !=, getClass, releaseLock, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala: Set(asInstanceOf, BlockDataManager, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala: Set(getBlockData, asInstanceOf, BlockDataManager, isInstanceOf, putBlockData, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, !=, releaseLock, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(!=)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, ==, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(getBlockData, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(getBlockData, asInstanceOf, synchronized, BlockDataManager, isInstanceOf, ==, toString, !=, getClass, releaseLock, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, DEFAULT_POOL_NAME, MINIMUM_SHARES_PROPERTY, DEFAULT_WEIGHT, FairSchedulableBuilder, wait, $asInstanceOf, equals, WEIGHT_PROPERTY, addTaskSetManager, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, SCHEDULING_MODE_PROPERTY, DEFAULT_MINIMUM_SHARE, logTrace, isTraceEnabled, DEFAULT_SCHEDULER_FILE, initializeLogIfNecessary$default$2, rootPool, schedulerAllocFile, logName, notifyAll, isInstanceOf, <init>, SchedulableBuilder, ==, clone, buildPools, $init$, toString, SCHEDULER_ALLOCATION_FILE_PROPERTY, logError, !=, getClass, DEFAULT_SCHEDULING_MODE, logWarning, ne, POOL_NAME_PROPERTY, FIFOSchedulableBuilder, POOLS_PROPERTY, eq, log, ##, finalize, hashCode, logDebug, FAIR_SCHEDULER_PROPERTIES, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(FairSchedulableBuilder, wait, addTaskSetManager, asInstanceOf, synchronized, rootPool, isInstanceOf, <init>, SchedulableBuilder, ==, buildPools, toString, logError, !=, logWarning, ne, FIFOSchedulableBuilder, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/WorkerOffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/WorkerOffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/WorkerOffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/WorkerOffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/WorkerOffer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, copy$default$2, $asInstanceOf, productArity, equals, cores, asInstanceOf, host, synchronized, $isInstanceOf, canEqual, executorId, productPrefix, notifyAll, WorkerOffer, isInstanceOf, <init>, ==, clone, $init$, copy$default$3, copy, toString, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(cores, asInstanceOf, host, synchronized, executorId, WorkerOffer, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, cores, asInstanceOf, host, synchronized, executorId, WorkerOffer, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, WorkerOffer, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onExecutorUnblacklisted, notify, redactEvent, initEventLog, IN_PROGRESS, getLogPath, wait, DEFAULT_LOG_DIR, $asInstanceOf, onJobEnd, onApplicationEnd, equals, onTaskEnd, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, initializeLogIfNecessary, synchronized, openEventLog, $isInstanceOf, loggedEvents, logTrace, onOtherEvent, isTraceEnabled, initializeLogIfNecessary$default$2, onTaskStart, stop, logName, notifyAll, onExecutorRemoved, onExecutorAdded, isInstanceOf, <init>, onBlockUpdated, ==, onBlockManagerRemoved, clone, onExecutorMetricsUpdate, onEnvironmentUpdate, onBlockManagerAdded, $init$, onNodeBlacklisted, onApplicationStart, onSpeculativeTaskSubmitted, toString, EventLoggingListener, logError, !=, getClass, codecName, logWarning, start, onExecutorBlacklisted, onUnpersistRDD, ne, onStageSubmitted, eq, onNodeUnblacklisted, log, logPath, getLogPath$default$4, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(IN_PROGRESS, DEFAULT_LOG_DIR, asInstanceOf, synchronized, openEventLog, isInstanceOf, <init>, ==, clone, toString, EventLoggingListener, logError, !=, getClass, codecName, logWarning, start, ne, eq, log, logPath, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(DEFAULT_LOG_DIR, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, clone, toString, EventLoggingListener, logError, !=, getClass, logWarning, start, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, receiverBlockTables, streamBlockStorageLevelDescriptionAndSize, prefix, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, StoragePage, ==, clone, renderJson, toString, !=, getClass, rddTable, render, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala: Set(<init>, StoragePage)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, getConf, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, mapPartitionsWithInputSplit$default$2, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, jobId, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, WholeTextFileRDD, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, hashCode, takeOrdered, logDebug, mapPartitionsWithInputSplit, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, getConf, parent, partitioner, name, setName, union, map, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, <init>, id, max, toDebugString, ++, flatMap, ==, clone, foreach, jobId, zipWithIndex, first, WholeTextFileRDD, toString, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SortDataFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SortDataFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SortDataFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SortDataFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SortDataFormat.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, KVArraySortDataFormat, asInstanceOf, synchronized, $isInstanceOf, getKey, newKey, notifyAll, isInstanceOf, allocate, <init>, ==, SortDataFormat, clone, swap, toString, copyElement, !=, getClass, ne, eq, ##, finalize, hashCode, copyRange.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala: Set(<init>, SortDataFormat)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala: Set(equals, KVArraySortDataFormat, asInstanceOf, <init>, ==, SortDataFormat, !=, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala: Set(KVArraySortDataFormat, asInstanceOf, <init>, ==, SortDataFormat)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, size, equals, asInstanceOf, chunks, synchronized, toChunkedByteBuffer, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, ChunkedByteBufferOutputStream, flush, position, toString, !=, getClass, close, ne, eq, write, ##, finalize, hashCode, lastChunkIndex.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asInstanceOf, toChunkedByteBuffer, isInstanceOf, <init>, ==, ChunkedByteBufferOutputStream, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(size, asInstanceOf, synchronized, toChunkedByteBuffer, isInstanceOf, <init>, ==, ChunkedByteBufferOutputStream, toString, !=, close, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, chunks, synchronized, toChunkedByteBuffer, isInstanceOf, <init>, ==, ChunkedByteBufferOutputStream, position, !=, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(size, asInstanceOf, synchronized, toChunkedByteBuffer, isInstanceOf, <init>, ==, ChunkedByteBufferOutputStream, flush, toString, !=, close, eq, write)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getEpoch, post, removeOutputsOnHost, askTracker, wait, onNetworkError, copy$default$2, $asInstanceOf, removeOutputsOnExecutor, epoch, MapOutputTrackerMasterEndpoint, epochLock, productArity, equals, StopMapOutputTracker, incrementEpoch, asInstanceOf, shuffleId, context, initializeLogIfNecessary, synchronized, self, registerMapOutput, $isInstanceOf, sendTracker, receive, trackerEndpoint, unregisterShuffle, logTrace, canEqual, convertMapStatuses, isTraceEnabled, deserializeMapStatuses, initializeLogIfNecessary$default$2, productPrefix, MapOutputTrackerMaster, stop, shuffleStatuses, logName, notifyAll, containsShuffle, getNumCachedSerializedBroadcast, isInstanceOf, MapOutputTrackerMessage, findMissingPartitions, updateEpoch, MapOutputTracker, <init>, onError, ENDPOINT_NAME, ==, receiveAndReply, clone, equallyDivide, getStatistics, MapOutputTrackerWorker, getNumAvailableOutputs, $init$, rangeGrouped, onDisconnected, copy, GetMapOutputStatuses, registerShuffle, toString, unregisterMapOutput, logError, !=, onConnected, getClass, logWarning, copy$default$1, onStop, serializeMapStatuses, ne, onStart, rpcEnv, getMapSizesByExecutorId, mapStatuses, eq, productIterator, log, GetMapOutputMessage, ##, finalize, getLocationsWithLargestOutputs, productElement, hashCode, getPreferredLocationsForShuffle, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(getEpoch, post, removeOutputsOnHost, removeOutputsOnExecutor, epoch, incrementEpoch, asInstanceOf, shuffleId, synchronized, registerMapOutput, logTrace, MapOutputTrackerMaster, stop, containsShuffle, isInstanceOf, findMissingPartitions, MapOutputTracker, <init>, ==, clone, getStatistics, registerShuffle, toString, unregisterMapOutput, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(getEpoch, epoch, asInstanceOf, synchronized, MapOutputTrackerMaster, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(MapOutputTrackerMasterEndpoint, asInstanceOf, synchronized, MapOutputTrackerMaster, stop, isInstanceOf, MapOutputTracker, <init>, ENDPOINT_NAME, ==, MapOutputTrackerWorker, toString, !=, getClass, logWarning, ne, rpcEnv, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, shuffleId, context, stop, isInstanceOf, MapOutputTracker, <init>, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, stop, isInstanceOf, MapOutputTracker, <init>, ==, copy, toString, logError, !=, getClass, logWarning, ne, rpcEnv, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, shuffleId, MapOutputTrackerMaster, findMissingPartitions, <init>, ==, getNumAvailableOutputs, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, MapOutputTrackerMaster, stop, isInstanceOf, MapOutputTracker, <init>, ==, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, epoch, asInstanceOf, context, synchronized, stop, notifyAll, isInstanceOf, updateEpoch, MapOutputTracker, <init>, ENDPOINT_NAME, ==, MapOutputTrackerWorker, logError, !=, logWarning, ne, rpcEnv, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, shuffleId, context, stop, isInstanceOf, MapOutputTracker, <init>, ==, ne, getMapSizesByExecutorId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, shuffleId, synchronized, unregisterShuffle, MapOutputTrackerMaster, isInstanceOf, MapOutputTracker, <init>, ==, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, shuffleId, context, unregisterShuffle, isInstanceOf, MapOutputTracker, <init>, ==, logError, !=, rpcEnv, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf, context, MapOutputTrackerMaster, MapOutputTracker, <init>, getPreferredLocationsForShuffle)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, dagScheduler, equals, DAGSchedulerSource, asInstanceOf, metricRegistry, synchronized, $isInstanceOf, notifyAll, sourceName, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne, messageProcessingTimer, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(dagScheduler, DAGSchedulerSource, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, ne, messageProcessingTimer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(dagScheduler, DAGSchedulerSource, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, shuffleId, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, bytesByPartitionId, ==, clone, MapOutputStatistics, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, shuffleId, synchronized, isInstanceOf, <init>, ==, clone, MapOutputStatistics, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, shuffleId, synchronized, notifyAll, isInstanceOf, <init>, ==, MapOutputStatistics, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, MapOutputStatistics, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, serverInfo, name, webUrl, wait, $asInstanceOf, detachHandler, WebUIPage, addStaticHandler, equals, prefix, getBasePath, asInstanceOf, detachTab, initializeLogIfNecessary, synchronized, bind, $isInstanceOf, pages, attachPage, logTrace, attachHandler, publicHostName, isTraceEnabled, initializeLogIfNecessary$default$2, handlers, basePath, stop, tabs, logName, notifyAll, initialize, isInstanceOf, attachTab, getSecurityManager, <init>, getTabs, pageToHandlers, ==, clone, securityManager, $init$, renderJson, removeStaticHandler, WebUITab, toString, logError, !=, getHandlers, getClass, logWarning, render, detachPage, headerTabs, ne, eq, log, WebUI, boundPort, ##, finalize, hashCode, logDebug, sslOptions, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(WebUIPage, attachPage, attachHandler, initialize, <init>, WebUI)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(webUrl, prefix, attachHandler, basePath, stop, initialize, attachTab, <init>, ==, securityManager, WebUITab, WebUI, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(WebUIPage, attachPage, attachHandler, initialize, <init>, ==, WebUI)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(WebUIPage, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(name, prefix, asInstanceOf, synchronized, bind, attachHandler, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, log, boundPort, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(name, webUrl, WebUIPage, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(name, asInstanceOf, synchronized, initialize, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(WebUIPage, <init>, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(name, WebUIPage, <init>, ==, render)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(name, WebUIPage, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(name, asInstanceOf, bind, attachHandler, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, log, boundPort, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(WebUIPage, attachPage, <init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala: Set(WebUIPage, attachPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(webUrl, prefix, attachHandler, basePath, stop, initialize, attachTab, <init>, ==, securityManager, WebUITab, WebUI, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(webUrl, asInstanceOf, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala: Set(WebUIPage, attachPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(name, webUrl, asInstanceOf, synchronized, bind, attachHandler, stop, initialize, isInstanceOf, <init>, ==, clone, securityManager, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(WebUIPage, attachPage, attachHandler, initialize, <init>, WebUI)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(serverInfo, detachHandler, WebUIPage, asInstanceOf, synchronized, bind, attachPage, attachHandler, handlers, stop, initialize, isInstanceOf, <init>, securityManager, toString, getHandlers, log, WebUI, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(WebUIPage, attachPage, attachHandler, initialize, <init>, ==, WebUI)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(name, prefix, asInstanceOf, basePath, isInstanceOf, <init>, ==, WebUITab, toString, logError, !=, getClass, headerTabs, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(WebUIPage, basePath, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala: Set(WebUIPage, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala: Set(WebUIPage, attachPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala: Set(isInstanceOf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(name, <init>, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, securityManager, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(name, equals, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(name, WebUIPage, basePath, <init>, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(name, WebUIPage, asInstanceOf, basePath, logName, isInstanceOf, <init>, ==, toString, !=, render, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala: Set(WebUIPage, attachPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(name, WebUIPage, asInstanceOf, basePath, isInstanceOf, <init>, ==, toString, !=, render, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(name, WebUIPage, basePath, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(name, prefix, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getHandlers, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(name, asInstanceOf, synchronized, initialize, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(WebUIPage, basePath, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(name, WebUIPage, asInstanceOf, basePath, isInstanceOf, <init>, ==, toString, !=, render)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(WebUIPage, attachPage, <init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(WebUIPage, attachPage, <init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(name, webUrl, WebUIPage, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(webUrl, asInstanceOf, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala: Set(WebUIPage, attachPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(name, WebUIPage, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(name, webUrl, asInstanceOf, synchronized, bind, attachHandler, stop, initialize, isInstanceOf, <init>, ==, clone, securityManager, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(WebUIPage, attachPage, attachHandler, initialize, <init>, WebUI)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(serverInfo, detachHandler, WebUIPage, asInstanceOf, synchronized, bind, attachPage, attachHandler, handlers, stop, initialize, isInstanceOf, <init>, securityManager, toString, getHandlers, log, WebUI, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(WebUIPage, attachPage, attachHandler, initialize, <init>, ==, WebUI)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(name, prefix, asInstanceOf, basePath, isInstanceOf, <init>, ==, WebUITab, toString, logError, !=, getClass, headerTabs, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(name, prefix, asInstanceOf, synchronized, bind, attachHandler, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, log, boundPort, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(WebUIPage, basePath, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala: Set(WebUIPage, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(name, basePath, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala: Set(WebUIPage, attachPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(webUrl, prefix, attachHandler, basePath, stop, initialize, attachTab, <init>, ==, securityManager, WebUITab, WebUI, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, securityManager, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(name, WebUIPage, basePath, <init>, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(name, WebUIPage, asInstanceOf, basePath, logName, isInstanceOf, <init>, ==, toString, !=, render, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala: Set(WebUIPage, attachPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(WebUIPage, <init>, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(name, asInstanceOf, bind, attachHandler, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, log, boundPort, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(name, WebUIPage, asInstanceOf, basePath, isInstanceOf, <init>, ==, toString, !=, render, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(name, WebUIPage, basePath, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(name, prefix, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getHandlers, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(WebUIPage, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(WebUIPage, basePath, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(name, WebUIPage, asInstanceOf, basePath, isInstanceOf, <init>, ==, toString, !=, render)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(WebUIPage, attachPage, <init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(WebUIPage, attachPage, <init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(name, WebUIPage, <init>, ==, render)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala: Set(WebUIPage, attachPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(WebUIPage, basePath, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(WebUIPage, attachPage, <init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(logName, <init>, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(webUrl, prefix, attachHandler, basePath, stop, initialize, attachTab, <init>, ==, securityManager, WebUITab, WebUI, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(serverInfo, detachHandler, WebUIPage, asInstanceOf, synchronized, bind, attachPage, attachHandler, handlers, stop, initialize, isInstanceOf, <init>, securityManager, toString, getHandlers, log, WebUI, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala: Set(WebUIPage, attachPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(WebUIPage, attachPage, <init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(WebUIPage, attachPage, attachHandler, initialize, <init>, WebUI)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(WebUIPage, attachPage, <init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(WebUIPage, attachPage, <init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(WebUIPage, basePath, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(name, WebUIPage, asInstanceOf, basePath, isInstanceOf, <init>, ==, toString, !=, render)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(webUrl, prefix, attachHandler, basePath, stop, initialize, attachTab, <init>, ==, securityManager, WebUITab, WebUI, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(name, basePath, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(webUrl, prefix, attachHandler, basePath, stop, initialize, attachTab, <init>, ==, securityManager, WebUITab, WebUI, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(name, WebUIPage, asInstanceOf, basePath, logName, isInstanceOf, <init>, ==, toString, !=, render, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(name, WebUIPage, basePath, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(WebUIPage, basePath, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(WebUIPage, attachPage, attachHandler, initialize, <init>, ==, WebUI)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, PagedDataSource, prevPageSizeFormField, wait, pageLink, copy$default$2, $asInstanceOf, pageSize, pageSizeFormField, productArity, pageNumberFormField, equals, asInstanceOf, data, synchronized, pageData, $isInstanceOf, canEqual, row, PagedTable, tableCssClass, dataSource, productPrefix, notifyAll, goButtonFormPath, isInstanceOf, <init>, ==, sliceData, clone, $init$, tableId, copy, toString, !=, totalPage, getClass, copy$default$1, headers, pageNavigation, PageData, ne, dataSize, eq, productIterator, ##, finalize, table, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(<init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala: Set(PagedDataSource, pageSize, pageSizeFormField, pageNumberFormField, asInstanceOf, data, PagedTable, dataSource, isInstanceOf, <init>, ==, !=, ne, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(PagedDataSource, pageSize, pageSizeFormField, pageNumberFormField, data, PagedTable, dataSource, isInstanceOf, <init>, ==, toString, !=, ne, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(PagedDataSource, pageSize, pageSizeFormField, pageNumberFormField, asInstanceOf, data, PagedTable, dataSource, isInstanceOf, <init>, ==, toString, !=, ne, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(PagedDataSource, pageSize, pageSizeFormField, pageNumberFormField, asInstanceOf, data, PagedTable, dataSource, isInstanceOf, <init>, ==, toString, !=, eq, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(PagedDataSource, pageSize, pageSizeFormField, pageNumberFormField, data, PagedTable, dataSource, isInstanceOf, <init>, ==, toString, !=, ne, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(<init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(<init>, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, localFraction, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, copy$default$2, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, CoalescedRDDPartition, zip, localCheckpoint, map, productArity, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, parents, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, prev, min, getCheckpointFile, CoalescedRDD, fold, getOutputDeterministicLevel, logTrace, canEqual, treeAggregate$default$4, copy$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, parentsIndices, productPrefix, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, rdd, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, preferredLocation, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, copy$default$3, sample, copy, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, copy$default$1, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, productIterator, withScope, log, ##, finalize, treeAggregate, index, productElement, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(creationSite, zipPartitions, parent, isLocallyCheckpointed, getOrCompute, partitioner, coalesce, name, count, isCheckpointedAndMaterialized, mapPartitions, union, map, subtract, intersection, scope, asInstanceOf, context, getPreferredLocations, doCheckpoint, synchronized, aggregate, compute, min, CoalescedRDD, fold, getOutputDeterministicLevel, iterator, conf, isInstanceOf, filter, rdd, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, ++, flatMap, take, groupBy, ==, clone, distinct, retag, foreach, reduce, zipWithIndex, checkpoint, elementClassTag, sample, toString, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, clearDependencies, isEmpty, ne, getDependencies, mapPartitionsWithIndex, keyBy, getCreationSite, computeOrReadCheckpoint, dependencies, isReliablyCheckpointed, withScope, log, treeAggregate, index, takeOrdered, firstParent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createJettySslContextFactory, notify, trustStorePassword, apply$default$11, unapply, wait, apply$default$4, <init>$default$5, copy$default$2, $asInstanceOf, <init>$default$6, copy$default$5, apply$default$12, apply$default$8, <init>$default$1, productArity, equals, keyPassword, copy$default$9, copy$default$12, trustStore, asInstanceOf, initializeLogIfNecessary, apply$default$7, keyStoreType, trustStoreType, enabled, synchronized, <init>$default$7, $isInstanceOf, apply$default$3, <init>$default$4, copy$default$8, logTrace, canEqual, copy$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, <init>$default$11, keyStore, logName, notifyAll, <init>$default$10, isInstanceOf, apply$default$6, needClientAuth, <init>$default$3, <init>$default$8, <init>, port, apply$default$2, apply, parse, ==, clone, SSLOptions, parse$default$3, <init>$default$12, copy$default$7, copy$default$10, keyStorePassword, $init$, apply$default$10, copy$default$3, copy, apply$default$1, toString, logError, !=, <init>$default$9, apply$default$5, getClass, logWarning, copy$default$1, apply$default$9, copy$default$6, ne, enabledAlgorithms, copy$default$11, <init>$default$2, protocol, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(<init>, SSLOptions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(unapply, asInstanceOf, synchronized, isInstanceOf, <init>, port, apply, SSLOptions, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(<init>, apply, ==, SSLOptions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala: Set(trustStorePassword, trustStore, enabled, <init>, apply, parse, ==, SSLOptions, toString, !=, logWarning, ne, protocol, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(<init>, apply, ==, SSLOptions, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(createJettySslContextFactory, asInstanceOf, isInstanceOf, <init>, port, apply, ==, SSLOptions, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(<init>, port, apply, ==, SSLOptions, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getChunk, notify, streamSent, addFile, registerChannel, wait, $asInstanceOf, addJar, equals, checkAuthorization, asInstanceOf, connectionTerminated, NettyStreamManager, synchronized, streamBeingSent, $isInstanceOf, notifyAll, isInstanceOf, <init>, chunkSent, validateDirectoryUri, openStream, ==, clone, $init$, toString, !=, getClass, chunksBeingTransferred, ne, chunkBeingSent, eq, addDirectory, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, NettyStreamManager, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	listStatus, notify, getConf, isSplitable, wait, $asInstanceOf, setMinSplitSizeRack, equals, asInstanceOf, synchronized, createPool, createSplits, $isInstanceOf, getFormatMinSplitSize, setMaxSplitSize, getSplits, setMinPartitions, getBlockIndex, setMinSplitSizeNode, notifyAll, computeSplitSize, isInstanceOf, addInputPathRecursively, <init>, ==, createRecordReader, makeSplit, clone, getFileBlockLocations, $init$, toString, WholeTextFileInputFormat, !=, setConf, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(getConf, asInstanceOf, getSplits, setMinPartitions, <init>, toString, WholeTextFileInputFormat, setConf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(getConf, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, WholeTextFileInputFormat, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	appSparkVersion, notify, DEFAULT_POOL_NAME, serverInfo, name, webUrl, DEFAULT_PORT, wait, startTime, $asInstanceOf, detachHandler, addStaticHandler, setAppId, equals, appName, prefix, getBasePath, asInstanceOf, detachTab, initializeLogIfNecessary, killEnabled, getApplicationInfoList, synchronized, withSparkUI, sc, bind, $isInstanceOf, pages, create, getAppName, store, attachPage, logTrace, attachHandler, publicHostName, isTraceEnabled, initializeLogIfNecessary$default$2, handlers, basePath, stop, tabs, logName, notifyAll, conf, getUIPort, initialize, isInstanceOf, attachTab, getSecurityManager, <init>, STATIC_RESOURCE_DIR, SparkUI, getTabs, getApplicationInfo, SparkUITab, pageToHandlers, ==, setStreamingJobProgressListener, clone, writeEventLogs, create$default$8, securityManager, $init$, removeStaticHandler, toString, getSparkUser, getStreamingJobProgressListener, logError, !=, getHandlers, getClass, logWarning, appId, detachPage, headerTabs, ne, eq, log, boundPort, ##, finalize, hashCode, logDebug, sslOptions, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appSparkVersion, DEFAULT_POOL_NAME, webUrl, DEFAULT_PORT, startTime, appName, prefix, killEnabled, getApplicationInfoList, sc, store, attachHandler, basePath, stop, conf, getUIPort, initialize, attachTab, <init>, STATIC_RESOURCE_DIR, SparkUI, SparkUITab, ==, securityManager, getSparkUser, appId, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appSparkVersion, DEFAULT_POOL_NAME, webUrl, DEFAULT_PORT, startTime, appName, prefix, killEnabled, getApplicationInfoList, sc, store, attachHandler, basePath, stop, conf, getUIPort, initialize, attachTab, <init>, STATIC_RESOURCE_DIR, SparkUI, SparkUITab, ==, securityManager, getSparkUser, appId, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(webUrl, setAppId, appName, asInstanceOf, sc, stop, conf, isInstanceOf, <init>, SparkUI, ==, toString, logError, !=, logWarning, appId, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala: Set(sc, attachPage, conf, <init>, SparkUI, SparkUITab)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(name, webUrl, startTime, setAppId, appName, asInstanceOf, synchronized, sc, bind, create, attachHandler, stop, conf, initialize, isInstanceOf, <init>, SparkUI, ==, clone, securityManager, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(attachPage, attachHandler, conf, initialize, <init>, STATIC_RESOURCE_DIR, SparkUI)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(serverInfo, detachHandler, asInstanceOf, synchronized, withSparkUI, bind, attachPage, attachHandler, handlers, stop, conf, initialize, isInstanceOf, <init>, STATIC_RESOURCE_DIR, SparkUI, getApplicationInfo, writeEventLogs, securityManager, toString, getHandlers, appId, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(killEnabled, attachPage, attachHandler, conf, initialize, <init>, STATIC_RESOURCE_DIR, SparkUI, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(appSparkVersion, name, appName, prefix, asInstanceOf, basePath, isInstanceOf, <init>, SparkUITab, ==, toString, logError, !=, getClass, headerTabs, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(startTime, killEnabled, sc, store, basePath, isInstanceOf, <init>, SparkUITab, ==, toString, getSparkUser, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala: Set(sc, isInstanceOf, <init>, SparkUITab, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala: Set(store, attachPage, <init>, SparkUI, SparkUITab)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala: Set(store, isInstanceOf, <init>, SparkUI, getApplicationInfo, ==, writeEventLogs, !=, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(DEFAULT_POOL_NAME, name, store, <init>, SparkUI, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala: Set(store, <init>, SparkUI)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(asInstanceOf, withSparkUI, isInstanceOf, <init>, SparkUI, ==, securityManager, toString, getClass, appId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala: Set(SparkUI)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(appSparkVersion, DEFAULT_POOL_NAME, name, startTime, equals, appName, asInstanceOf, conf, isInstanceOf, <init>, SparkUI, ==, toString, !=, getClass, appId, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(name, store, basePath, <init>, SparkUITab, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(appName, asInstanceOf, isInstanceOf, <init>, SparkUI, ==, toString, appId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(name, asInstanceOf, store, basePath, logName, conf, isInstanceOf, <init>, SparkUITab, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala: Set(store, attachPage, conf, <init>, SparkUI, SparkUITab)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(name, asInstanceOf, store, basePath, isInstanceOf, <init>, SparkUITab, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(name, killEnabled, sc, store, basePath, <init>, SparkUITab, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(name, prefix, asInstanceOf, getApplicationInfoList, isInstanceOf, <init>, SparkUI, ==, toString, !=, getHandlers, logWarning, appId, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(appSparkVersion, name, startTime, appName, asInstanceOf, synchronized, create, store, conf, initialize, isInstanceOf, <init>, SparkUI, ==, clone, toString, logError, !=, getClass, logWarning, appId, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkUI, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(killEnabled, sc, store, basePath, <init>, SparkUITab)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(name, startTime, asInstanceOf, killEnabled, store, basePath, isInstanceOf, <init>, SparkUITab, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(killEnabled, sc, store, attachPage, <init>, SparkUI, SparkUITab, ==, securityManager, toString, getSparkUser)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(killEnabled, sc, store, attachPage, conf, <init>, SparkUI, SparkUITab, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appSparkVersion, DEFAULT_POOL_NAME, webUrl, DEFAULT_PORT, startTime, appName, prefix, killEnabled, getApplicationInfoList, sc, store, attachHandler, basePath, stop, conf, getUIPort, initialize, attachTab, <init>, STATIC_RESOURCE_DIR, SparkUI, SparkUITab, ==, securityManager, getSparkUser, appId, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(startTime, killEnabled, sc, store, basePath, isInstanceOf, <init>, SparkUITab, ==, toString, getSparkUser, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(name, startTime, asInstanceOf, killEnabled, store, basePath, isInstanceOf, <init>, SparkUITab, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appSparkVersion, DEFAULT_POOL_NAME, webUrl, DEFAULT_PORT, startTime, appName, prefix, killEnabled, getApplicationInfoList, sc, store, attachHandler, basePath, stop, conf, getUIPort, initialize, attachTab, <init>, STATIC_RESOURCE_DIR, SparkUI, SparkUITab, ==, securityManager, getSparkUser, appId, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(name, basePath, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appSparkVersion, DEFAULT_POOL_NAME, webUrl, DEFAULT_PORT, startTime, appName, prefix, killEnabled, getApplicationInfoList, sc, store, attachHandler, basePath, stop, conf, getUIPort, initialize, attachTab, <init>, STATIC_RESOURCE_DIR, SparkUI, SparkUITab, ==, securityManager, getSparkUser, appId, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(name, asInstanceOf, store, basePath, logName, conf, isInstanceOf, <init>, SparkUITab, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(name, killEnabled, sc, store, basePath, <init>, SparkUITab, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(killEnabled, sc, store, basePath, <init>, SparkUITab)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	coresUsed, notify, drivers, wait, isAlive, $asInstanceOf, memoryUsed, state, equals, WorkerInfo, addExecutor, lastHeartbeat, hostPort, webUiAddress, memoryFree, cores, setState, asInstanceOf, host, synchronized, executors, $isInstanceOf, hasExecutor, removeExecutor, notifyAll, isInstanceOf, <init>, id, removeDriver, port, coresFree, ==, clone, endpoint, toString, !=, getClass, ne, addDriver, eq, ##, finalize, hashCode, memory.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(coresUsed, isAlive, memoryUsed, state, WorkerInfo, webUiAddress, cores, host, <init>, id, port, ==, toString, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala: Set(WorkerInfo, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala: Set(coresUsed, isAlive, memoryUsed, state, WorkerInfo, lastHeartbeat, webUiAddress, memoryFree, cores, host, executors, <init>, id, port, coresFree, toString, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(state, WorkerInfo, webUiAddress, <init>, id, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(coresUsed, drivers, memoryUsed, state, WorkerInfo, hostPort, cores, asInstanceOf, host, executors, isInstanceOf, <init>, id, port, ==, toString, eq, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala: Set(state, WorkerInfo, cores, asInstanceOf, isInstanceOf, <init>, id, ==, toString, hashCode, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(drivers, state, WorkerInfo, addExecutor, lastHeartbeat, hostPort, memoryFree, cores, setState, asInstanceOf, host, executors, removeExecutor, isInstanceOf, <init>, id, removeDriver, port, coresFree, ==, endpoint, toString, !=, ne, addDriver, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala: Set(WorkerInfo, <init>, id)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala: Set(state, WorkerInfo, cores, asInstanceOf, executors, isInstanceOf, <init>, id, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(state, WorkerInfo, webUiAddress, cores, executors, <init>, id, ==, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala: Set(state, WorkerInfo, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala: Set(state, WorkerInfo, <init>, id)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Distribution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Distribution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Distribution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Distribution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Distribution.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, getQuantiles$default$1, equals, getQuantiles, asInstanceOf, data, synchronized, $isInstanceOf, summary$default$1, statCounter, notifyAll, isInstanceOf, startIdx, <init>, apply, ==, clone, endIdx, showQuantiles$default$1, toString, length, !=, getClass, Distribution, defaultProbabilities, ne, showQuantiles, eq, summary, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, length, !=, Distribution, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(getQuantiles, asInstanceOf, statCounter, isInstanceOf, <init>, apply, ==, toString, !=, Distribution, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, buildProcessBuilder$default$7, wait, $asInstanceOf, equals, asInstanceOf, buildProcessBuilder$default$6, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, redirectStream, isInstanceOf, ==, clone, $init$, toString, logError, !=, CommandUtils, getClass, logWarning, ne, eq, log, buildProcessBuilder, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(==, toString, logError, !=, CommandUtils, logWarning, buildProcessBuilder, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, redirectStream, ==, !=, CommandUtils, logWarning, buildProcessBuilder, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/BoundedDouble.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/BoundedDouble.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/BoundedDouble.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/BoundedDouble.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/BoundedDouble.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, BoundedDouble, asInstanceOf, synchronized, $isInstanceOf, mean, confidence, notifyAll, isInstanceOf, <init>, low, ==, clone, toString, !=, getClass, ne, eq, high, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala: Set(BoundedDouble, mean, confidence, <init>, low, ==, high)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(BoundedDouble, asInstanceOf, synchronized, confidence, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala: Set(BoundedDouble, confidence, <init>, low, ==, high)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(BoundedDouble, mean, confidence, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(BoundedDouble, asInstanceOf, confidence, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala: Set(BoundedDouble, confidence, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(BoundedDouble, asInstanceOf, confidence, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(BoundedDouble, asInstanceOf, confidence, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(BoundedDouble, asInstanceOf, mean, confidence, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala: Set(BoundedDouble, mean, confidence, <init>, low, ==, high)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachPartitionFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachPartitionFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachPartitionFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachPartitionFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachPartitionFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, ForeachPartitionFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachPartitionFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	driverDesc, finishedExecutors, coresUsed, notify, unapply, curried, execId, drivers, RequestSubmitDriver, ReconnectWorker, executorIds, RequestKillDriver, restPort, success, wait, ApplicationRemoved, masterUrl, copy$default$2, $asInstanceOf, masterWebUiUrl, memoryUsed, copy$default$5, compose, ReregisterWithMaster, state, ApplicationFinished, RegisterApplication, productArity, appDescription, master, equals, ExecutorAdded, copy$default$9, RegisterWorkerFailed, finishedDrivers, copy$default$12, hostPort, cores, asInstanceOf, workers, UnregisterApplication, host, LaunchExecutor, uri, synchronized, executors, DeployMessages, $isInstanceOf, exception, andThen, workerHostPort, MasterChanged, WorkerRemoved, copy$default$8, tupled, ExecutorUpdated, copy$default$13, WorkerSchedulerStateResponse, canEqual, copy$default$4, RequestWorkerState, KillExecutor, productPrefix, ExecutorStateChanged, workerLost, RequestExecutors, notifyAll, MasterInStandby, isInstanceOf, WorkerLatestState, RegisteredWorker, activeApps, exitStatus, found, <init>, id, port, driver, KillDriverResponse, apply, RegisterWorker, DriverStatusResponse, requestedTotal, RegisterWorkerResponse, ==, activeDrivers, clone, status, SubmitDriverResponse, copy$default$7, WorkerStateResponse, copy$default$10, WorkDirCleanup, $init$, MasterChangeAcknowledged, RequestDriverStatus, KillDriver, DriverStateChanged, copy$default$3, copy, workerId, message, toString, SendHeartbeat, masterAddress, !=, getClass, LaunchDriver, copy$default$1, appId, StopAppClient, KillExecutors, completedDrivers, copy$default$6, completedApps, ne, RequestMasterState, copy$default$11, workerWebUiUrl, driverDescription, restUri, RegisteredApplication, eq, productIterator, driverIds, MasterStateResponse, Heartbeat, ##, finalize, appDesc, productElement, hashCode, worker, memory, DeployMessage, driverId.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(driverDesc, finishedExecutors, coresUsed, execId, drivers, masterUrl, masterWebUiUrl, memoryUsed, state, master, finishedDrivers, cores, host, executors, DeployMessages, RequestWorkerState, <init>, port, driver, apply, ==, WorkerStateResponse, workerId, toString, appId, appDesc, worker, memory, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(coresUsed, RequestKillDriver, memoryUsed, state, master, cores, workers, host, uri, DeployMessages, activeApps, <init>, id, port, driver, KillDriverResponse, apply, ==, activeDrivers, status, toString, completedDrivers, completedApps, RequestMasterState, restUri, MasterStateResponse, worker, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala: Set(finishedExecutors, coresUsed, execId, masterUrl, masterWebUiUrl, memoryUsed, state, master, cores, workers, host, uri, executors, DeployMessages, activeApps, <init>, id, port, apply, activeDrivers, status, WorkerStateResponse, workerId, toString, appId, completedDrivers, completedApps, MasterStateResponse, appDesc, worker, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(state, master, workers, DeployMessages, activeApps, <init>, id, apply, ==, RequestMasterState, MasterStateResponse)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(driverDesc, finishedExecutors, coresUsed, unapply, execId, drivers, ReconnectWorker, masterUrl, masterWebUiUrl, memoryUsed, ReregisterWithMaster, state, ApplicationFinished, master, RegisterWorkerFailed, finishedDrivers, cores, asInstanceOf, host, LaunchExecutor, synchronized, executors, DeployMessages, exception, MasterChanged, WorkerSchedulerStateResponse, RequestWorkerState, KillExecutor, ExecutorStateChanged, MasterInStandby, isInstanceOf, WorkerLatestState, RegisteredWorker, exitStatus, <init>, id, port, driver, apply, RegisterWorker, RegisterWorkerResponse, ==, WorkerStateResponse, WorkDirCleanup, KillDriver, DriverStateChanged, workerId, message, toString, SendHeartbeat, masterAddress, !=, LaunchDriver, appId, ne, workerWebUiUrl, Heartbeat, appDesc, worker, memory, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(execId, state, cores, host, DeployMessages, ExecutorStateChanged, <init>, apply, ==, workerId, message, toString, !=, appId, appDesc, worker, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(execId, drivers, RequestSubmitDriver, ReconnectWorker, executorIds, RequestKillDriver, restPort, ApplicationRemoved, masterUrl, masterWebUiUrl, state, ApplicationFinished, RegisterApplication, appDescription, master, ExecutorAdded, RegisterWorkerFailed, hostPort, cores, asInstanceOf, workers, UnregisterApplication, host, LaunchExecutor, executors, DeployMessages, exception, MasterChanged, WorkerRemoved, ExecutorUpdated, WorkerSchedulerStateResponse, KillExecutor, ExecutorStateChanged, RequestExecutors, MasterInStandby, isInstanceOf, WorkerLatestState, RegisteredWorker, exitStatus, <init>, id, port, driver, KillDriverResponse, apply, RegisterWorker, DriverStatusResponse, requestedTotal, ==, SubmitDriverResponse, MasterChangeAcknowledged, RequestDriverStatus, KillDriver, DriverStateChanged, workerId, message, toString, masterAddress, !=, LaunchDriver, appId, KillExecutors, completedDrivers, completedApps, ne, RequestMasterState, workerWebUiUrl, driverDescription, RegisteredApplication, driverIds, MasterStateResponse, Heartbeat, worker, memory, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(RequestSubmitDriver, RequestKillDriver, success, masterUrl, state, asInstanceOf, host, DeployMessages, exception, workerHostPort, isInstanceOf, found, <init>, KillDriverResponse, apply, DriverStatusResponse, SubmitDriverResponse, RequestDriverStatus, workerId, message, toString, driverDescription, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(unapply, executorIds, ApplicationRemoved, state, RegisterApplication, appDescription, master, ExecutorAdded, hostPort, cores, asInstanceOf, UnregisterApplication, host, DeployMessages, exception, andThen, MasterChanged, WorkerRemoved, ExecutorUpdated, workerLost, RequestExecutors, isInstanceOf, exitStatus, <init>, id, apply, requestedTotal, ==, MasterChangeAcknowledged, workerId, message, masterAddress, !=, appId, StopAppClient, KillExecutors, ne, RegisteredApplication, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(driverDesc, master, synchronized, DeployMessages, <init>, apply, ==, DriverStateChanged, !=, worker, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(RequestSubmitDriver, RequestKillDriver, success, state, master, hostPort, cores, asInstanceOf, DeployMessages, exception, workerHostPort, isInstanceOf, found, <init>, id, KillDriverResponse, apply, DriverStatusResponse, ==, SubmitDriverResponse, RequestDriverStatus, workerId, message, ne, driverDescription, memory, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(state, master, cores, executors, DeployMessages, activeApps, <init>, id, apply, ==, appId, completedApps, RequestMasterState, MasterStateResponse, worker, memory)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/SparkSubmitArgumentsParser.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/SparkSubmitArgumentsParser.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/SparkSubmitArgumentsParser.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/SparkSubmitArgumentsParser.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/SparkSubmitArgumentsParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/SparkSubmitArgumentsParser.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, switches, handleExtraArgs, PRINCIPAL, SUPERVISE, PROXY_USER, DRIVER_CLASS_PATH, wait, PY_FILES, $asInstanceOf, REPOSITORIES, VERBOSE, equals, NAME, USAGE_ERROR, asInstanceOf, KEYTAB, synchronized, $isInstanceOf, DRIVER_MEMORY, STATUS, EXECUTOR_MEMORY, DRIVER_CORES, CONF, notifyAll, TOTAL_EXECUTOR_CORES, isInstanceOf, VERSION, EXECUTOR_CORES, FILES, <init>, DEPLOY_MODE, parse, ==, HELP, PACKAGES_EXCLUDE, MASTER, clone, PROPERTIES_FILE, PACKAGES, KILL_SUBMISSION, SparkSubmitArgumentsParser, toString, NUM_EXECUTORS, QUEUE, !=, getClass, DRIVER_JAVA_OPTIONS, JARS, DRIVER_LIBRARY_PATH, ARCHIVES, handleUnknown, ne, opts, CLASS, eq, ##, finalize, hashCode, handle.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala: Set(isInstanceOf, <init>, parse, ==, SparkSubmitArgumentsParser, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(PRINCIPAL, asInstanceOf, KEYTAB, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	appSparkVersion, notify, parent, name, wait, $asInstanceOf, equals, appName, prefix, asInstanceOf, killEnabled, synchronized, sc, $isInstanceOf, pages, store, attachPage, basePath, notifyAll, conf, isInstanceOf, <init>, ==, clone, isFairScheduler, StagesTab, toString, !=, getClass, headerTabs, ne, handleKillRequest, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(parent, name, basePath, <init>, StagesTab, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appSparkVersion, parent, appName, prefix, killEnabled, sc, store, basePath, conf, <init>, ==, StagesTab, handleKillRequest)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(parent, name, asInstanceOf, store, basePath, conf, isInstanceOf, <init>, ==, StagesTab, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(parent, name, killEnabled, sc, store, basePath, <init>, ==, isFairScheduler, StagesTab)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(parent, killEnabled, sc, store, basePath, <init>, isFairScheduler, StagesTab)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, hasHostAliveOnRack, handleTaskGettingResult, wait, defaultParallelism, $asInstanceOf, MIN_TIME_TO_SPECULATION, workerRemoved, dagScheduler, taskResultGetter, nextTaskId, equals, hasExecutorsAliveOnHost, taskSetManagerForAttempt, mapOutputTracker, hostToExecutors, asInstanceOf, initializeLogIfNecessary, executorIdToHost, applicationId, cancelTasks, synchronized, sc, $isInstanceOf, checkSpeculatableTasks, maxTaskFailures, logTrace, executorAdded, isTraceEnabled, initializeLogIfNecessary$default$2, rootPool, isExecutorAlive, stop, getRackForHost, logName, notifyAll, conf, createTaskSetManager, STARVATION_TIMEOUT_MS, initialize, isInstanceOf, blacklistTrackerOpt, <init>$default$3, <init>, SPECULATION_INTERVAL_MS, prioritizeContainers, CPUS_PER_TASK, hostsByRack, error, applicationAttemptId, statusUpdate, taskIdToTaskSetManager, handleFailedTask, ==, executorLost, clone, isExecutorBusy, resourceOffers, $init$, setDAGScheduler, backend, SCHEDULER_MODE_PROPERTY, toString, newTaskId, logError, !=, getExecutorsAliveOnHost, getClass, taskSetFinished, logWarning, killTaskAttempt, start, runningTasksByExecutors, postStartHook, schedulingMode, shuffleOffers, ne, executorHeartbeatReceived, TaskSchedulerImpl, markPartitionCompletedInAllTaskSets, nodeBlacklist, eq, log, taskIdToExecutorId, ##, finalize, handleSuccessfulTask, hashCode, logDebug, submitTasks, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(workerRemoved, asInstanceOf, synchronized, sc, stop, conf, isInstanceOf, <init>, CPUS_PER_TASK, statusUpdate, taskIdToTaskSetManager, ==, executorLost, isExecutorBusy, resourceOffers, toString, logError, !=, getExecutorsAliveOnHost, logWarning, ne, TaskSchedulerImpl, nodeBlacklist, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(hasHostAliveOnRack, dagScheduler, hasExecutorsAliveOnHost, mapOutputTracker, hostToExecutors, asInstanceOf, synchronized, sc, checkSpeculatableTasks, maxTaskFailures, isExecutorAlive, getRackForHost, conf, isInstanceOf, <init>, handleFailedTask, ==, backend, toString, newTaskId, logError, !=, getExecutorsAliveOnHost, taskSetFinished, logWarning, ne, TaskSchedulerImpl, markPartitionCompletedInAllTaskSets, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, sc, stop, conf, isInstanceOf, <init>, error, ==, toString, logError, !=, logWarning, start, TaskSchedulerImpl, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(defaultParallelism, dagScheduler, asInstanceOf, applicationId, synchronized, sc, rootPool, stop, conf, initialize, isInstanceOf, <init>, applicationAttemptId, ==, clone, backend, toString, logError, !=, getClass, logWarning, killTaskAttempt, start, postStartHook, schedulingMode, ne, TaskSchedulerImpl, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, sc, stop, conf, isInstanceOf, <init>, CPUS_PER_TASK, statusUpdate, ==, resourceOffers, toString, TaskSchedulerImpl, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(handleTaskGettingResult, asInstanceOf, conf, isInstanceOf, <init>, handleFailedTask, ==, logError, !=, ne, TaskSchedulerImpl, handleSuccessfulTask, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, e, notify, withPadding, unapply, find, curried, span, toBuffer, count, reduceOption, wait, foldRight, copy$default$2, takeWhile, $asInstanceOf, minBy, size, copy$default$5, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, productArity, toMap, filterNot, equals, blocks, toList, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, tupled, min, scanRight, fold, logTrace, buf, nonEmpty, canEqual, copy$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, /:, toIterator, ShuffleBlockFetcherIterator, addString, to, collectFirst, drop, isInstanceOf, filter, FetchResult, releaseCurrentResultBuffer, GroupedIterator, FailureFetchResult, <init>, toStream, max, buffered, apply, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, copy$default$3, address, copy, toString, SuccessFetchResult, copyToArray, length, seq, logError, !=, FetchRequest, collect, getClass, logWarning, copy$default$1, hasDefiniteSize, createTempFile, patch, foldLeft, contains, isEmpty, ne, blockId, withPartial, reversed, hasNext, indexOf, reduceLeft, registerTempFileToClean, eq, productIterator, sum, log, ##, scanLeft, finalize, productElement, hashCode, zipAll, logDebug, product, logInfo, isNetworkReqDone.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(map, asInstanceOf, ShuffleBlockFetcherIterator, isInstanceOf, <init>, apply, flatMap, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	OFFSET_BITS, notify, getOffsetInPage, wait, releaseExecutionMemory, $assertionsDisabled, equals, acquireExecutionMemory, pageSizeBytes, notifyAll, showMemoryUsage, allocatePage, freePage, <init>, cleanUpAllAllocatedMemory, MAXIMUM_PAGE_SIZE_BYTES, tungstenMemoryMode, getTungstenMemoryMode, toString, decodePageNumber, getClass, getMemoryConsumptionForThisTask, getPage, TaskMemoryManager, encodePageNumberAndOffset, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(<init>, TaskMemoryManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(<init>, getClass, TaskMemoryManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(<init>, getClass, TaskMemoryManager, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(<init>, toString, TaskMemoryManager, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, notifyAll, <init>, cleanUpAllAllocatedMemory, TaskMemoryManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(<init>, TaskMemoryManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(notifyAll, <init>, TaskMemoryManager)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(<init>, getTungstenMemoryMode, TaskMemoryManager)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, SparkTransportConf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, fromSparkConf$default$3, fromSparkConf, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(SparkTransportConf, synchronized, ==, !=, fromSparkConf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, SparkTransportConf, synchronized, isInstanceOf, ==, clone, toString, !=, fromSparkConf, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, SparkTransportConf, synchronized, isInstanceOf, ==, toString, !=, getClass, fromSparkConf, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(SparkTransportConf, !=, getClass, fromSparkConf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(SparkTransportConf, ==, !=, fromSparkConf)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Utils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Utils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Utils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Utils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Utils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, Utils, !=, getClass, ne, eq, ##, finalize, hashCode, takeOrdered.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, Utils, !=, getClass, ne, takeOrdered)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, readCheckpointFile, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, writePartitionToCheckpointFile$default$3, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, ReliableCheckpointRDD, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, writePartitionToCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, writeRDDToCheckpointDirectory$default$3, filter, pipe$default$3, checkpointPath, countByValueApprox$default$3, unpersist$default$1, <init>$default$3, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, writeRDDToCheckpointDirectory, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(map, ReliableCheckpointRDD, context, synchronized, conf, checkpointPath, <init>, isCheckpointed, id, foreach, toString, writeRDDToCheckpointDirectory, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, parent, partitioner, name, setName, union, map, ReliableCheckpointRDD, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, <init>, id, max, toDebugString, ++, flatMap, ==, clone, foreach, zipWithIndex, first, toString, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, curried, ElectedLeader, storedWorkers, restPort, RevokedLeadership, wait, copy$default$2, $asInstanceOf, CompleteRecovery, productArity, MasterMessages, equals, webUIPort, asInstanceOf, synchronized, $isInstanceOf, tupled, canEqual, productPrefix, notifyAll, CheckForWorkerTimeOut, isInstanceOf, rpcEndpointPort, <init>, apply, ==, clone, BoundPortsRequest, BeginRecovery, $init$, BoundPortsResponse, copy$default$3, copy, toString, !=, storedApps, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(ElectedLeader, storedWorkers, restPort, RevokedLeadership, CompleteRecovery, MasterMessages, webUIPort, asInstanceOf, CheckForWorkerTimeOut, isInstanceOf, <init>, apply, ==, BoundPortsRequest, BoundPortsResponse, toString, !=, storedApps, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, getRecordLength, $assertionsDisabled, equals, notifyAll, UnsafeSorterSpillReader, <init>, getKeyPrefix, toString, loadNext, getBaseOffset, getBaseObject, getClass, close, getNumRecords, hasNext, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnvStoppedException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnvStoppedException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnvStoppedException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnvStoppedException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnvStoppedException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, RpcEnvStoppedException, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, RpcEnvStoppedException, isInstanceOf, <init>, getMessage, ==, clone, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala: Set(asInstanceOf, synchronized, RpcEnvStoppedException, isInstanceOf, <init>, getMessage, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, RpcEnvStoppedException, isInstanceOf, <init>, getMessage, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, transformStorageLevel, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, doCheckpoint, synchronized, cpState, $isInstanceOf, LocalRDDCheckpointData, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, isCheckpointed, ==, checkpointRDD, clone, $init$, DEFAULT_STORAGE_LEVEL, checkpoint, toString, logError, !=, getClass, logWarning, getPartitions, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(transformStorageLevel, asInstanceOf, doCheckpoint, synchronized, LocalRDDCheckpointData, isInstanceOf, <init>, isCheckpointed, ==, checkpointRDD, clone, DEFAULT_STORAGE_LEVEL, checkpoint, toString, !=, getClass, logWarning, getPartitions, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocality.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocality.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocality.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocality.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocality.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocality.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	METRIC_FILES_DISCOVERED, notify, wait, $asInstanceOf, equals, asInstanceOf, incrementHiveClientCalls, metricRegistry, synchronized, $isInstanceOf, METRIC_GENERATED_CLASS_BYTECODE_SIZE, METRIC_SOURCE_CODE_SIZE, incrementFileCacheHits, notifyAll, sourceName, METRIC_PARALLEL_LISTING_JOB_COUNT, isInstanceOf, METRIC_HIVE_CLIENT_CALLS, incrementParallelListingJobCount, METRIC_FILE_CACHE_HITS, incrementFilesDiscovered, CodegenMetrics, ==, clone, incrementFetchedPartitions, allSources, METRIC_COMPILATION_TIME, reset, toString, StaticSources, !=, getClass, ne, HiveCatalogMetrics, eq, METRIC_GENERATED_METHOD_BYTECODE_SIZE, ##, METRIC_PARTITIONS_FETCHED, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, metricRegistry, sourceName, ==, allSources, StaticSources, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, readerCount_=, assertBlockIsLockedForWriting, wait, getTaskLockCount, $asInstanceOf, size, registerTask, equals, writerTask, clear, size_=, asInstanceOf, initializeLogIfNecessary, BlockInfoManager, synchronized, writerTask_=, tellMaster, $isInstanceOf, removeBlock, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, unlock, logName, notifyAll, lockNewBlockForWriting, isInstanceOf, NO_WRITER, downgradeLock, classTag, <init>, lockForWriting, readerCount, entries, lockForWriting$default$2, ==, lockForReading, clone, getNumberOfMapEntries, $init$, toString, BlockInfo, releaseAllLocksForTask, lockForReading$default$2, logError, !=, unlock$default$2, get, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, level, logInfo, NON_TASK_WRITER.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala: Set(size, BlockInfoManager, unlock, <init>, ==, lockForReading, BlockInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(assertBlockIsLockedForWriting, size, registerTask, clear, size_=, asInstanceOf, BlockInfoManager, synchronized, tellMaster, removeBlock, logTrace, unlock, lockNewBlockForWriting, isInstanceOf, downgradeLock, classTag, <init>, lockForWriting, entries, ==, lockForReading, toString, BlockInfo, releaseAllLocksForTask, logError, !=, get, getClass, logWarning, ne, hashCode, logDebug, level, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(size, clear, asInstanceOf, BlockInfoManager, synchronized, removeBlock, unlock, isInstanceOf, classTag, <init>, lockForWriting, entries, ==, toString, BlockInfo, !=, get, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/UninterruptibleThread.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/UninterruptibleThread.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/UninterruptibleThread.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/UninterruptibleThread.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/UninterruptibleThread.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, runUninterruptibly, resume, wait, isAlive, threadLocalRandomSeed, $asInstanceOf, setName, getContextClassLoader, join, equals, setPriority, parkBlocker, threadLocalRandomProbe, asInstanceOf, run, getPriority, synchronized, blockedOn, $isInstanceOf, checkAccess, suspend, getUncaughtExceptionHandler, getThreadGroup, stop, notifyAll, getName, isInterrupted, isInstanceOf, getState, getStackTrace, <init>, destroy, ==, clone, setDaemon, inheritableThreadLocals, setContextClassLoader, threadLocals, toString, isDaemon, !=, getClass, setUncaughtExceptionHandler, countStackFrames, start, ne, UninterruptibleThread, getId, threadLocalRandomSecondarySeed, eq, interrupt, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, setName, getContextClassLoader, asInstanceOf, run, synchronized, stop, notifyAll, getName, isInstanceOf, <init>, ==, setDaemon, setContextClassLoader, !=, ne, UninterruptibleThread, getId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/JVMObjectTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/JVMObjectTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/JVMObjectTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/JVMObjectTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/JVMObjectTracker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, size, productArity, equals, clear, asInstanceOf, synchronized, $isInstanceOf, canEqual, productPrefix, JVMObjectId, notifyAll, isInstanceOf, <init>, id, remove, apply, ==, JVMObjectTracker, clone, $init$, copy, toString, !=, get, getClass, copy$default$1, ne, addAndGetId, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(asInstanceOf, JVMObjectId, isInstanceOf, <init>, remove, apply, ==, JVMObjectTracker, !=, get, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, id, apply, ==, JVMObjectTracker, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(clear, asInstanceOf, <init>, apply, ==, JVMObjectTracker, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SerDe.scala: Set(size, asInstanceOf, JVMObjectId, isInstanceOf, <init>, id, apply, ==, JVMObjectTracker, toString, !=, getClass, ne, addAndGetId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, stop, notifyAll, initialize, isInstanceOf, <init>, ==, clone, toString, !=, getClass, unbroadcast, newBroadcast, ne, eq, ##, finalize, hashCode, TorrentBroadcastFactory.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(synchronized, stop, initialize, <init>, unbroadcast, newBroadcast, TorrentBroadcastFactory)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobResult.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobResult.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobResult.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobResult.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobResult.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, JobResult, wait, $asInstanceOf, productArity, equals, JobSucceeded, asInstanceOf, synchronized, $isInstanceOf, exception, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, ==, clone, JobFailed, $init$, copy, toString, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(JobResult, JobSucceeded, asInstanceOf, exception, isInstanceOf, <init>, ==, JobFailed, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(JobResult, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(JobResult, JobSucceeded, asInstanceOf, synchronized, exception, isInstanceOf, <init>, ==, clone, JobFailed, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(JobResult, equals, JobSucceeded, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, SparkStatusTracker, asInstanceOf, getActiveStageIds, synchronized, $isInstanceOf, notifyAll, isInstanceOf, getActiveJobIds, <init>, ==, clone, getExecutorInfos, getStageInfo, toString, !=, getClass, getJobInfo, ne, eq, getJobIdsForGroup, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala: Set(SparkStatusTracker, getActiveStageIds, getActiveJobIds, <init>, getStageInfo, getJobInfo, getJobIdsForGroup)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala: Set(SparkStatusTracker, asInstanceOf, isInstanceOf, <init>, ==, getExecutorInfos, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(SparkStatusTracker, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, mergeValue, wait, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, canEqual, productPrefix, combineCombinersByKey, notifyAll, isInstanceOf, combineValuesByKey, createCombiner, <init>, Aggregator, mergeCombiners, ==, clone, $init$, copy$default$3, copy, toString, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>, Aggregator, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(mergeValue, asInstanceOf, isInstanceOf, combineValuesByKey, createCombiner, <init>, Aggregator, mergeCombiners, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(mergeValue, asInstanceOf, synchronized, isInstanceOf, createCombiner, <init>, Aggregator, mergeCombiners, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(asInstanceOf, <init>, Aggregator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, combineCombinersByKey, isInstanceOf, combineValuesByKey, <init>, Aggregator, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(mergeValue, equals, asInstanceOf, isInstanceOf, createCombiner, <init>, Aggregator, mergeCombiners, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf, <init>, Aggregator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, Aggregator, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, diskSize, wait, copy$default$2, $asInstanceOf, copy$default$5, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, canEqual, copy$default$4, productPrefix, notifyAll, isInstanceOf, BlockUpdatedInfo, blockManagerId, <init>, apply, ==, clone, $init$, copy$default$3, copy, toString, !=, memSize, getClass, copy$default$1, ne, blockId, eq, productIterator, storageLevel, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(unapply, diskSize, asInstanceOf, isInstanceOf, BlockUpdatedInfo, blockManagerId, <init>, apply, ==, toString, !=, memSize, ne, blockId, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(asInstanceOf, isInstanceOf, BlockUpdatedInfo, blockManagerId, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(diskSize, equals, asInstanceOf, isInstanceOf, BlockUpdatedInfo, blockManagerId, <init>, apply, ==, toString, !=, memSize, getClass, ne, blockId, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(diskSize, asInstanceOf, isInstanceOf, BlockUpdatedInfo, blockManagerId, <init>, apply, ==, toString, !=, memSize, ne, blockId, eq, storageLevel, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, SHUFFLE, printStackTrace, getLocalizedMessage, wait, StreamBlockId, copy$default$2, streamId, $asInstanceOf, field, isRDD, TempShuffleBlockId, BlockId, productArity, equals, fillInStackTrace, initCause, asInstanceOf, shuffleId, TEST, synchronized, BroadcastBlockId, TestBlockId, $isInstanceOf, getCause, isShuffle, canEqual, productPrefix, notifyAll, isInstanceOf, SHUFFLE_INDEX, broadcastId, getStackTrace, getStackTraceElement, mapId, <init>, id, getMessage, setStackTrace, apply, getSuppressed, TaskResultBlockId, splitIndex, ==, rddId, getStackTraceDepth, asRDDId, clone, TEMP_LOCAL, TEMP_SHUFFLE, addSuppressed, $init$, RDD, copy$default$3, copy, ShuffleBlockId, toString, !=, UnrecognizedBlockId, getClass, copy$default$1, uniqueId, ShuffleIndexBlockId, reduceId, isBroadcast, ne, RDDBlockId, TASKRESULT, eq, productIterator, taskId, BROADCAST, TempLocalBlockId, ##, finalize, productElement, hashCode, SHUFFLE_DATA, STREAM, ShuffleDataBlockId.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(name, BlockId, asInstanceOf, shuffleId, synchronized, isInstanceOf, getStackTrace, mapId, <init>, id, setStackTrace, apply, ==, clone, RDD, toString, !=, ne, RDDBlockId, taskId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(BlockId, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(name, BlockId, asInstanceOf, shuffleId, isInstanceOf, broadcastId, <init>, apply, ==, rddId, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala: Set(BlockId, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala: Set(BlockId, asInstanceOf, isInstanceOf, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala: Set(BlockId, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala: Set(BlockId, asInstanceOf, isInstanceOf, <init>, id, apply, ==, rddId, toString, !=, ne, RDDBlockId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(name, BlockId, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala: Set(BlockId, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala: Set(BlockId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(BlockId, shuffleId, synchronized, mapId, <init>, apply, ==, ShuffleBlockId, !=, ShuffleIndexBlockId, reduceId, ShuffleDataBlockId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(BlockId, asInstanceOf, synchronized, BroadcastBlockId, isInstanceOf, broadcastId, <init>, id, apply, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(name, BlockId, asInstanceOf, synchronized, isShuffle, isInstanceOf, <init>, id, apply, ==, clone, RDD, toString, !=, getClass, ne, RDDBlockId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(isRDD, BlockId, asInstanceOf, shuffleId, TestBlockId, isInstanceOf, broadcastId, <init>, id, apply, ==, rddId, asRDDId, toString, !=, isBroadcast, ne, RDDBlockId, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(BlockId, shuffleId, mapId, <init>, apply, ShuffleBlockId, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(StreamBlockId, BlockId, asInstanceOf, isInstanceOf, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala: Set(wait, BlockId, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, apply, ==, !=, taskId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala: Set(ShuffleBlockId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(BlockId, asInstanceOf, synchronized, BroadcastBlockId, isShuffle, isInstanceOf, broadcastId, <init>, id, getMessage, apply, ==, rddId, asRDDId, copy, ShuffleBlockId, toString, !=, getClass, ne, RDDBlockId, TempLocalBlockId, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(BlockId, <init>, id, apply, RDD, RDDBlockId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(BlockId, asInstanceOf, synchronized, <init>, apply, ==, !=, getClass, ne, TempLocalBlockId, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(BlockId, synchronized, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(<init>, id, apply, rddId, RDD, RDDBlockId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(name, StreamBlockId, BlockId, equals, asInstanceOf, isInstanceOf, <init>, id, apply, ==, rddId, toString, !=, getClass, ne, RDDBlockId, taskId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(name, BlockId, asInstanceOf, synchronized, <init>, apply, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(name, BlockId, <init>, apply, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala: Set(BlockId, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(BlockId, shuffleId, broadcastId, <init>, getMessage, apply, ==, rddId, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala: Set(BlockId, <init>, apply, ==, !=, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(BlockId, synchronized, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(TempShuffleBlockId, BlockId, asInstanceOf, synchronized, isInstanceOf, <init>, id, apply, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala: Set(BlockId, synchronized, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(name, wait, BlockId, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, apply, TaskResultBlockId, ==, !=, ne, eq, taskId)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(BlockId, asInstanceOf, shuffleId, isInstanceOf, <init>, apply, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(BlockId, asInstanceOf, isInstanceOf, <init>, id, apply, ==, RDD, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(BlockId, asInstanceOf, shuffleId, synchronized, isInstanceOf, mapId, <init>, apply, ==, ShuffleBlockId, toString, !=, reduceId, ne, eq, TempLocalBlockId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(BlockId, synchronized, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(BlockId, asInstanceOf, shuffleId, isInstanceOf, broadcastId, <init>, apply, ==, rddId, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(name, BlockId, asInstanceOf, isInstanceOf, <init>, apply, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, BlockId, asInstanceOf, shuffleId, synchronized, notifyAll, isInstanceOf, mapId, <init>, id, getMessage, apply, ==, RDD, ShuffleBlockId, toString, !=, reduceId, ne, eq, BROADCAST)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(StreamBlockId, BlockId, asInstanceOf, synchronized, isInstanceOf, <init>, id, apply, ==, rddId, asRDDId, copy, toString, !=, RDDBlockId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala: Set(streamId, BlockId, asInstanceOf, isInstanceOf, <init>, apply, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(name, TempShuffleBlockId, BlockId, synchronized, <init>, apply, clone, !=, UnrecognizedBlockId, TempLocalBlockId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(name, BlockId, asInstanceOf, shuffleId, isInstanceOf, getStackTrace, mapId, <init>, id, getMessage, setStackTrace, apply, ==, rddId, toString, !=, reduceId, ne, taskId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unpersist, PeriodicRDDCheckpointer, wait, $asInstanceOf, equals, deleteAllCheckpointsButLast, unpersistDataSet, asInstanceOf, initializeLogIfNecessary, synchronized, sc, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, persist, <init>, isCheckpointed, checkpointInterval, ==, clone, $init$, checkpoint, getCheckpointFiles, toString, logError, !=, getClass, logWarning, update, deleteAllCheckpoints, ne, getAllCheckpointFiles, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, ForeachFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ForeachFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, schedulerDelay, isInstanceOf, ==, clone, toString, AppStatusUtils, !=, getClass, gettingResultTime, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala: Set(asInstanceOf, schedulerDelay, isInstanceOf, ==, toString, AppStatusUtils, gettingResultTime, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, schedulerDelay, isInstanceOf, ==, toString, AppStatusUtils, !=, gettingResultTime, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	nextPos, notify, _bitset, wait, $asInstanceOf, IntHasher, size, union, OpenHashSet, addWithoutResize, equals, _capacity, INVALID_POS, asInstanceOf, synchronized, $isInstanceOf, hasher, POSITION_MASK, iterator, notifyAll, _mask, isInstanceOf, getValueSafe, hash, <init>, MAX_CAPACITY, LongHasher, ==, clone, _growThreshold, _size, NONEXISTENCE_MASK, toString, getPos, !=, _data, getClass, getValue, contains, ne, getBitSet, rehashIfNeeded, add, Hasher, eq, ##, finalize, hashCode, capacity.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala: Set(size, OpenHashSet, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, contains, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala: Set(nextPos, size, OpenHashSet, addWithoutResize, INVALID_POS, asInstanceOf, POSITION_MASK, <init>, ==, NONEXISTENCE_MASK, getPos, !=, getValue, rehashIfNeeded, capacity)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(size, OpenHashSet, <init>, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(size, OpenHashSet, equals, asInstanceOf, iterator, isInstanceOf, <init>, ==, toString, !=, getClass, getValue, contains, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala: Set(nextPos, size, OpenHashSet, addWithoutResize, INVALID_POS, POSITION_MASK, <init>, ==, NONEXISTENCE_MASK, getPos, !=, getValue, rehashIfNeeded, capacity)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/InputFileBlockHolder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/InputFileBlockHolder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/InputFileBlockHolder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/InputFileBlockHolder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/InputFileBlockHolder.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, unset, asInstanceOf, set, synchronized, $isInstanceOf, getLength, notifyAll, isInstanceOf, ==, getStartOffset, clone, toString, !=, getClass, ne, InputFileBlockHolder, eq, ##, finalize, hashCode, getInputFilePath.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, unset, asInstanceOf, set, synchronized, getLength, isInstanceOf, ==, toString, !=, InputFileBlockHolder)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, unset, asInstanceOf, set, synchronized, getLength, isInstanceOf, ==, toString, !=, InputFileBlockHolder)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, doTrace, statusRequestServlet, wait, contextToServlet, sendResponse, $asInstanceOf, equals, requestedPort, RestSubmissionServer, findUnknownFields, asInstanceOf, initializeLogIfNecessary, host, SubmitRequestServlet, handleSubmit, synchronized, $isInstanceOf, getServletContext, StatusRequestServlet, logTrace, killRequestServlet, isTraceEnabled, initializeLogIfNecessary$default$2, doHead, service, stop, logName, notifyAll, masterConf, isInstanceOf, doPut, getInitParameter, handleError, getServletInfo, <init>, destroy, getServletName, formatException, handleKill, RestServlet, doGet, ==, PROTOCOL_VERSION, clone, SC_UNKNOWN_PROTOCOL_VERSION, $init$, submitRequestServlet, getLastModified, toString, logError, !=, getClass, logWarning, getInitParameterNames, start, getServletConfig, baseContext, ne, parseSubmissionId, doDelete, init, handleStatus, KillRequestServlet, eq, log, doOptions, ##, finalize, hashCode, logDebug, doPost, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(statusRequestServlet, requestedPort, RestSubmissionServer, findUnknownFields, asInstanceOf, host, SubmitRequestServlet, StatusRequestServlet, killRequestServlet, masterConf, isInstanceOf, handleError, <init>, formatException, submitRequestServlet, toString, KillRequestServlet)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, host, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, start, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, host, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, start, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, isLocal, BlockStatus, diskSize, wait, onNetworkError, copy$default$2, $asInstanceOf, empty, productArity, equals, cachedBlocks, blocks, clear, asInstanceOf, initializeLogIfNecessary, synchronized, getStatus, self, $isInstanceOf, receive, lastSeenMs, removeBlock, logTrace, canEqual, isTraceEnabled, remainingMem, initializeLogIfNecessary$default$2, productPrefix, stop, logName, notifyAll, isInstanceOf, maxOffHeapMem, blockManagerId, <init>, onError, apply, updateBlockInfo, ==, receiveAndReply, updateLastSeenMs, BlockManagerMasterEndpoint, clone, slaveEndpoint, proactivelyReplicate, isCached, $init$, onDisconnected, copy$default$3, copy, toString, BlockManagerInfo, logError, !=, memSize, onConnected, getClass, logWarning, copy$default$1, onStop, ne, onStart, maxMem, rpcEnv, eq, productIterator, storageLevel, maxOnHeapMem, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(BlockStatus, diskSize, asInstanceOf, isInstanceOf, blockManagerId, <init>, apply, ==, toString, memSize, eq, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala: Set(BlockStatus, diskSize, empty, blocks, asInstanceOf, logTrace, isInstanceOf, maxOffHeapMem, blockManagerId, <init>, apply, ==, toString, !=, memSize, ne, maxMem, eq, storageLevel, maxOnHeapMem)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(isLocal, empty, asInstanceOf, synchronized, stop, isInstanceOf, <init>, apply, ==, BlockManagerMasterEndpoint, toString, !=, getClass, logWarning, ne, rpcEnv, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(unapply, BlockStatus, diskSize, empty, clear, asInstanceOf, synchronized, removeBlock, logTrace, stop, isInstanceOf, blockManagerId, <init>, apply, updateBlockInfo, ==, slaveEndpoint, copy, toString, logError, !=, memSize, getClass, logWarning, ne, rpcEnv, storageLevel, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(BlockStatus, empty, asInstanceOf, synchronized, <init>, apply, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(BlockStatus, diskSize, blockManagerId, <init>, apply, ==, slaveEndpoint, logError, !=, memSize, logWarning, ne, storageLevel, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(BlockStatus, asInstanceOf, getStatus, removeBlock, isInstanceOf, <init>, apply, ==, logError, !=, rpcEnv, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(unapply, BlockStatus, diskSize, empty, asInstanceOf, isInstanceOf, maxOffHeapMem, blockManagerId, <init>, apply, ==, toString, !=, memSize, ne, maxMem, storageLevel, maxOnHeapMem)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	htmlResponderToServlet, jsonResponderToServlet, textResponderToServlet.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ToolTips.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ToolTips.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ToolTips.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ToolTips.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ToolTips.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	BLACKLISTED, notify, JOB_TIMELINE, wait, $asInstanceOf, SHUFFLE_READ, equals, asInstanceOf, PEAK_EXECUTION_MEMORY, synchronized, $isInstanceOf, SHUFFLE_WRITE, SHUFFLE_READ_REMOTE_SIZE, notifyAll, STORAGE_MEMORY, SHUFFLE_READ_BLOCKED_TIME, isInstanceOf, TASK_DESERIALIZATION_TIME, STAGE_TIMELINE, ==, clone, RESULT_SERIALIZATION_TIME, ToolTips, STAGE_DAG, toString, !=, SCHEDULER_DELAY, GETTING_RESULT_TIME, getClass, INPUT, GC_TIME, APPLICATION_EXECUTOR_LIMIT, TASK_TIME, JOB_DAG, ne, OUTPUT, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(asInstanceOf, isInstanceOf, ==, ToolTips, STAGE_DAG, toString, !=, getClass, JOB_DAG, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(JOB_TIMELINE, isInstanceOf, ==, ToolTips, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala: Set(SHUFFLE_READ, asInstanceOf, SHUFFLE_WRITE, isInstanceOf, ==, ToolTips, !=, INPUT, ne, OUTPUT)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(BLACKLISTED, SHUFFLE_READ, SHUFFLE_WRITE, ToolTips, toString, INPUT, ne, OUTPUT)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(SHUFFLE_READ, asInstanceOf, PEAK_EXECUTION_MEMORY, SHUFFLE_READ_REMOTE_SIZE, SHUFFLE_READ_BLOCKED_TIME, isInstanceOf, TASK_DESERIALIZATION_TIME, ==, RESULT_SERIALIZATION_TIME, ToolTips, toString, !=, SCHEDULER_DELAY, GETTING_RESULT_TIME, GC_TIME, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(asInstanceOf, isInstanceOf, STAGE_TIMELINE, ==, ToolTips, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(==, ToolTips, APPLICATION_EXECUTOR_LIMIT)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, AppHistoryServerPlugin, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, createListeners, ==, clone, setupUI, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(AppHistoryServerPlugin, asInstanceOf, synchronized, isInstanceOf, createListeners, ==, clone, setupUI, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	incLocalBlocksFetched, notify, totalBlocksFetched, setFetchWaitTime, ShuffleReadMetrics, incRecordsRead, wait, setRemoteBytesRead, _localBytesRead, $asInstanceOf, equals, setMergeValues, asInstanceOf, synchronized, $isInstanceOf, setRemoteBlocksFetched, recordsRead, setRecordsRead, localBytesRead, fetchWaitTime, remoteBytesRead, _remoteBytesRead, TempShuffleReadMetrics, notifyAll, setLocalBlocksFetched, incLocalBytesRead, _fetchWaitTime, setRemoteBytesReadToDisk, isInstanceOf, incRemoteBlocksFetched, <init>, ==, clone, _remoteBlocksFetched, _recordsRead, localBlocksFetched, toString, !=, getClass, incRemoteBytesRead, setLocalBytesRead, remoteBlocksFetched, _localBlocksFetched, ne, incFetchWaitTime, _remoteBytesReadToDisk, incRemoteBytesReadToDisk, eq, totalBytesRead, ##, finalize, hashCode, remoteBytesReadToDisk.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(ShuffleReadMetrics, recordsRead, localBytesRead, fetchWaitTime, remoteBytesRead, <init>, ==, localBlocksFetched, toString, !=, remoteBlocksFetched, eq, remoteBytesReadToDisk)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(ShuffleReadMetrics, equals, asInstanceOf, localBytesRead, remoteBytesRead, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(ShuffleReadMetrics, _localBytesRead, setMergeValues, asInstanceOf, synchronized, _remoteBytesRead, TempShuffleReadMetrics, _fetchWaitTime, <init>, ==, _remoteBlocksFetched, _recordsRead, _localBlocksFetched, ne, _remoteBytesReadToDisk)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(ShuffleReadMetrics, wait, asInstanceOf, synchronized, recordsRead, localBytesRead, fetchWaitTime, remoteBytesRead, notifyAll, isInstanceOf, <init>, ==, localBlocksFetched, !=, remoteBlocksFetched, ne, eq, totalBytesRead, remoteBytesReadToDisk)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(incRecordsRead, asInstanceOf, TempShuffleReadMetrics, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(incLocalBlocksFetched, asInstanceOf, synchronized, TempShuffleReadMetrics, incLocalBytesRead, isInstanceOf, incRemoteBlocksFetched, <init>, ==, toString, !=, incRemoteBytesRead, ne, incFetchWaitTime, incRemoteBytesReadToDisk, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(ShuffleReadMetrics, asInstanceOf, fetchWaitTime, remoteBytesRead, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(incLocalBlocksFetched, ShuffleReadMetrics, incRecordsRead, asInstanceOf, recordsRead, localBytesRead, fetchWaitTime, remoteBytesRead, TempShuffleReadMetrics, incLocalBytesRead, isInstanceOf, incRemoteBlocksFetched, <init>, ==, localBlocksFetched, toString, !=, incRemoteBytesRead, remoteBlocksFetched, ne, incFetchWaitTime, incRemoteBytesReadToDisk, remoteBytesReadToDisk)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, ExecutorThreadDumpPage, equals, prefix, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, renderJson, toString, !=, getClass, render, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala: Set(ExecutorThreadDumpPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, BlockRDD, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, isValid, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, getBlockIdLocations, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, BlockRDDPartition, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, assertValid, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, _locations, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, blockIds, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, blockId, countByValueApprox, removeBlocks, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getAttemptURI, notify, serverInfo, attachSparkUI, webUrl, wait, $asInstanceOf, detachSparkUI, detachHandler, addStaticHandler, equals, getProviderConfig, getBasePath, asInstanceOf, detachTab, initializeLogIfNecessary, getApplicationInfoList, synchronized, withSparkUI, bind, $isInstanceOf, HistoryServer, attachPage, logTrace, attachHandler, publicHostName, isTraceEnabled, main, initializeLogIfNecessary$default$2, emptyListingHtml, handlers, stop, tabs, logName, notifyAll, initialize, isInstanceOf, attachTab, UI_PATH_PREFIX, getSecurityManager, getLastUpdatedTime, <init>, initSecurity, getTabs, getApplicationInfo, getEventLogsUnderProcess, pageToHandlers, ==, clone, getApplicationList, writeEventLogs, securityManager, $init$, maxApplications, removeStaticHandler, toString, cacheMetrics, logError, !=, getHandlers, getClass, createSecurityManager, logWarning, getAppUI, detachPage, ne, eq, log, boundPort, ##, finalize, hashCode, logDebug, sslOptions, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(getAttemptURI, asInstanceOf, synchronized, HistoryServer, initialize, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, getAppUI, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(getProviderConfig, HistoryServer, emptyListingHtml, getLastUpdatedTime, <init>, getEventLogsUnderProcess, getApplicationList, maxApplications, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, delegationTokensRequired, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, serviceName, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, obtainDelegationTokens, getClass, logWarning, ne, eq, HiveDelegationTokenProvider, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(delegationTokensRequired, serviceName, <init>, obtainDelegationTokens, logWarning, HiveDelegationTokenProvider, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	listStatus, notify, StreamFileInputFormat, isSplitable, wait, getConfiguration, $asInstanceOf, StreamRecordReader, setMinSplitSizeRack, equals, parseStream, asInstanceOf, synchronized, createPool, createSplits, $isInstanceOf, getFormatMinSplitSize, getPath, setMaxSplitSize, getSplits, getProgress, setMinPartitions, getBlockIndex, open, setMinSplitSizeNode, notifyAll, initialize, StreamInputFormat, computeSplitSize, isInstanceOf, getCurrentValue, addInputPathRecursively, <init>, PortableDataStream, nextKeyValue, ==, createRecordReader, makeSplit, clone, toArray, getFileBlockLocations, toString, !=, getCurrentKey, getClass, StreamBasedRecordReader, close, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(StreamFileInputFormat, asInstanceOf, getSplits, setMinPartitions, isInstanceOf, <init>, toArray, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf, <init>, PortableDataStream, toArray)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, getPath, isInstanceOf, <init>, PortableDataStream, ==, toArray, toString, !=, getClass, close, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(StreamFileInputFormat, getConfiguration, asInstanceOf, synchronized, getPath, initialize, StreamInputFormat, isInstanceOf, <init>, PortableDataStream, ==, clone, toArray, toString, !=, getClass, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, constructDoubleArray, constructIntArrayFromUShort, ByteArrayConstructor, span, toBuffer, count, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, constructFloatArray, zip, toSet, toJavaArray, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, pairRDDToPython, toList, constructShortArrayFromUByte, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, fold, logTrace, pythonToPairRDD, nonEmpty, isTraceEnabled, initializeLogIfNecessary$default$2, machineCodes, ArrayConstructor, logName, notifyAll, /:, toIterator, addString, initialize, to, collectFirst, drop, isInstanceOf, constructShortArraySigned, filter, constructCharArrayUTF16, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, javaToPython, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, constructIntArrayFromInt32, toSeq, next, zipWithIndex, constructLongArrayFromUInt64, toString, copyToArray, length, seq, logError, !=, collect, getClass, constructLongArrayFromInt64, logWarning, hasDefiniteSize, constructCharArrayUTF32, patch, AutoBatchedPickler, foldLeft, contains, isEmpty, ne, withPartial, constructLongArrayFromUInt32, pythonToJava, reversed, hasNext, construct, indexOf, reduceLeft, SerDeUtil, eq, sum, log, ##, scanLeft, finalize, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, size, map, pairRDDToPython, asInstanceOf, synchronized, mkString, min, logTrace, pythonToPairRDD, isInstanceOf, GroupedIterator, <init>, grouped, flatMap, ==, foreach, exists, toArray, toString, length, !=, collect, getClass, logWarning, isEmpty, ne, SerDeUtil, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryState.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryState.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, stop, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne, ShuffleWriter, eq, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, stop, <init>, ==, !=, ne, ShuffleWriter, write)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, stop, isInstanceOf, <init>, getClass, ShuffleWriter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(stop, <init>, !=, ShuffleWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala: Set(ShuffleWriter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, stop, isInstanceOf, <init>, getClass, ShuffleWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	metricsSystem, notify, wait, $asInstanceOf, SparkEnv, equals, mapOutputTracker, asInstanceOf, initializeLogIfNecessary, set, synchronized, destroyPythonWorker, releasePythonWorker, $isInstanceOf, memoryManager, logTrace, isTraceEnabled, executorId, initializeLogIfNecessary$default$2, stop, logName, driverTmpDir, notifyAll, conf, isInstanceOf, serializerManager, isStopped, <init>, executorSystemName, createDriverEnv$default$5, serializer, ==, clone, securityManager, $init$, hadoopJobMetadata, createExecutorEnv, broadcastManager, toString, shuffleManager, logError, !=, get, environmentDetails, getClass, closureSerializer, logWarning, createDriverEnv, ne, createPythonWorker, blockManager, driverSystemName, rpcEnv, eq, log, outputCommitCoordinator, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(SparkEnv, mapOutputTracker, asInstanceOf, synchronized, logTrace, executorId, stop, isInstanceOf, <init>, ==, clone, toString, logError, !=, get, closureSerializer, logWarning, ne, blockManager, outputCommitCoordinator, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(SparkEnv, asInstanceOf, synchronized, executorId, stop, conf, isInstanceOf, <init>, ==, securityManager, toString, logError, !=, get, logWarning, ne, blockManager, rpcEnv, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(SparkEnv, mapOutputTracker, asInstanceOf, set, synchronized, executorId, conf, isInstanceOf, <init>, ==, toString, logError, !=, get, closureSerializer, logWarning, ne, blockManager, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(SparkEnv, asInstanceOf, stop, conf, isInstanceOf, <init>, ==, toString, logError, !=, get, logWarning, rpcEnv, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(SparkEnv, asInstanceOf, stop, <init>, ==, shuffleManager, !=, get, closureSerializer, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala: Set(SparkEnv, asInstanceOf, set, isInstanceOf, <init>, ==, clone, toString, !=, get, getClass, closureSerializer, logWarning, ne, eq, log, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(SparkEnv, synchronized, conf, <init>, toString, logError, !=, get, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(SparkEnv, asInstanceOf, set, executorId, stop, conf, isInstanceOf, <init>, serializer, ==, createExecutorEnv, logError, !=, get, closureSerializer, logWarning, ne, rpcEnv, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala: Set(SparkEnv, driverTmpDir, <init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(metricsSystem, SparkEnv, asInstanceOf, set, synchronized, executorId, stop, conf, isInstanceOf, <init>, ==, clone, securityManager, broadcastManager, toString, logError, !=, get, environmentDetails, getClass, logWarning, createDriverEnv, ne, blockManager, rpcEnv, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(SparkEnv, asInstanceOf, isInstanceOf, <init>, serializer, ==, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(SparkEnv, asInstanceOf, conf, isInstanceOf, <init>, serializer, toString, !=, get, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(SparkEnv, equals, asInstanceOf, set, synchronized, logTrace, conf, isInstanceOf, <init>, serializer, ==, toString, logError, !=, get, getClass, logWarning, ne, rpcEnv, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(SparkEnv, asInstanceOf, set, destroyPythonWorker, releasePythonWorker, logTrace, conf, isInstanceOf, serializerManager, <init>, ==, logError, !=, get, logWarning, ne, createPythonWorker, blockManager, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(SparkEnv, conf, <init>, logError, get, outputCommitCoordinator, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala: Set(SparkEnv, asInstanceOf, isInstanceOf, <init>, serializer, ==, toString, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(SparkEnv, equals, asInstanceOf, set, isInstanceOf, <init>, toString, logError, !=, get, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala: Set(SparkEnv, asInstanceOf, conf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(SparkEnv, synchronized, conf, <init>, ==, logError, !=, get, logWarning, blockManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(SparkEnv, asInstanceOf, synchronized, conf, isInstanceOf, <init>, serializer, ==, broadcastManager, !=, get, ne, blockManager, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(SparkEnv, asInstanceOf, stop, conf, isInstanceOf, <init>, ==, toString, get, rpcEnv, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(SparkEnv, asInstanceOf, stop, conf, isInstanceOf, <init>, serializer, get, getClass, logWarning, blockManager, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(SparkEnv, asInstanceOf, set, synchronized, conf, isInstanceOf, <init>, serializer, ==, clone, toString, !=, get, getClass, closureSerializer, logWarning, ne, blockManager, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(SparkEnv, stop, conf, <init>, serializer, logError, !=, get, blockManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(SparkEnv, asInstanceOf, set, synchronized, conf, isInstanceOf, <init>, serializer, ==, toString, logError, !=, get, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(SparkEnv, mapOutputTracker, asInstanceOf, synchronized, memoryManager, logTrace, executorId, stop, conf, isInstanceOf, serializerManager, <init>, serializer, ==, securityManager, toString, shuffleManager, logError, !=, get, getClass, logWarning, ne, blockManager, rpcEnv, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(SparkEnv, <init>, get, blockManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(SparkEnv, asInstanceOf, synchronized, conf, serializerManager, <init>, serializer, ==, !=, get, getClass, logWarning, ne, blockManager, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala: Set(SparkEnv, conf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(SparkEnv, asInstanceOf, set, conf, isInstanceOf, <init>, serializer, ==, toString, !=, get, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala: Set(SparkEnv, asInstanceOf, conf, isInstanceOf, <init>, serializer, ==, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(SparkEnv, asInstanceOf, synchronized, conf, isInstanceOf, serializerManager, <init>, serializer, ==, toString, !=, get, logWarning, ne, blockManager, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(SparkEnv, asInstanceOf, stop, conf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(SparkEnv, asInstanceOf, <init>, serializer, shuffleManager, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(SparkEnv, asInstanceOf, conf, isInstanceOf, <init>, serializer, ==, get, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, SparkEnv, asInstanceOf, synchronized, logTrace, conf, isInstanceOf, serializerManager, <init>, ==, toString, !=, get, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, SparkEnv, mapOutputTracker, asInstanceOf, synchronized, executorId, stop, conf, isInstanceOf, <init>, ==, toString, logError, !=, get, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(metricsSystem, wait, SparkEnv, mapOutputTracker, asInstanceOf, set, synchronized, memoryManager, executorId, stop, notifyAll, conf, isInstanceOf, serializerManager, <init>, serializer, ==, securityManager, logError, !=, get, closureSerializer, logWarning, ne, blockManager, rpcEnv, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(SparkEnv, mapOutputTracker, asInstanceOf, stop, conf, isInstanceOf, serializerManager, <init>, serializer, ==, get, ne, blockManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(SparkEnv, asInstanceOf, isInstanceOf, <init>, ==, toString, get, blockManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(metricsSystem, SparkEnv, synchronized, memoryManager, notifyAll, conf, <init>, !=, get, closureSerializer, blockManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(SparkEnv, asInstanceOf, <init>, ==, get, closureSerializer, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(SparkEnv, mapOutputTracker, asInstanceOf, synchronized, conf, isInstanceOf, <init>, ==, broadcastManager, toString, logError, blockManager, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(SparkEnv, equals, asInstanceOf, isInstanceOf, <init>, serializer, ==, toString, shuffleManager, get, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(SparkEnv, mapOutputTracker, asInstanceOf, isInstanceOf, <init>, ==, shuffleManager, logError, !=, get, blockManager, rpcEnv, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala: Set(SparkEnv, asInstanceOf, conf, isInstanceOf, <init>, ==, !=, get, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(SparkEnv, asInstanceOf, conf, isInstanceOf, isStopped, <init>, serializer, ==, logError, !=, get, closureSerializer, ne, blockManager, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(SparkEnv, mapOutputTracker, asInstanceOf, serializerManager, <init>, serializer, shuffleManager, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(SparkEnv, asInstanceOf, isInstanceOf, <init>, ==, shuffleManager, !=, get, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(SparkEnv, asInstanceOf, logTrace, executorId, conf, isInstanceOf, <init>, ==, toString, !=, logWarning, ne, rpcEnv, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(SparkEnv, conf, <init>, ==, !=, get, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(SparkEnv, equals, asInstanceOf, set, synchronized, conf, isInstanceOf, <init>, ==, hadoopJobMetadata, toString, !=, get, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	topologyFile, notify, wait, $asInstanceOf, equals, getTopologyForHost, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, FileBasedTopologyMapper, isInstanceOf, DefaultTopologyMapper, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, TopologyMapper, ne, eq, log, ##, finalize, hashCode, topologyMap, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(getTopologyForHost, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, TopologyMapper, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, prevPageSizeFormField, wait, pageLink, copy$default$2, $asInstanceOf, pageSize, memoryUsed, copy$default$5, pageSizeFormField, diskUsed, productArity, pageNumberFormField, equals, prefix, asInstanceOf, synchronized, pageData, executors, $isInstanceOf, canEqual, row, copy$default$4, tableCssClass, dataSource, productPrefix, notifyAll, goButtonFormPath, isInstanceOf, <init>, BlockTableRowData, ==, BlockPagedTable, sliceData, clone, BlockDataSource, $init$, renderJson, tableId, copy$default$3, copy, blockName, toString, !=, getClass, copy$default$1, headers, pageNavigation, render, ne, RDDPage, dataSize, eq, productIterator, storageLevel, ##, finalize, table, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala: Set(<init>, RDDPage)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, AbstractApplicationResource, stages, getEventLogs, wait, getApp, $asInstanceOf, rddList, equals, withUI, httpRequest, asInstanceOf, synchronized, rddData, $isInstanceOf, executorList, notifyAll, isInstanceOf, applicationAttempt, <init>, attemptId, getAttempt, ==, jobsList, clone, OneApplicationResource, uiRoot, allExecutorList, $init$, servletContext, toString, !=, getClass, appId, ne, oneJob, environmentInfo, eq, ##, finalize, hashCode, OneApplicationAttemptResource.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(httpRequest, asInstanceOf, isInstanceOf, <init>, attemptId, ==, OneApplicationResource, uiRoot, servletContext, toString, getClass, appId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, stageAttemptId, _executorDeserializeTime, wait, collectAccumulatorUpdates$default$1, $asInstanceOf, epoch, equals, stageId, asInstanceOf, context, collectAccumulatorUpdates, run, synchronized, $isInstanceOf, reasonIfKilled, notifyAll, partitionId, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, <init>, ==, ResultTask, appAttemptId, clone, setTaskMemoryManager, jobId, toString, preferredLocations, metrics, !=, getClass, appId, runTask, localProperties, kill, _executorDeserializeCpuTime, ne, eq, outputId, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(stageAttemptId, epoch, stageId, asInstanceOf, synchronized, partitionId, isInstanceOf, <init>, ==, ResultTask, clone, jobId, toString, preferredLocations, !=, ne, outputId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, OneToOneDependency, partitioner, combinerClassName, wait, $asInstanceOf, aggregator, equals, asInstanceOf, shuffleId, keyClassName, synchronized, $isInstanceOf, valueClassName, RangeDependency, notifyAll, keyOrdering, isInstanceOf, rdd, shuffleHandle, <init>, serializer, NarrowDependency, ==, clone, getParents, toString, !=, mapSideCombine, getClass, ShuffleDependency, ne, Dependency, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(OneToOneDependency, partitioner, aggregator, asInstanceOf, synchronized, keyOrdering, isInstanceOf, rdd, <init>, serializer, NarrowDependency, ==, clone, toString, !=, getClass, ShuffleDependency, ne, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(partitioner, asInstanceOf, isInstanceOf, rdd, <init>, serializer, ==, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(partitioner, asInstanceOf, isInstanceOf, rdd, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, shuffleId, synchronized, isInstanceOf, rdd, <init>, ==, clone, getParents, toString, !=, ShuffleDependency, ne, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, rdd, shuffleHandle, <init>, ==, !=, ShuffleDependency, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(asInstanceOf, rdd, <init>, !=, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(partitioner, asInstanceOf, synchronized, isInstanceOf, rdd, <init>, ==, clone, toString, !=, getClass, ShuffleDependency, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(asInstanceOf, isInstanceOf, rdd, <init>, serializer, ==, ne, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala: Set(shuffleId, <init>, ShuffleDependency)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(OneToOneDependency, partitioner, asInstanceOf, rdd, <init>, ==, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala: Set(asInstanceOf, rdd, <init>, NarrowDependency, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(partitioner, aggregator, asInstanceOf, shuffleId, isInstanceOf, <init>, serializer, getClass, ShuffleDependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(OneToOneDependency, partitioner, aggregator, asInstanceOf, synchronized, keyOrdering, isInstanceOf, rdd, <init>, serializer, NarrowDependency, ==, clone, toString, !=, getClass, ShuffleDependency, ne, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(partitioner, aggregator, shuffleId, keyOrdering, <init>, serializer, !=, mapSideCombine, ShuffleDependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(OneToOneDependency, partitioner, equals, asInstanceOf, rdd, <init>, ==, ne, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala: Set(asInstanceOf, rdd, <init>, NarrowDependency, ne, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, isInstanceOf, rdd, <init>, ==, toString, ShuffleDependency, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, shuffleId, rdd, <init>, ==, !=, ShuffleDependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(partitioner, equals, asInstanceOf, synchronized, isInstanceOf, rdd, <init>, ==, toString, !=, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala: Set(rdd, <init>, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(aggregator, asInstanceOf, shuffleId, keyOrdering, isInstanceOf, <init>, serializer, ==, mapSideCombine, ShuffleDependency, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, rdd, <init>, ==, toString, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, shuffleId, synchronized, isInstanceOf, rdd, <init>, ==, toString, ShuffleDependency, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(OneToOneDependency, partitioner, equals, asInstanceOf, isInstanceOf, rdd, shuffleHandle, <init>, serializer, ==, toString, ShuffleDependency, ne, Dependency, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala: Set(ShuffleDependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(partitioner, wait, asInstanceOf, shuffleId, synchronized, notifyAll, isInstanceOf, rdd, <init>, ==, toString, !=, ShuffleDependency, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(partitioner, aggregator, asInstanceOf, keyOrdering, rdd, shuffleHandle, <init>, serializer, mapSideCombine, ShuffleDependency, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(asInstanceOf, isInstanceOf, rdd, <init>, NarrowDependency, ==, toString, ne, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(OneToOneDependency, partitioner, asInstanceOf, isInstanceOf, rdd, shuffleHandle, <init>, ==, !=, ShuffleDependency, ne, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala: Set(rdd, <init>, toString, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(asInstanceOf, RangeDependency, rdd, <init>, ne, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(partitioner, equals, asInstanceOf, synchronized, isInstanceOf, rdd, <init>, ==, toString, !=, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala: Set(rdd, <init>, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(OneToOneDependency, partitioner, aggregator, asInstanceOf, synchronized, keyOrdering, isInstanceOf, rdd, <init>, serializer, NarrowDependency, ==, clone, toString, !=, getClass, ShuffleDependency, ne, Dependency)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	masters, notify, wait, supervise, $asInstanceOf, equals, cores, asInstanceOf, isValidJarUrl, synchronized, jarUrl, $isInstanceOf, DEFAULT_MEMORY, mainClass, notifyAll, cmd, isInstanceOf, DEFAULT_SUPERVISE, <init>, ==, clone, DEFAULT_CORES, toString, !=, driverOptions, getClass, ClientArguments, ne, eq, logLevel, ##, finalize, hashCode, memory, driverId.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(masters, supervise, cores, asInstanceOf, jarUrl, mainClass, cmd, isInstanceOf, <init>, ==, driverOptions, ClientArguments, ne, logLevel, memory, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, DEFAULT_MEMORY, mainClass, isInstanceOf, DEFAULT_SUPERVISE, <init>, DEFAULT_CORES, toString, ClientArguments, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function0.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function0.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function0.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function0.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function0.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, Function0.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function0.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unpersist, wait, $asInstanceOf, equals, removeCheckpointFile, deleteAllCheckpointsButLast, unpersistDataSet, asInstanceOf, initializeLogIfNecessary, synchronized, sc, $isInstanceOf, PeriodicCheckpointer, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, persist, <init>, isCheckpointed, checkpointInterval, ==, clone, $init$, checkpoint, getCheckpointFiles, toString, logError, !=, getClass, logWarning, update, deleteAllCheckpoints, ne, getAllCheckpointFiles, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala: Set(unpersist, sc, PeriodicCheckpointer, persist, <init>, isCheckpointed, checkpointInterval, ==, checkpoint)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/Pseudorandom.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/Pseudorandom.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/Pseudorandom.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/Pseudorandom.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/Pseudorandom.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/Pseudorandom.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, setSeed, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, Pseudorandom, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala: Set(asInstanceOf, setSeed, ==, Pseudorandom)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(asInstanceOf, setSeed, clone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, setSeed, isInstanceOf, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(asInstanceOf, setSeed, clone)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/SamplingUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/SamplingUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/SamplingUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/SamplingUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/SamplingUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, PoissonBounds, synchronized, $isInstanceOf, reservoirSampleAndCount$default$3, minSamplingRate, BinomialBounds, notifyAll, getLowerBound, isInstanceOf, computeFractionForSampleSize, ==, clone, SamplingUtils, toString, !=, getClass, ne, getUpperBound, eq, reservoirSampleAndCount, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, isInstanceOf, ==, SamplingUtils, ne, reservoirSampleAndCount, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, computeFractionForSampleSize, ==, clone, SamplingUtils, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala: Set(PoissonBounds, BinomialBounds, getLowerBound, ==, ne, getUpperBound)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, readExternal, wait, writeExternal, $asInstanceOf, location, HighlyCompressedMapStatus, equals, asInstanceOf, synchronized, CompressedMapStatus, $isInstanceOf, decompressSize, notifyAll, compressSize, isInstanceOf, <init>, apply, ==, MapStatus, clone, toString, !=, getClass, getSizeForBlock, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(location, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, MapStatus, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, <init>, apply, ==, MapStatus, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleWriter.scala: Set(<init>, MapStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>, apply, MapStatus, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, location, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, apply, ==, MapStatus, toString, !=, getSizeForBlock, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(HighlyCompressedMapStatus, asInstanceOf, CompressedMapStatus, isInstanceOf, <init>, apply, ==, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	removeRdd, notify, toByteBuffer, shuffleMetricsSource, blockTransferService, shuffleServerId, wait, putSingle$default$4, $asInstanceOf, BlockManager, size, getBlockData, registerTask, getLocalBytes, removeBroadcast, master, equals, getMatchingBlockIds, releaseLock$default$2, dropFromMemory, asInstanceOf, initializeLogIfNecessary, diskStore, data, getLocalValues, synchronized, BlockResult, getStatus, toChunkedByteBuffer, $isInstanceOf, releaseLockAndDispose, releaseLockAndDispose$default$3, blockInfoManager, removeBlock, buffer, logTrace, getOrElseUpdate, blockIdsToHosts, isTraceEnabled, initializeLogIfNecessary$default$2, stop, getSingle, putBytes, logName, notifyAll, blockIdsToHosts$default$3, conf, initialize, isInstanceOf, serializerManager, toNetty, putBytes$default$4, downgradeLock, diskBlockManager, blockManagerId, bytes, <init>, shuffleClient, removeBlock$default$2, ==, dispose, clone, replicateBlock, ByteBufferBlockData, shouldDispose, remoteBlockTempFileManager, reregister, waitForAsyncReregister, $init$, putIterator$default$4, readMethod, getDiskWriter, memoryStore, toString, releaseAllLocksForTask, externalShuffleServiceEnabled, logError, !=, get, putBlockData, getClass, releaseLock, logWarning, toInputStream, createTempFile, RemoteBlockDownloadFileManager, ne, encryptionKey, registerTempFileToClean, eq, getRemoteBytes, BlockData, log, putIterator, ##, finalize, hashCode, logDebug, logInfo, putSingle.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(BlockManager, size, master, asInstanceOf, synchronized, logTrace, getOrElseUpdate, stop, isInstanceOf, blockManagerId, <init>, ==, clone, toString, externalShuffleServiceEnabled, logError, !=, get, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(BlockManager, size, master, asInstanceOf, data, synchronized, stop, conf, isInstanceOf, <init>, ==, toString, logError, !=, get, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(BlockManager, size, asInstanceOf, synchronized, getOrElseUpdate, conf, isInstanceOf, <init>, ==, toString, externalShuffleServiceEnabled, logError, !=, get, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(removeRdd, BlockManager, size, master, asInstanceOf, synchronized, stop, conf, initialize, isInstanceOf, blockManagerId, bytes, <init>, ==, clone, toString, logError, !=, get, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(BlockManager, size, asInstanceOf, logTrace, conf, isInstanceOf, serializerManager, diskBlockManager, <init>, ==, logError, !=, get, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala: Set(toByteBuffer, size, data, blockInfoManager, buffer, toNetty, <init>, ==, dispose, toInputStream, BlockData)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala: Set(BlockManager, master, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(toByteBuffer, size, asInstanceOf, buffer, conf, isInstanceOf, toNetty, bytes, <init>, ==, toString, !=, get, logWarning, toInputStream, BlockData, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(BlockManager, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(blockTransferService, BlockManager, master, asInstanceOf, synchronized, getOrElseUpdate, stop, conf, isInstanceOf, serializerManager, <init>, ==, toString, !=, get, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(BlockManager, data, synchronized, buffer, conf, diskBlockManager, <init>, ==, logError, !=, get, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(BlockManager, getLocalBytes, removeBroadcast, master, asInstanceOf, data, getLocalValues, synchronized, BlockResult, toChunkedByteBuffer, putBytes, conf, isInstanceOf, bytes, <init>, ==, dispose, ByteBufferBlockData, !=, get, releaseLock, toInputStream, ne, getRemoteBytes, BlockData, logDebug, logInfo, putSingle)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(BlockManager, asInstanceOf, stop, conf, isInstanceOf, <init>, get, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(BlockManager, size, asInstanceOf, data, synchronized, BlockResult, getOrElseUpdate, conf, isInstanceOf, bytes, <init>, ==, clone, toString, !=, get, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(shuffleServerId, BlockManager, stop, conf, <init>, logError, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(BlockManager, master, <init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(BlockManager, size, asInstanceOf, synchronized, buffer, conf, serializerManager, diskBlockManager, <init>, ==, getDiskWriter, !=, get, getClass, logWarning, ne, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(BlockManager, size, asInstanceOf, data, synchronized, buffer, conf, isInstanceOf, serializerManager, diskBlockManager, <init>, ==, getDiskWriter, toString, !=, get, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(shuffleMetricsSource, wait, BlockManager, size, asInstanceOf, synchronized, stop, putBytes, notifyAll, conf, initialize, isInstanceOf, serializerManager, blockManagerId, <init>, ==, reregister, releaseAllLocksForTask, logError, !=, get, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(BlockManager, asInstanceOf, stop, conf, isInstanceOf, serializerManager, <init>, shuffleClient, ==, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(BlockManager, master, asInstanceOf, data, BlockResult, removeBlock, blockIdsToHosts, isInstanceOf, <init>, ==, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(BlockManager, size, getBlockData, asInstanceOf, synchronized, toChunkedByteBuffer, buffer, logTrace, initialize, isInstanceOf, diskBlockManager, blockManagerId, <init>, shuffleClient, ==, toString, logError, !=, logWarning, toInputStream, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(BlockManager, registerTask, synchronized, notifyAll, conf, <init>, memoryStore, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(BlockManager, master, asInstanceOf, synchronized, conf, isInstanceOf, <init>, ==, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(BlockManager, equals, asInstanceOf, isInstanceOf, <init>, ==, toString, get, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(removeRdd, BlockManager, removeBroadcast, getMatchingBlockIds, asInstanceOf, getStatus, removeBlock, isInstanceOf, <init>, ==, replicateBlock, logError, !=, get, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(toByteBuffer, BlockManager, size, master, asInstanceOf, removeBlock, conf, isInstanceOf, <init>, ==, logError, !=, get, ne, getRemoteBytes, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(toByteBuffer, blockTransferService, shuffleServerId, BlockManager, size, getBlockData, registerTask, getLocalBytes, master, asInstanceOf, diskStore, data, getLocalValues, synchronized, BlockResult, toChunkedByteBuffer, releaseLockAndDispose, blockInfoManager, removeBlock, buffer, logTrace, getOrElseUpdate, stop, putBytes, conf, isInstanceOf, serializerManager, toNetty, downgradeLock, diskBlockManager, blockManagerId, bytes, <init>, shuffleClient, ==, dispose, ByteBufferBlockData, shouldDispose, remoteBlockTempFileManager, reregister, readMethod, memoryStore, toString, releaseAllLocksForTask, externalShuffleServiceEnabled, logError, !=, get, getClass, releaseLock, logWarning, toInputStream, RemoteBlockDownloadFileManager, ne, encryptionKey, getRemoteBytes, BlockData, putIterator, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getAllFiles, notify, subDirsPerLocalDir, wait, $asInstanceOf, createTempLocalBlock, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, DiskBlockManager, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, getFile, containsBlock, ==, clone, createTempShuffleBlock, $init$, toString, logError, !=, getClass, getAllBlocks, logWarning, localDirs, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, DiskBlockManager, logTrace, isInstanceOf, <init>, ==, logError, !=, logWarning, localDirs, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, DiskBlockManager, isInstanceOf, <init>, getFile, ==, toString, !=, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, DiskBlockManager, <init>, getFile, ==, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(subDirsPerLocalDir, createTempLocalBlock, asInstanceOf, synchronized, DiskBlockManager, logTrace, stop, isInstanceOf, <init>, ==, toString, logError, !=, getClass, getAllBlocks, logWarning, localDirs, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(createTempLocalBlock, asInstanceOf, synchronized, DiskBlockManager, <init>, ==, !=, getClass, logWarning, ne, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, DiskBlockManager, isInstanceOf, <init>, ==, createTempShuffleBlock, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(createTempLocalBlock, asInstanceOf, synchronized, DiskBlockManager, logTrace, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, askSync, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, RpcEndpointRef, $init$, address, toString, logError, !=, getClass, ask, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo, send.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(name, askSync, <init>, ==, RpcEndpointRef, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(name, asInstanceOf, askSync, synchronized, logTrace, isInstanceOf, <init>, ==, clone, RpcEndpointRef, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, askSync, synchronized, isInstanceOf, <init>, ==, RpcEndpointRef, address, toString, logError, !=, ask, logWarning, ne, logDebug, logInfo, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(name, asInstanceOf, isInstanceOf, <init>, ==, RpcEndpointRef, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, askSync, isInstanceOf, <init>, ==, RpcEndpointRef, address, logError, !=, ask, logWarning, ne, log, logInfo, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(name, askSync, <init>, ==, RpcEndpointRef, toString, ask)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, RpcEndpointRef, toString, logError, ask, eq, logDebug, logInfo, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(name, asInstanceOf, askSync, synchronized, isInstanceOf, <init>, ==, clone, RpcEndpointRef, toString, logError, !=, getClass, ask, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(askSync, <init>, ==, RpcEndpointRef)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, RpcEndpointRef, address, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(name, asInstanceOf, synchronized, isInstanceOf, <init>, ==, RpcEndpointRef, address, toString, logError, !=, logWarning, ne, log, logDebug, logInfo, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(name, asInstanceOf, isInstanceOf, <init>, ==, RpcEndpointRef, address, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(name, asInstanceOf, synchronized, isInstanceOf, <init>, ==, RpcEndpointRef, address, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, RpcEndpointRef, toString, ask, eq, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(name, asInstanceOf, synchronized, isInstanceOf, <init>, ==, RpcEndpointRef, address, logError, !=, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, RpcEndpointRef, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, RpcEndpointRef, toString, logError, !=, ask, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(name, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, RpcEndpointRef, address, toString, logError, !=, ask, logWarning, ne, eq, logDebug, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, ==, RpcEndpointRef, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(<init>, ==, RpcEndpointRef, toString, logError, !=, logWarning, logInfo, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala: Set(asInstanceOf, <init>, RpcEndpointRef, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala: Set(asInstanceOf, <init>, RpcEndpointRef)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala: Set(<init>, ==, RpcEndpointRef)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala: Set(name, <init>, RpcEndpointRef)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(askSync, <init>, ==, RpcEndpointRef, logError, !=, ask, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(name, asInstanceOf, askSync, isInstanceOf, <init>, ==, RpcEndpointRef, address, toString, logError, !=, logWarning, ne, log, logInfo, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(name, wait, asInstanceOf, askSync, synchronized, notifyAll, isInstanceOf, <init>, ==, RpcEndpointRef, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, askSync, isInstanceOf, <init>, RpcEndpointRef, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, RpcEndpointRef, address, !=, ask, logWarning, ne, logInfo, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, askSync, synchronized, notifyAll, isInstanceOf, <init>, ==, RpcEndpointRef, toString, logError, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, logTrace, isInstanceOf, <init>, ==, RpcEndpointRef, toString, !=, ask, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, <init>, ==, RpcEndpointRef, !=, logWarning, logInfo, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, RpcEndpointRef, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, askSync, isInstanceOf, <init>, ==, RpcEndpointRef, logError, ask, logWarning, ne, logInfo, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, RpcEndpointRef, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala: Set(asInstanceOf, <init>, ==, RpcEndpointRef, address, logError, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(name, askSync, <init>, ==, RpcEndpointRef)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, address, toString, logError, !=, logWarning, ne, eq, logDebug, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(name, asInstanceOf, isInstanceOf, <init>, ==, RpcEndpointRef, address, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(name, asInstanceOf, synchronized, isInstanceOf, <init>, ==, RpcEndpointRef, address, logError, !=, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala: Set(<init>, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala: Set(<init>, ==, address, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, prefix, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, renderJson, toString, !=, getClass, PoolPage, render, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(<init>, ==, toString, PoolPage)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, DEFAULT_LOG_DIR, $asInstanceOf, equals, asInstanceOf, MAX_LOG_AGE_S, synchronized, $isInstanceOf, config, notifyAll, isInstanceOf, ==, MAX_LOCAL_DISK_USAGE, clone, HISTORY_SERVER_UI_PORT, EVENT_LOG_DIR, toString, !=, getClass, LOCAL_STORE_DIR, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(DEFAULT_LOG_DIR, asInstanceOf, MAX_LOG_AGE_S, synchronized, config, isInstanceOf, ==, clone, EVENT_LOG_DIR, toString, !=, getClass, LOCAL_STORE_DIR, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, MAX_LOG_AGE_S, config, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(asInstanceOf, synchronized, config, isInstanceOf, ==, MAX_LOCAL_DISK_USAGE, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, config, isInstanceOf, HISTORY_SERVER_UI_PORT, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, seed, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, PartitionwiseSampledRDDPartition, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, prev, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, PartitionwiseSampledRDD, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(creationSite, zipPartitions, parent, isLocallyCheckpointed, getOrCompute, partitioner, coalesce, name, count, isCheckpointedAndMaterialized, mapPartitions, union, seed, map, subtract, intersection, scope, asInstanceOf, context, getPreferredLocations, doCheckpoint, synchronized, aggregate, compute, min, fold, getOutputDeterministicLevel, iterator, conf, isInstanceOf, filter, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, ++, flatMap, take, PartitionwiseSampledRDD, groupBy, ==, clone, distinct, retag, foreach, reduce, zipWithIndex, checkpoint, elementClassTag, sample, toString, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, clearDependencies, isEmpty, ne, getDependencies, mapPartitionsWithIndex, keyBy, getCreationSite, computeOrReadCheckpoint, dependencies, isReliablyCheckpointed, withScope, log, treeAggregate, index, takeOrdered, firstParent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, computeIfAbsent, wait, replace, $asInstanceOf, size, equals, computeIfPresent, clear, asInstanceOf, containsKey, synchronized, mapAsSerializableJavaMap, $isInstanceOf, compute, getOrDefault, forEach, JavaUtils, notifyAll, keySet, isInstanceOf, containsValue, optionToOptional, putIfAbsent, <init>, merge, remove, ==, clone, putAll, put, values, toString, SerializableMapWrapper, !=, get, getClass, isEmpty, replaceAll, ne, entrySet, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf, JavaUtils, optionToOptional, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(asInstanceOf, mapAsSerializableJavaMap, JavaUtils, optionToOptional, <init>, SerializableMapWrapper, isEmpty)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(asInstanceOf, mapAsSerializableJavaMap, JavaUtils, optionToOptional, <init>, SerializableMapWrapper, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, dependency, wait, $asInstanceOf, equals, asInstanceOf, shuffleId, synchronized, BaseShuffleHandle, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, numMaps, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(dependency, shuffleId, BaseShuffleHandle, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(dependency, asInstanceOf, shuffleId, BaseShuffleHandle, isInstanceOf, <init>, numMaps, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(dependency, asInstanceOf, shuffleId, BaseShuffleHandle, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, clearFailures, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, makeNewStageAttempt, parents, synchronized, latestInfo, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, rdd, findMissingPartitions, <init>, id, ==, clone, numTasks, $init$, details, toString, logError, firstJobId, !=, callSite, getClass, logWarning, jobIds, Stage, ne, numPartitions, eq, log, ##, finalize, hashCode, logDebug, makeNewStageAttempt$default$2, logInfo, fetchFailedAttemptIds.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(name, clearFailures, asInstanceOf, makeNewStageAttempt, parents, synchronized, latestInfo, logTrace, isInstanceOf, rdd, findMissingPartitions, <init>, id, ==, clone, numTasks, toString, logError, firstJobId, !=, callSite, logWarning, jobIds, Stage, ne, numPartitions, logDebug, logInfo, fetchFailedAttemptIds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf, rdd, <init>, callSite, Stage, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf, rdd, <init>, callSite, Stage, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(name, clearFailures, asInstanceOf, makeNewStageAttempt, parents, synchronized, latestInfo, logTrace, isInstanceOf, rdd, findMissingPartitions, <init>, id, ==, clone, numTasks, toString, logError, firstJobId, !=, callSite, logWarning, jobIds, Stage, ne, numPartitions, logDebug, logInfo, fetchFailedAttemptIds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala: Set(name, parents, rdd, <init>, id, numTasks, details, Stage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala: Set(asInstanceOf, parents, rdd, <init>, id, firstJobId, callSite, Stage, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, parents, rdd, findMissingPartitions, <init>, id, ==, numTasks, firstJobId, !=, callSite, Stage, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(name, clearFailures, asInstanceOf, makeNewStageAttempt, parents, synchronized, latestInfo, logTrace, isInstanceOf, rdd, findMissingPartitions, <init>, id, ==, clone, numTasks, toString, logError, firstJobId, !=, callSite, logWarning, jobIds, Stage, ne, numPartitions, logDebug, logInfo, fetchFailedAttemptIds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf, rdd, <init>, callSite, Stage, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newTaskTempFile, notify, wait, $asInstanceOf, setupJob, equals, newTaskTempFileAbsPath, setupCommitter, abortJob, abortTask, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, commitJob, <init>, commitTask, ==, clone, $init$, deleteWithJob, toString, logError, !=, getClass, logWarning, HadoopMapReduceCommitProtocol, setupTask, ne, eq, log, onTaskCommit, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(setupJob, abortJob, abortTask, asInstanceOf, isInstanceOf, commitJob, <init>, commitTask, ==, toString, logError, !=, getClass, logWarning, HadoopMapReduceCommitProtocol, setupTask, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala: Set(asInstanceOf, <init>, HadoopMapReduceCommitProtocol)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala: Set(<init>, HadoopMapReduceCommitProtocol)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, f, sortBy, pipe$default$6, doCheckpoint, ZippedPartitionsPartition, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, rdd2, ZippedPartitionsRDD2, mapPartitions$default$2, ZippedPartitionsBaseRDD, min, getCheckpointFile, fold, rdd1, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, ZippedPartitionsRDD4, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, rdd4, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, ZippedPartitionsRDD3, toString, mapPartitionsInternal, preferredLocations, rdds, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, partitionValues, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, rdd3, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(creationSite, zipPartitions, parent, isLocallyCheckpointed, getOrCompute, partitioner, coalesce, name, count, isCheckpointedAndMaterialized, mapPartitions, union, map, subtract, intersection, scope, asInstanceOf, context, getPreferredLocations, f, doCheckpoint, synchronized, aggregate, compute, rdd2, ZippedPartitionsRDD2, min, fold, getOutputDeterministicLevel, iterator, conf, isInstanceOf, filter, ZippedPartitionsRDD4, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, ++, flatMap, take, groupBy, ==, clone, distinct, retag, foreach, rdd4, reduce, zipWithIndex, checkpoint, elementClassTag, sample, ZippedPartitionsRDD3, toString, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, clearDependencies, isEmpty, ne, getDependencies, mapPartitionsWithIndex, rdd3, keyBy, getCreationSite, computeOrReadCheckpoint, dependencies, isReliablyCheckpointed, withScope, log, treeAggregate, index, takeOrdered, firstParent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, CachedQuantile, duration, shuffleRecordsWritten, skippedStages, stageAttemptId, taskCount, stage, name, cached, resultSerializationTime, diskSize, wait, toRDDOperationGraph, MEM_SPILL, HOST, SHUFFLE_READ_TIME, LAUNCH_TIME, shuffleWriteTime, $asInstanceOf, outgoingEdges, numCompletedJobs, ExecutorSummaryWrapper, errorMessage, shuffleRemoteBytesReadToDisk, DISK_SPILL, productArity, shuffleRemoteBytesRead, recordsWritten, equals, JobDataWrapper, hostPort, stageId, shuffleRecordsRead, ATTEMPT, peakExecutionMemory, SHUFFLE_TOTAL_BLOCKS, SER_TIME, asInstanceOf, EXECUTOR, host, bytesWritten, SHUFFLE_REMOTE_BLOCKS, synchronized, childClusters, diskBytesSpilled, SHUFFLE_WRITE_TIME, $isInstanceOf, EXEC_RUN_TIME, deserialized, shuffleLocalBlocksFetched, RDDOperationGraphWrapper, SHUFFLE_WRITE_RECORDS, recordsRead, STATUS, inputRecordsRead, DESER_TIME, ERROR, TaskIndexNames, INPUT_SIZE, ApplicationInfoWrapper, speculative, SHUFFLE_READ_RECORDS, canEqual, toApi, EXEC_CPU_TIME, hasMetrics, LOCALITY, rootCluster, executorId, productPrefix, shuffleWriteBytes, info, notifyAll, shuffleReadBytes, RDDOperationClusterWrapper, schedulerDelay, key, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, useDisk, PEAK_MEM, inputBytesRead, RDDStorageInfoWrapper, toRDDOperationCluster, bytesRead, version, quantile, <init>, OUTPUT_SIZE, id, ExecutorStageSummaryWrapper, useMemory, AppSummary, ==, ApplicationEnvironmentInfoWrapper, SHUFFLE_WRITE_SIZE, clone, status, outputBytesWritten, memoryBytesSpilled, stageIds, launchTime, $init$, INPUT_RECORDS, SHUFFLE_LOCAL_BLOCKS, RESULT_SIZE, copy, SHUFFLE_REMOTE_READS, outputRecordsWritten, toString, STAGE, jvmGcTime, !=, SHUFFLE_TOTAL_READS, SCHEDULER_DELAY, DURATION, GETTING_RESULT_TIME, memSize, executorCpuTime, TaskDataWrapper, edges, getClass, copy$default$1, numCompletedStages, attempt, PoolData, accumulatorUpdates, taskLocality, GC_TIME, StreamBlockData, jobIds, locality, gettingResultTime, TASK_INDEX, ACCUMULATORS, AppStatusStoreMetadata, OUTPUT_RECORDS, shuffleTotalBlocksFetched, ne, shuffleFetchWaitTime, DESER_CPU_TIME, shuffleRemoteBlocksFetched, executorRunTime, shuffleLocalBytesRead, childNodes, shuffleWriteRecords, eq, incomingEdges, productIterator, storageLevel, taskId, COMPLETION_TIME, resultFetchStart, SHUFFLE_REMOTE_READS_TO_DISK, ##, finalize, resultSize, index, productElement, hashCode, shuffleBytesWritten, StageDataWrapper.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(duration, numCompletedJobs, isInstanceOf, <init>, id, AppSummary, ==, status, stageIds, toString, !=, numCompletedStages, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(name, <init>, status, stageIds, PoolData, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(duration, shuffleRecordsWritten, skippedStages, stageAttemptId, name, resultSerializationTime, shuffleWriteTime, ExecutorSummaryWrapper, errorMessage, shuffleRemoteBytesReadToDisk, shuffleRemoteBytesRead, recordsWritten, JobDataWrapper, hostPort, stageId, shuffleRecordsRead, peakExecutionMemory, host, bytesWritten, diskBytesSpilled, shuffleLocalBlocksFetched, recordsRead, inputRecordsRead, speculative, toApi, executorId, info, executorDeserializeTime, executorDeserializeCpuTime, inputBytesRead, RDDStorageInfoWrapper, bytesRead, <init>, id, ExecutorStageSummaryWrapper, ==, status, outputBytesWritten, memoryBytesSpilled, stageIds, launchTime, outputRecordsWritten, toString, jvmGcTime, !=, executorCpuTime, TaskDataWrapper, PoolData, taskLocality, jobIds, gettingResultTime, shuffleFetchWaitTime, shuffleRemoteBlocksFetched, executorRunTime, shuffleLocalBytesRead, eq, storageLevel, taskId, resultSize, index, shuffleBytesWritten, StageDataWrapper)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(CachedQuantile, shuffleRecordsWritten, skippedStages, stageAttemptId, taskCount, stage, name, cached, resultSerializationTime, toRDDOperationGraph, shuffleWriteTime, ExecutorSummaryWrapper, shuffleRemoteBytesReadToDisk, shuffleRemoteBytesRead, recordsWritten, JobDataWrapper, stageId, shuffleRecordsRead, peakExecutionMemory, asInstanceOf, bytesWritten, diskBytesSpilled, shuffleLocalBlocksFetched, recordsRead, inputRecordsRead, ApplicationInfoWrapper, toApi, rootCluster, executorId, shuffleWriteBytes, info, shuffleReadBytes, schedulerDelay, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, inputBytesRead, RDDStorageInfoWrapper, bytesRead, <init>, id, ExecutorStageSummaryWrapper, AppSummary, ==, ApplicationEnvironmentInfoWrapper, status, outputBytesWritten, memoryBytesSpilled, stageIds, outputRecordsWritten, toString, jvmGcTime, !=, executorCpuTime, TaskDataWrapper, PoolData, accumulatorUpdates, StreamBlockData, locality, gettingResultTime, shuffleTotalBlocksFetched, ne, shuffleFetchWaitTime, shuffleRemoteBlocksFetched, executorRunTime, shuffleLocalBytesRead, shuffleWriteRecords, taskId, resultSize, index, shuffleBytesWritten, StageDataWrapper)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(CachedQuantile, duration, skippedStages, stageAttemptId, stage, name, diskSize, outgoingEdges, numCompletedJobs, ExecutorSummaryWrapper, errorMessage, equals, JobDataWrapper, hostPort, stageId, asInstanceOf, host, bytesWritten, childClusters, deserialized, RDDOperationGraphWrapper, ApplicationInfoWrapper, toApi, rootCluster, executorId, info, RDDOperationClusterWrapper, key, isInstanceOf, useDisk, bytesRead, version, <init>, id, ExecutorStageSummaryWrapper, useMemory, AppSummary, ==, ApplicationEnvironmentInfoWrapper, status, stageIds, launchTime, toString, !=, memSize, TaskDataWrapper, edges, getClass, numCompletedStages, attempt, taskLocality, StreamBlockData, jobIds, locality, ne, childNodes, incomingEdges, storageLevel, taskId, index, StageDataWrapper)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(name, diskSize, hostPort, deserialized, executorId, useDisk, <init>, id, useMemory, ==, status, toString, memSize, StreamBlockData, ne, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(skippedStages, name, <init>, AppSummary, ==, status, numCompletedStages, PoolData)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(duration, name, asInstanceOf, synchronized, ApplicationInfoWrapper, info, isInstanceOf, version, <init>, id, ==, clone, status, copy, toString, !=, getClass, attempt, AppStatusStoreMetadata, ne, eq, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(<init>, status, stageIds, PoolData)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, prefix, asInstanceOf, synchronized, $isInstanceOf, JobPage, notifyAll, isInstanceOf, <init>, ==, clone, renderJson, toString, !=, getClass, makeExecutorEvent, render, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(JobPage, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, ParallelCollectionPartition, countByValue$default$1, groupBy, treeReduce$default$2, ==, ParallelCollectionRDD, rddId, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, slice, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, values, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, parent, partitioner, name, setName, union, map, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, <init>, id, max, toDebugString, ++, flatMap, ==, ParallelCollectionRDD, rddId, clone, foreach, zipWithIndex, first, values, toString, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, index, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, PythonGatewayServer, synchronized, $isInstanceOf, logTrace, isTraceEnabled, main, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, metricRegistry, synchronized, JvmSource, $isInstanceOf, notifyAll, sourceName, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, visitAttribute, wait, clean$default$3, $asInstanceOf, getClassReader, equals, visitMethod, fillInStackTrace, initCause, asInstanceOf, initializeLogIfNecessary, visitField, clean, synchronized, $isInstanceOf, getCause, ClosureCleaner, logTrace, visitOuterClass, isTraceEnabled, visitTypeAnnotation, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, clean$default$2, getStackTrace, cv, getStackTraceElement, <init>, getMessage, setStackTrace, visitSource, getSuppressed, ReturnStatementInClosureException, ==, getStackTraceDepth, visit, clone, addSuppressed, $init$, toString, logError, !=, api, getClass, logWarning, visitInnerClass, ne, FieldAccessFinder, eq, visitAnnotation, log, ##, finalize, visitEnd, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, clean, synchronized, ClosureCleaner, isInstanceOf, <init>, getMessage, ==, clone, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, SerializableConfiguration, clone, toString, !=, getClass, ne, value, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, SerializableConfiguration, toString, !=, getClass, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, SerializableConfiguration, clone, toString, !=, getClass, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, SerializableConfiguration, toString, !=, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, SerializableConfiguration, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, SerializableConfiguration, toString, !=, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, <init>, ==, SerializableConfiguration, toString, !=, getClass, ne, value, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, SerializableConfiguration, toString, !=, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, SerializableConfiguration, clone, toString, getClass, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, SerializableConfiguration, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, PrefixComparator, notifyAll, <init>, toString, getClass, compare, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparator.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	postLocalMessage, notify, postToAll, verify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, removeRpcEndpointRef, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, ==, postRemoteMessage, clone, postOneWayMessage, $init$, registerRpcEndpoint, toString, awaitTermination, Dispatcher, logError, !=, getClass, logWarning, getRpcEndpointRef, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, removeRpcEndpointRef, isInstanceOf, <init>, ==, toString, Dispatcher, logError, !=, logWarning, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(postLocalMessage, postToAll, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, postRemoteMessage, clone, postOneWayMessage, registerRpcEndpoint, toString, awaitTermination, Dispatcher, logError, !=, logWarning, getRpcEndpointRef, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala: Set(verify, asInstanceOf, isInstanceOf, <init>, ==, toString, Dispatcher, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, defaultParallelism, $asInstanceOf, workerRemoved, equals, asInstanceOf, applicationId, cancelTasks, synchronized, $isInstanceOf, rootPool, stop, notifyAll, isInstanceOf, applicationAttemptId, ==, TaskScheduler, executorLost, clone, $init$, setDAGScheduler, toString, !=, getClass, killTaskAttempt, start, postStartHook, schedulingMode, ne, executorHeartbeatReceived, eq, ##, finalize, hashCode, submitTasks.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(workerRemoved, asInstanceOf, synchronized, stop, isInstanceOf, ==, executorLost, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, stop, isInstanceOf, ==, toString, !=, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(defaultParallelism, asInstanceOf, applicationId, synchronized, rootPool, stop, isInstanceOf, applicationAttemptId, ==, TaskScheduler, clone, toString, !=, getClass, killTaskAttempt, start, postStartHook, schedulingMode, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, stop, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, applicationId, cancelTasks, synchronized, stop, isInstanceOf, applicationAttemptId, ==, TaskScheduler, clone, setDAGScheduler, toString, !=, killTaskAttempt, start, ne, submitTasks)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(defaultParallelism, asInstanceOf, applicationId, synchronized, rootPool, stop, isInstanceOf, applicationAttemptId, ==, TaskScheduler, clone, toString, !=, getClass, killTaskAttempt, start, postStartHook, schedulingMode, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala: Set(TaskScheduler)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, defaultParallelism, workerRemoved, asInstanceOf, applicationId, synchronized, rootPool, stop, isInstanceOf, applicationAttemptId, ==, TaskScheduler, executorLost, toString, !=, start, schedulingMode, ne, executorHeartbeatReceived)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, isInstanceOf, ==, TaskScheduler, executorLost, toString, !=, ne, executorHeartbeatReceived, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	intWritableConverter, bytesWritableFactory, floatWritableConverterFn, stringWritableConverter, stringWritableConverterFn, writableWritableFactory, booleanWritableFactory, intWritableFactory, longWritableFactory, bytesWritableConverterFn, doubleWritableConverter, longWritableConverter, floatWritableConverter, writableWritableConverterFn, bytesWritableConverter, booleanWritableConverter, longWritableConverterFn, intWritableConverterFn, stringWritableFactory, floatWritableFactory, booleanWritableConverterFn, doubleWritableConverterFn, doubleWritableFactory, writableWritableConverter.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createRDDFromArray, notify, createSparkContext, wait, $asInstanceOf, equals, RRDD, asInstanceOf, synchronized, $isInstanceOf, createRDDFromFile, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, buffer, notifyAll, isInstanceOf, <init>, SerializableBuffer, ==, clone, toString, !=, getClass, ne, value, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SerializableBuffer, ==, toString, !=, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala: Set(asInstanceOf, isInstanceOf, <init>, SerializableBuffer, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, <init>, SerializableBuffer, ==, !=, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction2.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction2.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction2.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction2.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction2.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, FlatMapFunction2.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction2.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(call, FlatMapFunction2)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, asIterator, SerializerInstance, equals, serializeStream, asInstanceOf, synchronized, $isInstanceOf, asKeyValueIterator, readObject, deserialize, readKey, deserializeStream, readValue, notifyAll, isInstanceOf, defaultClassLoader, writeObject, <init>, supportsRelocationOfSerializedObjects, setDefaultClassLoader, writeValue, ==, Serializer, clone, newInstance, flush, toString, !=, writeKey, DeserializationStream, getClass, close, ne, serialize, eq, ##, finalize, writeAll, SerializationStream, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(SerializerInstance, asInstanceOf, synchronized, isInstanceOf, <init>, ==, Serializer, clone, newInstance, toString, !=, ne, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala: Set(SerializerInstance, deserialize, <init>, Serializer, newInstance, close, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(SerializerInstance, serializeStream, <init>, writeValue, ==, flush, !=, writeKey, close, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(SerializerInstance, asInstanceOf, synchronized, isInstanceOf, <init>, ==, Serializer, newInstance, toString, !=, ne, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(SerializerInstance, asInstanceOf, deserialize, <init>, ==, Serializer, newInstance, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala: Set(SerializerInstance, asInstanceOf, isInstanceOf, <init>, ==, Serializer, clone, newInstance, toString, !=, getClass, ne, serialize, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(SerializerInstance, asInstanceOf, isInstanceOf, <init>, ==, Serializer, newInstance, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(SerializerInstance, asInstanceOf, readObject, isInstanceOf, writeObject, <init>, ==, Serializer, newInstance, DeserializationStream, ne, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asIterator, SerializerInstance, serializeStream, asInstanceOf, readObject, deserializeStream, isInstanceOf, writeObject, <init>, Serializer, newInstance, toString, !=, DeserializationStream, close, ne, writeAll, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(SerializerInstance, equals, serializeStream, asInstanceOf, synchronized, readObject, deserialize, deserializeStream, isInstanceOf, writeObject, <init>, ==, newInstance, flush, toString, !=, DeserializationStream, getClass, close, ne, serialize, eq, SerializationStream, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala: Set(SerializerInstance, serializeStream, readObject, deserializeStream, writeObject, <init>, Serializer, newInstance, !=, DeserializationStream, close, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, Serializer, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala: Set(SerializerInstance, asInstanceOf, readObject, deserialize, isInstanceOf, writeObject, <init>, ==, Serializer, newInstance, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, Serializer, newInstance, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(SerializerInstance, serializeStream, asInstanceOf, synchronized, readObject, deserializeStream, isInstanceOf, writeObject, <init>, ==, Serializer, newInstance, !=, DeserializationStream, close, ne, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, <init>, supportsRelocationOfSerializedObjects, Serializer, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(SerializerInstance, asInstanceOf, synchronized, isInstanceOf, <init>, ==, Serializer, clone, newInstance, toString, !=, getClass, ne, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>, Serializer, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asIterator, SerializerInstance, serializeStream, asInstanceOf, deserializeStream, isInstanceOf, <init>, setDefaultClassLoader, ==, Serializer, newInstance, DeserializationStream, close, writeAll, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(SerializerInstance, serializeStream, asInstanceOf, synchronized, deserialize, isInstanceOf, writeObject, <init>, ==, clone, newInstance, toString, !=, close, ne, serialize, eq, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(SerializerInstance, asInstanceOf, synchronized, isInstanceOf, <init>, ==, newInstance, toString, !=, getClass, close, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(SerializerInstance, asInstanceOf, synchronized, readKey, deserializeStream, readValue, <init>, ==, Serializer, newInstance, flush, !=, DeserializationStream, getClass, close, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(asInstanceOf, <init>, Serializer, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(SerializerInstance, <init>, Serializer, newInstance, !=, getClass, close, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(SerializerInstance, asInstanceOf, deserialize, isInstanceOf, <init>, ==, Serializer, newInstance, toString, !=, ne, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(<init>, Serializer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(SerializerInstance, asInstanceOf, synchronized, readKey, deserializeStream, readValue, isInstanceOf, <init>, ==, Serializer, newInstance, flush, toString, !=, DeserializationStream, close, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(asInstanceOf, <init>, Serializer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(SerializerInstance, asInstanceOf, readObject, isInstanceOf, writeObject, <init>, ==, Serializer, newInstance, DeserializationStream, ne, SerializationStream, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, Serializer, newInstance, toString, !=, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, SerializerInstance, asInstanceOf, synchronized, deserialize, notifyAll, isInstanceOf, <init>, setDefaultClassLoader, ==, Serializer, newInstance, !=, ne, serialize, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(SerializerInstance, asInstanceOf, asKeyValueIterator, deserializeStream, isInstanceOf, <init>, ==, Serializer, newInstance, DeserializationStream, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(SerializerInstance, synchronized, deserialize, notifyAll, <init>, Serializer, newInstance, !=, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala: Set(SerializerInstance, serializeStream, asInstanceOf, readObject, deserializeStream, isInstanceOf, defaultClassLoader, writeObject, <init>, Serializer, flush, DeserializationStream, close, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(SerializerInstance, asInstanceOf, deserialize, <init>, ==, Serializer, newInstance, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, ==, Serializer, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(SerializerInstance, asInstanceOf, deserialize, isInstanceOf, <init>, ==, Serializer, newInstance, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf, <init>, Serializer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(SerializerInstance, serializeStream, asInstanceOf, synchronized, isInstanceOf, writeObject, <init>, ==, Serializer, newInstance, flush, toString, !=, close, eq, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, Serializer, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(SerializerInstance, asInstanceOf, deserialize, <init>, ==, newInstance, toString, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala: Set(SerializerInstance, asInstanceOf, deserialize, isInstanceOf, <init>, Serializer, newInstance, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(SerializerInstance, asInstanceOf, deserialize, isInstanceOf, defaultClassLoader, <init>, supportsRelocationOfSerializedObjects, ==, Serializer, newInstance, flush, !=, DeserializationStream, getClass, close, ne, serialize, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asIterator, SerializerInstance, serializeStream, asInstanceOf, deserializeStream, isInstanceOf, <init>, setDefaultClassLoader, ==, Serializer, newInstance, DeserializationStream, close, writeAll, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(SerializerInstance, asInstanceOf, readObject, isInstanceOf, writeObject, <init>, ==, Serializer, newInstance, DeserializationStream, ne, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, Serializer, newInstance, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(SerializerInstance, serializeStream, asInstanceOf, synchronized, deserialize, isInstanceOf, writeObject, <init>, ==, clone, newInstance, toString, !=, close, ne, serialize, eq, SerializationStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(SerializerInstance, <init>, Serializer, newInstance, !=, getClass, close, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(SerializerInstance, asInstanceOf, readObject, isInstanceOf, writeObject, <init>, ==, Serializer, newInstance, DeserializationStream, ne, SerializationStream, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, Serializer, newInstance, toString, !=, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(SerializerInstance, asInstanceOf, deserialize, <init>, ==, newInstance, toString, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, getConf, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, CONFIGURATION_INSTANTIATION_LOCK, name, count, inputSplit, wait, getJobConf, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, mapPartitionsWithInputSplit$default$2, union, coalesce$default$3, zip, localCheckpoint, map, convertSplitLocationInfo, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, getPipeEnvVars, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, addLocalConfiguration, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, <init>$default$3, persist, jobConfCacheKey, checkpointData, <init>, isCheckpointed, HadoopRDD, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, getCachedMetadata, RECORDS_BETWEEN_BYTES_READ_METRIC_UPDATES, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, HadoopMapPartitionsWithSplitRDD, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, inputFormatCacheKey, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, getInputFormat, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, HadoopPartition, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, mapPartitionsWithInputSplit, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(getConf, asInstanceOf, context, iterator, addLocalConfiguration, conf, isInstanceOf, <init>, HadoopRDD, id, ==, sparkContext, toString, logError, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, getConf, parent, partitioner, name, setName, union, map, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, <init>, HadoopRDD, id, max, toDebugString, ++, flatMap, ==, clone, foreach, zipWithIndex, first, toString, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, index, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(name, equals, asInstanceOf, context, getPipeEnvVars, iterator, isInstanceOf, <init>, foreach, toString, logError, !=, partitions, ne, HadoopPartition, logDebug, firstParent)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(getConf, partitioner, CONFIGURATION_INSTANTIATION_LOCK, inputSplit, convertSplitLocationInfo, equals, asInstanceOf, context, synchronized, iterator, conf, isInstanceOf, filter, persist, <init>, HadoopRDD, id, ==, foreach, sparkContext, toString, !=, partitions, logWarning, index, logDebug, firstParent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(getConf, name, union, map, asInstanceOf, conf, <init>, HadoopRDD, ++, first)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(asInstanceOf, <init>, HadoopRDD, mapPartitionsWithInputSplit)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ObjectStreamClassMethods.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	appSparkVersion, notify, name, wait, $asInstanceOf, ExecutorsTab, equals, appName, prefix, asInstanceOf, synchronized, $isInstanceOf, pages, attachPage, basePath, notifyAll, isInstanceOf, <init>, ==, clone, ExecutorsPage, renderJson, toString, !=, getClass, render, headerTabs, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appSparkVersion, ExecutorsTab, appName, prefix, basePath, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, SparkHadoopMapRedUtil, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, commitTask, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala: Set(SparkHadoopMapRedUtil, asInstanceOf, isInstanceOf, commitTask, toString, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, BroadcastManager, wait, $asInstanceOf, equals, cachedValues, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, ==, clone, isDriver, $init$, toString, logError, !=, getClass, logWarning, unbroadcast, newBroadcast, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(BroadcastManager, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, newBroadcast, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(BroadcastManager, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, isDriver, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(BroadcastManager, cachedValues, asInstanceOf, synchronized, isInstanceOf, <init>, ==, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(BroadcastManager, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, logError, unbroadcast, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(BroadcastManager, wait, asInstanceOf, synchronized, stop, notifyAll, isInstanceOf, <init>, ==, toString, logError, !=, newBroadcast, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, appList, equals, httpRequest, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, uiRoot, $init$, servletContext, toString, !=, getClass, ne, ApplicationListResource, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(httpRequest, asInstanceOf, isInstanceOf, <init>, ==, uiRoot, servletContext, toString, getClass, ApplicationListResource, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ShuffleReader, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, ShuffleReader, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, ShuffleReader, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ShuffleReader, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(read, equals, asInstanceOf, isInstanceOf, ShuffleReader, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala: Set(ShuffleReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(read, asInstanceOf, ShuffleReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(read, asInstanceOf, isInstanceOf, ShuffleReader, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, unpersist, addApplication, wait, $asInstanceOf, equals, addWorker, asInstanceOf, initializeLogIfNecessary, synchronized, readPersistedData, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, removeApplication, persist, <init>, removeDriver, serializer, ==, clone, ZooKeeperPersistenceEngine, $init$, toString, logError, !=, getClass, logWarning, close, ne, addDriver, eq, log, removeWorker, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(<init>, serializer, ZooKeeperPersistenceEngine, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ConfigReader, wait, $asInstanceOf, bindEnv, equals, asInstanceOf, synchronized, bind, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, substitute, toString, !=, get, getClass, ne, eq, bindSystem, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(ConfigReader, bindEnv, asInstanceOf, isInstanceOf, <init>, ==, substitute, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala: Set(ConfigReader, asInstanceOf, <init>, ==, substitute, get)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, MapFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, ASYNC_TRACKING_ENABLED, asInstanceOf, synchronized, $isInstanceOf, config, notifyAll, MAX_RETAINED_STAGES, isInstanceOf, LIVE_ENTITY_UPDATE_PERIOD, MAX_RETAINED_TASKS_PER_STAGE, ==, clone, MAX_RETAINED_DEAD_EXECUTORS, MAX_RETAINED_ROOT_NODES, toString, !=, getClass, ne, eq, ##, finalize, hashCode, MAX_RETAINED_JOBS.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(ASYNC_TRACKING_ENABLED, asInstanceOf, synchronized, config, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala: Set(ASYNC_TRACKING_ENABLED, asInstanceOf, synchronized, config, isInstanceOf, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, config, MAX_RETAINED_STAGES, isInstanceOf, LIVE_ENTITY_UPDATE_PERIOD, MAX_RETAINED_TASKS_PER_STAGE, ==, MAX_RETAINED_DEAD_EXECUTORS, MAX_RETAINED_ROOT_NODES, toString, !=, getClass, ne, MAX_RETAINED_JOBS)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	measure, notify, unapply, curried, addCase, name, wait, <init>$default$5, benchmarks, copy$default$2, $asInstanceOf, <init>$default$6, productArity, addTimerCase, equals, Case, addTimerCase$default$2, asInstanceOf, run, avgMs, synchronized, <init>$default$7, $isInstanceOf, <init>$default$4, tupled, canEqual, getJVMOSInfo, productPrefix, startTiming, notifyAll, out, bestRate, isInstanceOf, Timer, <init>$default$3, fn, <init>, addCase$default$2, apply, iteration, ==, getProcessorName, clone, numIters, bestMs, $init$, Benchmark, copy$default$3, copy, toString, !=, getClass, copy$default$1, Result, ne, totalTime, eq, stopTiming, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, CSV_DEFAULT_UNIT, registry, wait, $asInstanceOf, CSV_KEY_DIR, equals, reporter, pollUnit, asInstanceOf, synchronized, $isInstanceOf, CSV_KEY_UNIT, CSV_DEFAULT_PERIOD, stop, notifyAll, isInstanceOf, pollDir, <init>, pollPeriod, ==, clone, CSV_DEFAULT_DIR, report, toString, property, !=, CsvSink, getClass, start, ne, CSV_KEY_PERIOD, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ID, name, wait, DECREASING_RUNTIME, INCREASING_RUNTIME, valueOf, fromString, equals, getDeclaringClass, TaskSorting, notifyAll, compareTo, ordinal, values, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala: Set(TaskSorting)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(name, TaskSorting, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, metricRegistry, synchronized, $isInstanceOf, notifyAll, sourceName, isInstanceOf, <init>, BlockManagerSource, ==, clone, toString, !=, getClass, ne, blockManager, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, BlockManagerSource, ==, clone, toString, !=, getClass, ne, blockManager)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, HistoryServerArguments, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, toString, HistoryServerArguments, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, MapPartitionsRDD, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, prev, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(creationSite, zipPartitions, parent, isLocallyCheckpointed, getOrCompute, partitioner, coalesce, name, count, MapPartitionsRDD, isCheckpointedAndMaterialized, mapPartitions, union, map, subtract, intersection, scope, asInstanceOf, context, getPreferredLocations, doCheckpoint, synchronized, aggregate, compute, min, fold, getOutputDeterministicLevel, iterator, conf, isInstanceOf, filter, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, ++, flatMap, take, groupBy, ==, clone, distinct, retag, foreach, reduce, zipWithIndex, checkpoint, elementClassTag, sample, toString, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, clearDependencies, isEmpty, ne, getDependencies, mapPartitionsWithIndex, keyBy, getCreationSite, computeOrReadCheckpoint, dependencies, isReliablyCheckpointed, withScope, log, treeAggregate, takeOrdered, firstParent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(partitioner, MapPartitionsRDD, mapPartitions, map, asInstanceOf, context, iterator, conf, isInstanceOf, filter, <init>, flatMap, ==, foreach, sparkContext, reduce, toString, !=, partitions, collect, logWarning, isEmpty, ne, countByValueApprox, mapPartitionsWithIndex, withScope, log)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	estimateSize, toIterable, withFilter, toTraversable, notify, find, span, flatten, toBuffer, count, reduceOption, sliceWithKnownDelta, wait, foldRight, takeWhile, stringPrefix, $asInstanceOf, splitAt, minBy, size, inits, zip, toSet, :\, map, takeRight, dropWhile, toMap, filterNot, equals, par, unzip3, repr, toList, resetSamples, isTraversableAgain, head, asInstanceOf, sameElements, unzip, reduceLeftOption, synchronized, sliding, atGrowThreshold, Self, partition, aggregate, $isInstanceOf, forall, newBuilder, mkString, afterUpdate, min, scanRight, fold, scan, nonEmpty, canEqual, destructiveSortedIterator, tail, lastOption, dropRight, iterator, last, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, ++:, <init>, toStream, companion, max, tails, apply, ++, grouped, flatMap, take, parCombiner, reduceRight, groupBy, ==, maxBy, sliceWithKnownBound, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, changeValue, $init$, toSeq, zipWithIndex, SizeTrackingAppendOnlyMap, growTable, toString, genericBuilder, copyToArray, seq, !=, transpose, collect, headOption, getClass, WithFilter, update, hasDefiniteSize, foldLeft, toCollection, isEmpty, ne, init, reversed, reduceLeft, eq, sum, thisCollection, ##, scanLeft, finalize, hashCode, zipAll, product, view.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala: Set(map, partition, destructiveSortedIterator, <init>, SizeTrackingAppendOnlyMap, update)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(estimateSize, size, map, head, asInstanceOf, synchronized, mkString, nonEmpty, destructiveSortedIterator, iterator, last, <init>, apply, ++, ==, foreach, exists, changeValue, SizeTrackingAppendOnlyMap, !=, getClass, update, isEmpty, ne, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(estimateSize, withFilter, flatten, size, zip, map, head, asInstanceOf, synchronized, mkString, iterator, isInstanceOf, filter, <init>, apply, ++, flatMap, ==, foreach, exists, toArray, changeValue, toString, !=, update, isEmpty, ne, eq, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(estimateSize, withFilter, flatten, size, zip, map, head, asInstanceOf, synchronized, mkString, iterator, isInstanceOf, filter, <init>, apply, ++, flatMap, ==, foreach, exists, toArray, changeValue, toString, !=, update, isEmpty, ne, eq, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	rddCleaned, notify, shuffleCleaned, checkpointCleaned, wait, $asInstanceOf, doCleanupAccum, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, attachListener, doCleanCheckpoint, doCleanupRDD, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, registerRDDForCleanup, isInstanceOf, registerBroadcastForCleanup, <init>, doCleanupShuffle, ==, clone, $init$, registerShuffleForCleanup, registerAccumulatorForCleanup, toString, logError, !=, getClass, CleanerListener, logWarning, start, doCleanupBroadcast, broadcastCleaned, ne, accumCleaned, registerRDDCheckpointDataForCleanup, ContextCleaner, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, registerBroadcastForCleanup, <init>, ==, clone, registerAccumulatorForCleanup, toString, logError, !=, getClass, logWarning, start, ne, ContextCleaner, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, registerRDDForCleanup, isInstanceOf, <init>, ==, clone, toString, !=, getClass, logWarning, ne, ContextCleaner, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, registerAccumulatorForCleanup, toString, !=, getClass, logWarning, ContextCleaner, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(asInstanceOf, <init>, registerShuffleForCleanup, ContextCleaner)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, <init>, toString, registerRDDCheckpointDataForCleanup, ContextCleaner, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, CheckpointRDDPartition, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, CheckpointRDD, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(map, iterator, filter, <init>, id, sparkContext, getStorageLevel, CheckpointRDD, partitions, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(map, context, synchronized, conf, <init>, isCheckpointed, id, foreach, CheckpointRDD, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, parent, partitioner, name, setName, union, map, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, <init>, id, max, toDebugString, ++, flatMap, ==, clone, foreach, zipWithIndex, first, toString, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, index, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala: Set(markCheckpointed, map, doCheckpoint, synchronized, <init>, ==, checkpoint, CheckpointRDD, partitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(partitioner, CheckpointRDDPartition, map, asInstanceOf, context, sortBy, getCheckpointFile, iterator, conf, isInstanceOf, filter, <init>, id, flatMap, foreach, sparkContext, zipWithIndex, CheckpointRDD, toString, !=, partitions, logWarning, isEmpty, ne, index, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(creationSite, zipPartitions, parent, isLocallyCheckpointed, getOrCompute, partitioner, coalesce, name, count, isCheckpointedAndMaterialized, mapPartitions, union, map, subtract, intersection, scope, asInstanceOf, context, getPreferredLocations, doCheckpoint, synchronized, aggregate, compute, min, fold, getOutputDeterministicLevel, iterator, conf, isInstanceOf, filter, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, ++, flatMap, take, groupBy, ==, clone, distinct, retag, foreach, reduce, zipWithIndex, checkpoint, CheckpointRDD, elementClassTag, sample, toString, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, clearDependencies, isEmpty, ne, getDependencies, mapPartitionsWithIndex, keyBy, getCreationSite, computeOrReadCheckpoint, dependencies, isReliablyCheckpointed, withScope, log, treeAggregate, index, takeOrdered, firstParent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(map, iterator, filter, <init>, id, sparkContext, getStorageLevel, CheckpointRDD, partitions, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(CheckpointRDDPartition, map, context, <init>, id, CheckpointRDD, partitions, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(map, context, synchronized, conf, <init>, isCheckpointed, id, foreach, CheckpointRDD, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, offHeapStorageMemoryPool, releaseStorageMemory, wait, releaseExecutionMemory, MemoryManager, $asInstanceOf, maxOnHeapStorageMemory, releaseAllStorageMemory, equals, acquireUnrollMemory, acquireExecutionMemory, offHeapExecutionMemoryPool, asInstanceOf, initializeLogIfNecessary, releaseAllExecutionMemoryForTask, synchronized, $isInstanceOf, pageSizeBytes, logTrace, getExecutionMemoryUsageForTask, storageMemoryUsed, isTraceEnabled, initializeLogIfNecessary$default$2, releaseUnrollMemory, logName, notifyAll, isInstanceOf, acquireStorageMemory, <init>, setMemoryStore, ==, maxOffHeapStorageMemory, clone, $init$, onHeapStorageMemoryPool, tungstenMemoryMode, offHeapStorageMemory, toString, onHeapExecutionMemoryPool, logError, !=, maxOffHeapMemory, getClass, logWarning, tungstenMemoryAllocator, ne, eq, log, executionMemoryUsed, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(MemoryManager, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(MemoryManager, maxOnHeapStorageMemory, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, setMemoryStore, ==, maxOffHeapStorageMemory, toString, logError, !=, maxOffHeapMemory, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(offHeapStorageMemoryPool, MemoryManager, maxOnHeapStorageMemory, offHeapExecutionMemoryPool, synchronized, acquireStorageMemory, <init>, ==, maxOffHeapStorageMemory, onHeapStorageMemoryPool, offHeapStorageMemory, onHeapExecutionMemoryPool, maxOffHeapMemory, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(offHeapStorageMemoryPool, MemoryManager, maxOnHeapStorageMemory, offHeapExecutionMemoryPool, synchronized, <init>, ==, onHeapStorageMemoryPool, onHeapExecutionMemoryPool, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, MemoryManager, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, ==, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(MemoryManager, synchronized, notifyAll, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(releaseStorageMemory, MemoryManager, maxOnHeapStorageMemory, releaseAllStorageMemory, acquireUnrollMemory, asInstanceOf, synchronized, storageMemoryUsed, releaseUnrollMemory, isInstanceOf, acquireStorageMemory, <init>, ==, maxOffHeapStorageMemory, toString, !=, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(MemoryManager, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(MemoryManager, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	used, notify, wait, Spillable, $asInstanceOf, equals, taskMemoryManager, asInstanceOf, initializeLogIfNecessary, synchronized, allocateArray, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, maybeSpill, logName, notifyAll, allocatePage, releaseMemory, isInstanceOf, freePage, <init>, elementsRead, ==, clone, memoryBytesSpilled, acquireMemory, $init$, freeArray, _spillCount, toString, logError, !=, freeMemory, spill, getClass, addElementsRead, logWarning, getMode, ne, eq, forceSpill, log, ##, finalize, hashCode, getUsed, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, memoryBytesSpilled, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, memoryBytesSpilled, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(Spillable, taskMemoryManager, asInstanceOf, synchronized, maybeSpill, releaseMemory, <init>, ==, !=, spill, getClass, addElementsRead, logWarning, ne, hashCode, getUsed, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(Spillable, taskMemoryManager, asInstanceOf, synchronized, maybeSpill, releaseMemory, isInstanceOf, <init>, ==, memoryBytesSpilled, toString, !=, spill, addElementsRead, logWarning, ne, eq, hashCode, getUsed, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, memoryBytesSpilled, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, ==, memoryBytesSpilled, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, memoryBytesSpilled, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, ==, memoryBytesSpilled, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, putBytes, logName, notifyAll, isInstanceOf, <init>, remove, ==, clone, getBytes, $init$, put, toString, logError, DiskStore, !=, getClass, getSize, logWarning, contains, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, putBytes, isInstanceOf, <init>, remove, ==, getBytes, put, toString, logError, DiskStore, !=, getClass, getSize, logWarning, contains, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordPointerAndKeyPrefix.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordPointerAndKeyPrefix.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordPointerAndKeyPrefix.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordPointerAndKeyPrefix.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordPointerAndKeyPrefix.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, recordPointer, wait, RecordPointerAndKeyPrefix, equals, notifyAll, <init>, keyPrefix, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordPointerAndKeyPrefix.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferOutputStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferOutputStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferOutputStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferOutputStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferOutputStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, toByteBuffer, count, wait, $asInstanceOf, size, equals, asInstanceOf, synchronized, $isInstanceOf, writeTo, buf, notifyAll, isInstanceOf, ByteBufferOutputStream, <init>, ==, clone, reset, flush, toString, !=, getClass, getCount, toByteArray, close, ne, eq, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala: Set(toByteBuffer, size, ByteBufferOutputStream, <init>, close, ne, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(toByteBuffer, asInstanceOf, synchronized, buf, isInstanceOf, ByteBufferOutputStream, <init>, ==, clone, toString, !=, close, ne, eq, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala: Set(toByteBuffer, asInstanceOf, isInstanceOf, ByteBufferOutputStream, <init>, reset, flush, close)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, create, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, PartitionPruningRDDPartition, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, rdd, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, PartitionPruningRDD, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, getParents, elementClassTag, sample, pipe$default$7, parentSplit, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, PruneDependency, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(partitioner, map, asInstanceOf, context, sortBy, min, conf, getNumPartitions, isInstanceOf, filter, rdd, <init>, id, PartitionPruningRDD, max, ++, ==, foreach, sample, partitions, collect, isEmpty, ne, mapPartitionsWithIndex, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(partitioner, asInstanceOf, create, min, isInstanceOf, filter, rdd, <init>, PartitionPruningRDD, max, partitions, ne, withScope)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toTaskFailedReason, notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, MetadataFetchFailedException, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, FetchFailedException, getStackTraceDepth, clone, addSuppressed, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(synchronized, <init>, getMessage, FetchFailedException)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(toTaskFailedReason, wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, ==, FetchFailedException, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(<init>, FetchFailedException, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, FetchFailedException, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, MetadataFetchFailedException, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, getMessage, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, readExternal, wait, writeExternal, $asInstanceOf, asIterator, equals, serializeStream, asInstanceOf, synchronized, $isInstanceOf, asKeyValueIterator, readObject, JavaSerializerInstance, deserialize, readKey, deserializeStream, readValue, notifyAll, isInstanceOf, defaultClassLoader, writeObject, <init>, supportsRelocationOfSerializedObjects, setDefaultClassLoader, JavaSerializer, writeValue, JavaSerializationStream, ==, clone, newInstance, flush, toString, !=, writeKey, getClass, close, JavaDeserializationStream, ne, serialize, eq, ##, finalize, writeAll, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(asInstanceOf, readObject, isInstanceOf, writeObject, <init>, JavaSerializer, ==, newInstance, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, JavaSerializer, ==, newInstance, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(serializeStream, asInstanceOf, synchronized, JavaSerializerInstance, deserialize, isInstanceOf, writeObject, <init>, JavaSerializer, ==, clone, newInstance, toString, !=, close, ne, serialize, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(<init>, JavaSerializer, newInstance, !=, getClass, close, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, readObject, isInstanceOf, writeObject, <init>, JavaSerializer, ==, newInstance, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, <init>, JavaSerializer, ==, newInstance, toString, !=, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(asInstanceOf, deserialize, <init>, JavaSerializer, ==, newInstance, toString, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	buildRegistryName, notify, wait, $asInstanceOf, getSourcesByName, SINK_REGEX, equals, MetricsSystem, asInstanceOf, initializeLogIfNecessary, synchronized, removeSource, $isInstanceOf, getServletHandlers, instance, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, checkMinimalPollingPeriod, ==, clone, $init$, report, toString, logError, !=, getClass, logWarning, SOURCE_REGEX, start, registerSource, ne, eq, log, createMetricsSystem, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala: Set(MetricsSystem, asInstanceOf, stop, isInstanceOf, checkMinimalPollingPeriod, ==, report, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(MetricsSystem, asInstanceOf, synchronized, getServletHandlers, stop, isInstanceOf, ==, clone, report, toString, logError, !=, getClass, logWarning, start, registerSource, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala: Set(MetricsSystem, asInstanceOf, stop, isInstanceOf, checkMinimalPollingPeriod, ==, report, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(MetricsSystem, asInstanceOf, synchronized, getServletHandlers, stop, isInstanceOf, ==, report, toString, logError, !=, logWarning, start, registerSource, ne, log, createMetricsSystem, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(MetricsSystem, asInstanceOf, synchronized, stop, isInstanceOf, ==, toString, !=, getClass, logWarning, start, ne, createMetricsSystem, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(getSourcesByName, MetricsSystem, synchronized, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(MetricsSystem, asInstanceOf, synchronized, stop, isInstanceOf, ==, logError, start, registerSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala: Set(MetricsSystem, asInstanceOf, stop, isInstanceOf, checkMinimalPollingPeriod, ==, report, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(MetricsSystem, asInstanceOf, removeSource, getServletHandlers, stop, isInstanceOf, ==, report, toString, logError, !=, logWarning, start, registerSource, ne, log, createMetricsSystem, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, MetricsSystem, asInstanceOf, synchronized, stop, notifyAll, isInstanceOf, ==, report, logError, !=, logWarning, registerSource, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(MetricsSystem, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala: Set(MetricsSystem, stop, checkMinimalPollingPeriod, report, start, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(MetricsSystem, synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala: Set(MetricsSystem, asInstanceOf, stop, isInstanceOf, checkMinimalPollingPeriod, ==, report, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(MetricsSystem, stop, ==, !=, start, registerSource, log, createMetricsSystem, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, StandaloneRecoveryModeFactory, wait, FileSystemRecoveryModeFactory, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createLeaderElectionAgent, logName, notifyAll, isInstanceOf, <init>, ==, ZooKeeperRecoveryModeFactory, clone, RECOVERY_DIR, $init$, createPersistenceEngine, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(StandaloneRecoveryModeFactory, FileSystemRecoveryModeFactory, asInstanceOf, createLeaderElectionAgent, isInstanceOf, <init>, ==, ZooKeeperRecoveryModeFactory, createPersistenceEngine, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapGroupsFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapGroupsFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapGroupsFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapGroupsFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapGroupsFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	MapGroupsFunction, call.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapGroupsFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, post, wait, $asInstanceOf, addToEventLogQueue, listeners, equals, addToStatusQueue, LiveListenerBus, asInstanceOf, initializeLogIfNecessary, queuedEvents, metricRegistry, synchronized, $isInstanceOf, activeQueues, addToQueue, logTrace, LiveListenerBusMetrics, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, sourceName, getTimerForListenerClass, isInstanceOf, APP_STATUS_QUEUE, removeListener, <init>, ==, EVENT_LOG_QUEUE, addToManagementQueue, clone, $init$, findListenersByClass, waitUntilEmpty, withinListenerThread, toString, metrics, logError, !=, EXECUTOR_MANAGEMENT_QUEUE, getClass, logWarning, SHARED_QUEUE, numEventsPosted, start, ne, eq, log, addToSharedQueue, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(post, LiveListenerBus, asInstanceOf, synchronized, logTrace, stop, isInstanceOf, <init>, ==, clone, toString, logError, !=, logWarning, start, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(post, LiveListenerBus, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(post, addToEventLogQueue, listeners, addToStatusQueue, LiveListenerBus, asInstanceOf, synchronized, stop, isInstanceOf, removeListener, <init>, ==, clone, withinListenerThread, toString, logError, !=, getClass, logWarning, start, ne, log, addToSharedQueue, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(LiveListenerBus, asInstanceOf, metricRegistry, synchronized, sourceName, isInstanceOf, <init>, ==, addToManagementQueue, metrics, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(post, LiveListenerBus, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(LiveListenerBus, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, toString, metrics, !=, getClass, logWarning, start, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(post, LiveListenerBus, asInstanceOf, stop, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(post, LiveListenerBus, asInstanceOf, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(post, LiveListenerBus, metricRegistry, logTrace, LiveListenerBusMetrics, stop, getTimerForListenerClass, removeListener, <init>, waitUntilEmpty, withinListenerThread, metrics, logError, !=, getClass, logWarning, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(LiveListenerBus, asInstanceOf, logTrace, isInstanceOf, <init>, ==, addToManagementQueue, toString, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, createWritableChannel, equals, toCryptoConf, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, IV_LENGTH_IN_BYTES, initializeLogIfNecessary$default$2, logName, notifyAll, SPARK_IO_ENCRYPTION_COMMONS_CONFIG_PREFIX, isInstanceOf, ==, clone, createKey, createReadableChannel, $init$, CryptoStreamUtils, toString, logError, !=, getClass, logWarning, createCryptoInputStream, ne, eq, log, ##, finalize, hashCode, logDebug, createCryptoOutputStream, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asInstanceOf, isInstanceOf, ==, CryptoStreamUtils, createCryptoInputStream, createCryptoOutputStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(createWritableChannel, asInstanceOf, isInstanceOf, ==, createReadableChannel, CryptoStreamUtils, toString, !=, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, createKey, CryptoStreamUtils, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, name, wait, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, apply, ==, clone, $init$, copy, toString, !=, getClass, copy$default$1, ne, RpcEndpointAddress, rpcAddress, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, RpcEndpointAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(unapply, name, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, toString, !=, ne, RpcEndpointAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(name, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, RpcEndpointAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(unapply, name, asInstanceOf, synchronized, isInstanceOf, <init>, ==, !=, ne, RpcEndpointAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(unapply, name, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, clone, toString, !=, ne, RpcEndpointAddress, rpcAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	setIfMissing, notify, getTimeAsMs, wait, getTimeAsSeconds, $asInstanceOf, SparkConf, getAll, getSizeAsMb, getSizeAsGb, equals, registerKryoClasses, asInstanceOf, initializeLogIfNecessary, getDouble, set, synchronized, $isInstanceOf, isSparkPortConf, getDeprecatedConfig, getOption, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, loadFromSystemProperties, logName, notifyAll, logDeprecationWarning, isInstanceOf, setSparkHome, <init>, getSizeAsBytes, remove, toDebugString, getAppId, getAvroSchema, getenv, ==, clone, getInt, isExecutorStartupConf, $init$, getSizeAsKb, toString, getBoolean, setAll, logError, !=, setJars, get, registerAvroSchemas, getClass, setExecutorEnv, logWarning, setMaster, getExecutorEnv, contains, ne, getWithSubstitution, eq, log, setAppName, ##, finalize, getLong, hashCode, logDebug, validateSettings, getAllWithPrefix, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala: Set(SparkConf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(SparkConf, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, remove, ==, clone, getInt, toString, getBoolean, logError, !=, get, logWarning, contains, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala: Set(SparkConf, <init>, get, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(getTimeAsMs, SparkConf, getAll, asInstanceOf, getDouble, synchronized, isInstanceOf, <init>, remove, ==, getInt, toString, logError, !=, get, logWarning, contains, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(getTimeAsMs, SparkConf, asInstanceOf, getDouble, set, synchronized, isInstanceOf, <init>, remove, ==, toString, getBoolean, logError, !=, get, logWarning, contains, ne, getLong, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(SparkConf, asInstanceOf, getOption, isInstanceOf, <init>, ==, isExecutorStartupConf, toString, logError, !=, get, logWarning, contains, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, !=, get, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala: Set(SparkConf, <init>, ==, getBoolean, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(SparkConf, asInstanceOf, getOption, <init>, ==, logError, !=, get, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(SparkConf, synchronized, <init>, toString, getBoolean, logError, !=, get, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(setIfMissing, SparkConf, asInstanceOf, set, isInstanceOf, <init>, ==, isExecutorStartupConf, logError, !=, get, logWarning, contains, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala: Set(SparkConf, <init>, getBoolean)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala: Set(SparkConf, getAll, asInstanceOf, set, synchronized, loadFromSystemProperties, isInstanceOf, <init>, getenv, ==, toString, !=, get, logWarning, contains, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, remove, ==, toString, logError, get, contains, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(SparkConf, getAll, asInstanceOf, getOption, isInstanceOf, <init>, ==, logError, !=, get, logWarning, contains, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(setIfMissing, SparkConf, getAll, asInstanceOf, set, synchronized, getOption, isInstanceOf, setSparkHome, <init>, remove, toDebugString, getenv, ==, clone, toString, getBoolean, logError, !=, setJars, get, getClass, setExecutorEnv, logWarning, setMaster, getExecutorEnv, contains, ne, log, setAppName, logDebug, validateSettings, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(setIfMissing, SparkConf, set, <init>, clone, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(SparkConf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, getInt, toString, !=, get, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(setIfMissing, getTimeAsSeconds, SparkConf, getAll, equals, asInstanceOf, set, synchronized, getOption, logTrace, isInstanceOf, <init>, getenv, ==, getInt, toString, getBoolean, logError, !=, get, getClass, logWarning, contains, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala: Set(SparkConf, getOption, <init>, ==, logError, !=, get, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, get, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala: Set(getTimeAsMs, SparkConf, synchronized, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala: Set(SparkConf, getOption, <init>, get, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(SparkConf, asInstanceOf, set, synchronized, getOption, isInstanceOf, <init>, getInt, toString, getBoolean, get, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(getTimeAsMs, SparkConf, asInstanceOf, set, logTrace, isInstanceOf, <init>, remove, ==, logError, !=, get, logWarning, contains, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(setIfMissing, SparkConf, getAll, asInstanceOf, initializeLogIfNecessary, set, getOption, isInstanceOf, <init>, remove, ==, toString, !=, get, getClass, contains, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala: Set(SparkConf, getAll, asInstanceOf, getOption, isInstanceOf, <init>, ==, toString, logError, !=, get, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(SparkConf, <init>, ==, getBoolean)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, getSizeAsBytes, remove, ==, toString, !=, get, logWarning, contains, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, !=, get, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(SparkConf, <init>, getBoolean, logError, get, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(getTimeAsSeconds, SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, remove, ==, getInt, getBoolean, !=, get, logWarning, contains, ne, getLong, logDebug, validateSettings, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(SparkConf, asInstanceOf, set, isInstanceOf, <init>, remove, ==, toString, logError, get, logWarning, contains, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala: Set(SparkConf, <init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(SparkConf, getAll, asInstanceOf, synchronized, isInstanceOf, <init>, remove, getenv, ==, getInt, toString, getBoolean, logError, !=, get, logWarning, contains, ne, log, getLong, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala: Set(SparkConf, asInstanceOf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(SparkConf, <init>, getenv, ==, toString, logError, !=, get, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala: Set(SparkConf, <init>, getInt, toString, getBoolean, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(SparkConf, getAll, asInstanceOf, set, synchronized, isInstanceOf, <init>, ==, toString, getBoolean, !=, get, getClass, logWarning, contains, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(SparkConf, synchronized, <init>, ==, logError, !=, get, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, ==, getSizeAsKb, getBoolean, !=, get, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala: Set(SparkConf, <init>, get, contains, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(SparkConf, asInstanceOf, getOption, isInstanceOf, <init>, ==, getInt, toString, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, remove, ==, getInt, logError, !=, get, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala: Set(SparkConf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, remove, getBoolean, get, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala: Set(SparkConf, getOption, <init>, getenv, ==, toString, getBoolean, !=, get, logWarning, contains, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala: Set(SparkConf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(SparkConf, asInstanceOf, set, synchronized, isInstanceOf, <init>, ==, clone, getInt, toString, getBoolean, !=, get, getClass, logWarning, contains, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(SparkConf, <init>, ==, getInt, getBoolean, get, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, remove, ==, toString, logError, !=, get, logWarning, contains, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(SparkConf, synchronized, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(SparkConf, <init>, getInt, logError, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(SparkConf, isInstanceOf, <init>, ==, toString, get, getClass, logWarning, contains, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, getBoolean, contains)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(setIfMissing, SparkConf, getAll, asInstanceOf, set, synchronized, isInstanceOf, <init>, remove, ==, clone, getInt, toString, logError, !=, get, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(SparkConf, asInstanceOf, set, synchronized, isInstanceOf, <init>, ==, getInt, toString, getBoolean, logError, !=, get, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala: Set(SparkConf, asInstanceOf, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(SparkConf, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, remove, ==, getInt, toString, getBoolean, logError, !=, get, getClass, logWarning, contains, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala: Set(SparkConf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(SparkConf, logTrace, <init>, remove, logError, !=, get, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, getSizeAsBytes, ==, getInt, get, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala: Set(SparkConf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala: Set(SparkConf, set, <init>, ==, setMaster, ne, setAppName)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(SparkConf, asInstanceOf, synchronized, <init>, ==, getSizeAsKb, !=, get, getClass, logWarning, ne, getLong, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala: Set(SparkConf, asInstanceOf, <init>, ==, toString, get, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(SparkConf, getAll, asInstanceOf, getOption, isInstanceOf, <init>, ==, toString, !=, get, logWarning, contains, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, toString, getBoolean, get, contains, getWithSubstitution, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala: Set(SparkConf, getDeprecatedConfig, <init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(SparkConf, getDouble, synchronized, <init>, getSizeAsBytes, ==, contains, ne, getLong, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala: Set(SparkConf, <init>, ==, !=, get, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(SparkConf, <init>, ==, toString, getBoolean, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(SparkConf, equals, asInstanceOf, isInstanceOf, <init>, remove, ==, toString, !=, get, getClass, contains, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala: Set(SparkConf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala: Set(SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, get, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, getenv, ==, !=, get, contains)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(setIfMissing, SparkConf, asInstanceOf, set, setSparkHome, <init>, ==, toString, !=, get, setExecutorEnv, setMaster, ne, setAppName)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(SparkConf, <init>, remove, getenv, ==, logError, get, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala: Set(SparkConf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, remove, ==, getInt, logError, !=, get, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala: Set(SparkConf, getAll, <init>, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(SparkConf, logTrace, <init>, getAppId, logError, !=, getClass, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(SparkConf, getOption, <init>, get, logWarning, contains, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala: Set(SparkConf, getOption, isInstanceOf, <init>, !=, get, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(SparkConf, asInstanceOf, set, isInstanceOf, <init>, ==, toString, getBoolean, !=, get, logWarning, contains, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(SparkConf, equals, asInstanceOf, set, synchronized, isInstanceOf, <init>, ==, toString, getBoolean, !=, get, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, remove, ==, logError, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, getenv, ==, !=, get, logWarning, contains)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(SparkConf, asInstanceOf, logName, isInstanceOf, <init>, ==, getInt, toString, !=, get, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(SparkConf, <init>, get, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala: Set(SparkConf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala: Set(setIfMissing, SparkConf, getAll, <init>, clone, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala: Set(getTimeAsMs, SparkConf, <init>, getInt, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(SparkConf, <init>, ==, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(SparkConf, getDouble, synchronized, <init>, getSizeAsBytes, ==, !=, contains, getLong, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, ==, getSizeAsKb, toString, !=, get, logWarning, ne, eq, getLong, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala: Set(SparkConf, initializeLogIfNecessary, <init>, ==, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(SparkConf, asInstanceOf, <init>, setMaster, setAppName)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, get, contains, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, SparkConf, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, ==, getInt, toString, getBoolean, !=, get, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, getenv, ==, getInt, toString, getBoolean, logError, !=, get, logWarning, contains, ne, log, getLong, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(getTimeAsMs, wait, SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, remove, ==, getInt, toString, getBoolean, logError, !=, get, logWarning, contains, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(SparkConf, asInstanceOf, logName, isInstanceOf, <init>, ==, toString, !=, get, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(getTimeAsMs, wait, SparkConf, asInstanceOf, set, synchronized, notifyAll, isInstanceOf, <init>, getSizeAsBytes, remove, getAppId, ==, getInt, getBoolean, logError, !=, get, logWarning, contains, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala: Set(SparkConf, asInstanceOf, set, isInstanceOf, <init>, ==, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala: Set(SparkConf, synchronized, <init>, ==, get, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(SparkConf, getSizeAsMb, asInstanceOf, isInstanceOf, <init>, ==, getInt, getBoolean, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(SparkConf, synchronized, notifyAll, <init>, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, getInt, getBoolean)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(getTimeAsSeconds, SparkConf, asInstanceOf, set, synchronized, isInstanceOf, <init>, remove, ==, clone, getInt, toString, getBoolean, logError, !=, get, getClass, logWarning, contains, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(SparkConf, set, <init>, ==, getInt, getBoolean, !=, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(getTimeAsSeconds, SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, remove, ==, toString, getBoolean, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(SparkConf, synchronized, <init>, toString, getBoolean, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(SparkConf, asInstanceOf, set, isInstanceOf, <init>, toString, setAll, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(SparkConf, getDouble, synchronized, <init>, getSizeAsBytes, ==, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, contains, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(SparkConf, asInstanceOf, set, isInstanceOf, <init>, ==, !=, get, logWarning, contains, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, remove, ==, toString, get, getClass, logWarning, contains, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, !=, get, contains, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(SparkConf, asInstanceOf, initializeLogIfNecessary, <init>, ==, getInt, logError, !=, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala: Set(SparkConf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, getInt, logError, !=, get, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, SparkConf, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, getSizeAsBytes, remove, ==, getInt, toString, getBoolean, logError, !=, get, contains, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala: Set(SparkConf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala: Set(SparkConf, getAll, <init>, get, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(SparkConf, asInstanceOf, synchronized, isInstanceOf, <init>, remove, ==, toString, !=, get, logWarning, contains, eq, getLong, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala: Set(SparkConf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(SparkConf, asInstanceOf, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala: Set(getTimeAsSeconds, SparkConf, asInstanceOf, getOption, isInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(getTimeAsMs, getTimeAsSeconds, SparkConf, asInstanceOf, logTrace, isInstanceOf, <init>, remove, ==, toString, !=, logWarning, contains, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(SparkConf, asInstanceOf, <init>, getInt, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(SparkConf, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(SparkConf, isInstanceOf, <init>, ==, logError, !=, get, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(SparkConf, <init>, ==, !=, get, getLong, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(getTimeAsMs, SparkConf, synchronized, <init>, ==, !=, get, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(SparkConf, synchronized, <init>, clone, getInt, logError, !=, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(SparkConf, equals, asInstanceOf, set, synchronized, isInstanceOf, <init>, ==, toString, getBoolean, !=, get, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(SparkConf, asInstanceOf, set, isInstanceOf, <init>, ==, logError, get, logWarning, contains, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(SparkConf, <init>, ==, toString, contains)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala: Set(SparkConf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(SparkConf, getSizeAsMb, asInstanceOf, isInstanceOf, <init>, getAvroSchema, ==, getSizeAsKb, getBoolean, logError, !=, get, getClass, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, readExternal, wait, apply$default$4, writeExternal, $asInstanceOf, fromString, isValid, MEMORY_ONLY_2, equals, description, asInstanceOf, getCachedStorageLevel, synchronized, $isInstanceOf, deserialized, useOffHeap, MEMORY_AND_DISK_SER, notifyAll, MEMORY_AND_DISK_2, isInstanceOf, useDisk, replication, <init>, useMemory, DISK_ONLY, apply, NONE, ==, toInt, OFF_HEAP, clone, MEMORY_AND_DISK, storageLevelCache, DISK_ONLY_2, MEMORY_AND_DISK_SER_2, toString, !=, getClass, MEMORY_ONLY_SER_2, StorageLevel, MEMORY_ONLY_SER, ne, eq, memoryMode, MEMORY_ONLY, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, apply, NONE, ==, clone, toString, !=, StorageLevel, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(writeExternal, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, StorageLevel, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, toInt, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala: Set(asInstanceOf, useOffHeap, isInstanceOf, <init>, apply, NONE, ==, toString, !=, StorageLevel, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala: Set(asInstanceOf, <init>, apply, toString, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala: Set(StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, MEMORY_AND_DISK_SER, isInstanceOf, <init>, apply, ==, toInt, MEMORY_AND_DISK, !=, StorageLevel, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(description, <init>, apply, ==, toString, !=, StorageLevel, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(description, asInstanceOf, synchronized, isInstanceOf, <init>, apply, NONE, ==, toInt, clone, toString, !=, getClass, StorageLevel, ne, MEMORY_ONLY)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(isValid, asInstanceOf, isInstanceOf, useDisk, <init>, useMemory, apply, NONE, ==, toString, !=, StorageLevel, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, apply, ==, !=, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(isValid, asInstanceOf, synchronized, deserialized, useOffHeap, isInstanceOf, useDisk, replication, <init>, useMemory, apply, ==, toInt, toString, !=, getClass, StorageLevel, ne, memoryMode, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, NONE, ==, toString, !=, StorageLevel, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(deserialized, useDisk, replication, <init>, useMemory, apply, MEMORY_AND_DISK, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(isValid, equals, description, asInstanceOf, deserialized, useOffHeap, isInstanceOf, useDisk, <init>, useMemory, apply, ==, toInt, toString, !=, getClass, StorageLevel, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(<init>, apply, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(asInstanceOf, <init>, apply, StorageLevel, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(<init>, apply, !=, getClass, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, StorageLevel, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, deserialized, isInstanceOf, <init>, apply, ==, toString, !=, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(<init>, apply, ==, !=, StorageLevel, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, MEMORY_AND_DISK_SER, notifyAll, isInstanceOf, <init>, apply, ==, !=, StorageLevel, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(asInstanceOf, <init>, apply, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala: Set(<init>, apply, ==, !=, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(isValid, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, toInt, toString, !=, StorageLevel, eq, memoryMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala: Set(<init>, apply, toString, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, StorageLevel, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, deserialized, isInstanceOf, <init>, apply, ==, toString, !=, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toInt, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(description, asInstanceOf, deserialized, isInstanceOf, useDisk, replication, <init>, useMemory, apply, ==, toString, !=, StorageLevel, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala: Set(<init>, NONE, ==, StorageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, currentResult, notifyAll, isInstanceOf, <init>, merge, ==, clone, MeanEvaluator, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, merge, ==, MeanEvaluator, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, name, wait, onNetworkError, $asInstanceOf, compose, productArity, equals, NAME, asInstanceOf, synchronized, self, $isInstanceOf, andThen, receive, canEqual, productPrefix, stop, notifyAll, CheckExistence, isInstanceOf, <init>, onError, apply, ==, receiveAndReply, RpcEndpointVerifier, clone, $init$, onDisconnected, copy, toString, !=, onConnected, getClass, copy$default$1, onStop, ne, onStart, rpcEnv, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(unapply, name, NAME, asInstanceOf, synchronized, stop, CheckExistence, isInstanceOf, <init>, apply, ==, RpcEndpointVerifier, clone, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, registry, wait, $asInstanceOf, JmxSink, equals, reporter, asInstanceOf, synchronized, $isInstanceOf, stop, notifyAll, isInstanceOf, <init>, ==, clone, report, toString, property, !=, getClass, start, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, getConf, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, CONFIGURATION_INSTANTIATION_LOCK, name, count, wait, NewHadoopMapPartitionsWithSplitRDD, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, mapPartitionsWithInputSplit$default$2, union, coalesce$default$3, NewHadoopPartition, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, <init>$default$3, NewHadoopRDD, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, jobId, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, serializableHadoopSplit, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, mapPartitionsWithInputSplit, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, getConf, parent, partitioner, name, setName, union, map, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, NewHadoopRDD, <init>, id, max, toDebugString, ++, flatMap, ==, clone, foreach, jobId, zipWithIndex, first, toString, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, index, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(getConf, NewHadoopPartition, asInstanceOf, conf, isInstanceOf, NewHadoopRDD, <init>, id, foreach, jobId, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(getConf, name, union, map, asInstanceOf, conf, NewHadoopRDD, <init>, ++, first)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(getConf, NewHadoopPartition, asInstanceOf, conf, NewHadoopRDD, <init>, id, foreach, jobId, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala: Set(asInstanceOf, NewHadoopRDD, <init>, mapPartitionsWithInputSplit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, getConf, parent, partitioner, name, setName, union, map, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, NewHadoopRDD, <init>, id, max, toDebugString, ++, flatMap, ==, clone, foreach, jobId, zipWithIndex, first, toString, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, index, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, getConf, parent, partitioner, name, setName, union, map, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, NewHadoopRDD, <init>, id, max, toDebugString, ++, flatMap, ==, clone, foreach, jobId, zipWithIndex, first, toString, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, index, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, shuffleMetrics, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, fetchBlocks, synchronized, $isInstanceOf, hostName, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, uploadBlockSync, isInstanceOf, <init>, port, ==, clone, uploadBlock, $init$, NettyBlockTransferService, toString, logError, !=, fetchBlockSync, getClass, logWarning, close, ne, init, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, port, ==, NettyBlockTransferService, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, duration, prevPageSizeFormField, submissionTime, wait, pageLink, $asInstanceOf, jobData, pageSize, pageSizeFormField, pageNumberFormField, equals, prefix, jobDescription, asInstanceOf, JobPagedTable, synchronized, pageData, $isInstanceOf, lastStageName, JobDataSource, row, tableCssClass, dataSource, notifyAll, goButtonFormPath, isInstanceOf, <init>, JobTableRowData, ==, sliceData, clone, parameterPath, $init$, renderJson, tableId, AllJobsPage, formattedSubmissionTime, toString, !=, getClass, lastStageDescription, headers, pageNavigation, render, ne, dataSize, formattedDuration, detailUrl, eq, ##, finalize, table, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(<init>, ==, AllJobsPage, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction2.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction2.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction2.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction2.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction2.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, VoidFunction2.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction2.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	rollingPolicy, notify, wait, <init>$default$5, GZIP_LOG_SUFFIX, $asInstanceOf, equals, deleteOldFiles, asInstanceOf, initializeLogIfNecessary, synchronized, appendStreamToFile, INTERVAL_DEFAULT, SIZE_DEFAULT, $isInstanceOf, INTERVAL_PROPERTY, closeFile, DEFAULT_BUFFER_SIZE, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, appendToFile, isInstanceOf, <init>, STRATEGY_PROPERTY, RETAINED_FILES_PROPERTY, ==, clone, openFile, $init$, SIZE_PROPERTY, ENABLE_COMPRESSION, toString, awaitTermination, logError, !=, getClass, logWarning, STRATEGY_DEFAULT, ne, RollingFileAppender, getSortedRolledOverFiles, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala: Set(DEFAULT_BUFFER_SIZE, <init>, logWarning, RollingFileAppender, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, logError, ne, RollingFileAppender, getSortedRolledOverFiles, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(appendStreamToFile, INTERVAL_DEFAULT, INTERVAL_PROPERTY, closeFile, appendToFile, isInstanceOf, <init>, STRATEGY_PROPERTY, ==, openFile, SIZE_PROPERTY, logError, !=, logWarning, STRATEGY_DEFAULT, ne, RollingFileAppender, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, LocalSparkCluster, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, masterWebUIPort, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, start, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(LocalSparkCluster, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, start, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFlatMapFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFlatMapFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFlatMapFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFlatMapFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFlatMapFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, DoubleFlatMapFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFlatMapFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(call, DoubleFlatMapFunction)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IntParam.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IntParam.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IntParam.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IntParam.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IntParam.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, IntParam, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala: Set(unapply, asInstanceOf, IntParam, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala: Set(unapply, asInstanceOf, IntParam, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala: Set(unapply, asInstanceOf, IntParam, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(unapply, IntParam, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskState.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskState.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unpersist, wait, $asInstanceOf, TorrentBroadcast, doDestroy, isValid, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, doUnpersist, assertValid, <init>, id, destroy, ==, clone, $init$, toString, logError, !=, getClass, getValue, logWarning, ne, value, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala: Set(unpersist, TorrentBroadcast, <init>, id)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, getProgress, notifyAll, initialize, isInstanceOf, getCurrentValue, <init>, nextKeyValue, ==, clone, toString, FixedLengthBinaryRecordReader, !=, getCurrentKey, getClass, close, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala: Set(<init>, ==, FixedLengthBinaryRecordReader)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashMap.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, find, span, flatten, toBuffer, count, move, reduceOption, sliceWithKnownDelta, wait, foldRight, takeWhile, stringPrefix, $asInstanceOf, splitAt, minBy, size, inits, zip, toSet, :\, map, takeRight, dropWhile, toMap, filterNot, equals, par, unzip3, repr, toList, isTraversableAgain, OpenHashMap, head, asInstanceOf, sameElements, unzip, reduceLeftOption, synchronized, sliding, Self, partition, aggregate, $isInstanceOf, forall, newBuilder, mkString, min, scanRight, fold, scan, nonEmpty, canEqual, tail, lastOption, dropRight, iterator, last, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, ++:, _keySet, <init>, toStream, companion, max, tails, apply, ++, grouped, flatMap, take, parCombiner, reduceRight, grow, groupBy, ==, maxBy, sliceWithKnownBound, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, changeValue, $init$, toSeq, zipWithIndex, toString, genericBuilder, copyToArray, seq, !=, transpose, collect, headOption, getClass, WithFilter, update, hasDefiniteSize, foldLeft, contains, toCollection, isEmpty, ne, init, reversed, reduceLeft, eq, sum, thisCollection, ##, scanLeft, finalize, hashCode, zipAll, product, view.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, OpenHashMap, head, asInstanceOf, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, iterator, last, isInstanceOf, filter, <init>, max, apply, ++, grouped, flatMap, take, groupBy, ==, maxBy, clone, foreach, exists, toArray, reduce, changeValue, toSeq, zipWithIndex, toString, !=, collect, getClass, contains, isEmpty, ne, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala: Set(map, toMap, OpenHashMap, <init>, ==, foreach, changeValue, ne, sum)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, toList, asInstanceOf, synchronized, $isInstanceOf, toScalaMap, PythonUtils, notifyAll, isInstanceOf, ==, clone, sparkPythonPath, toArray, toSeq, generateRDDWithNull, toString, !=, getClass, ne, mergePythonPaths, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(synchronized, PythonUtils, sparkPythonPath, toString, !=, mergePythonPaths)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala: Set(PythonUtils, ==, sparkPythonPath, !=, mergePythonPaths)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, doTrace, statusRequestServlet, wait, contextToServlet, sendResponse, $asInstanceOf, equals, requestedPort, findUnknownFields, asInstanceOf, initializeLogIfNecessary, host, handleSubmit, StandaloneStatusRequestServlet, synchronized, $isInstanceOf, getServletContext, logTrace, killRequestServlet, isTraceEnabled, initializeLogIfNecessary$default$2, doHead, service, stop, logName, notifyAll, masterConf, isInstanceOf, doPut, getInitParameter, handleError, getServletInfo, <init>, destroy, getServletName, StandaloneKillRequestServlet, formatException, handleKill, doGet, ==, clone, StandaloneRestServer, $init$, submitRequestServlet, getLastModified, toString, logError, !=, getClass, logWarning, getInitParameterNames, start, getServletConfig, baseContext, ne, parseSubmissionId, doDelete, init, handleStatus, StandaloneSubmitRequestServlet, eq, log, doOptions, ##, finalize, hashCode, logDebug, doPost, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, host, stop, isInstanceOf, <init>, ==, StandaloneRestServer, toString, logError, !=, logWarning, start, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, wait, copy$default$2, $asInstanceOf, fromSparkURL, productArity, equals, hostPort, asInstanceOf, host, synchronized, $isInstanceOf, toSparkURL, canEqual, productPrefix, notifyAll, fromURIString, isInstanceOf, <init>, port, apply, ==, clone, $init$, copy, toString, !=, getClass, copy$default$1, ne, RpcAddress, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, host, synchronized, isInstanceOf, <init>, apply, ==, toString, !=, ne, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(unapply, asInstanceOf, isInstanceOf, <init>, apply, ==, !=, ne, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(<init>, port, apply, clone, ne, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala: Set(unapply, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, toString, !=, ne, RpcAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, host, isInstanceOf, <init>, port, apply, ==, !=, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(unapply, fromSparkURL, asInstanceOf, host, synchronized, toSparkURL, isInstanceOf, <init>, port, apply, ==, toString, !=, ne, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(asInstanceOf, host, isInstanceOf, <init>, port, ==, toString, !=, RpcAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, port, apply, ==, toString, !=, getClass, ne, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(unapply, asInstanceOf, synchronized, isInstanceOf, <init>, ==, !=, ne, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(hostPort, asInstanceOf, host, isInstanceOf, <init>, port, ==, toString, RpcAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(unapply, asInstanceOf, host, synchronized, isInstanceOf, <init>, port, apply, ==, clone, toString, !=, ne, RpcAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala: Set(<init>, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala: Set(asInstanceOf, <init>, !=, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(unapply, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, RpcAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala: Set(RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala: Set(asInstanceOf, host, isInstanceOf, <init>, port, ==, toString, !=, RpcAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala: Set(asInstanceOf, <init>, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala: Set(<init>, apply, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala: Set(<init>, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(hostPort, asInstanceOf, host, toSparkURL, isInstanceOf, <init>, port, apply, ==, toString, !=, ne, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala: Set(toSparkURL, <init>, apply, ==, !=, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(unapply, fromSparkURL, hostPort, asInstanceOf, host, toSparkURL, isInstanceOf, <init>, apply, ==, !=, ne, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, !=, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(unapply, wait, hostPort, asInstanceOf, host, synchronized, notifyAll, isInstanceOf, <init>, apply, ==, toString, !=, ne, RpcAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(fromSparkURL, hostPort, asInstanceOf, isInstanceOf, <init>, apply, ==, ne, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala: Set(asInstanceOf, fromURIString, <init>, ==, RpcAddress)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskKilledException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskKilledException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskKilledException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskKilledException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskKilledException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, TaskKilledException, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, reason, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(TaskKilledException, synchronized, reason, <init>, getMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(TaskKilledException, wait, asInstanceOf, synchronized, reason, notifyAll, isInstanceOf, <init>, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(TaskKilledException, asInstanceOf, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/HashMapGrowthStrategy.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/HashMapGrowthStrategy.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/HashMapGrowthStrategy.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/HashMapGrowthStrategy.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/HashMapGrowthStrategy.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $assertionsDisabled, equals, HashMapGrowthStrategy$Doubling, notifyAll, <init>, DOUBLING, HashMapGrowthStrategy, nextCapacity, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/HashMapGrowthStrategy.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, onHeapMemUsed, numBlocks, StorageStatus, wait, rddStorageLevel, memUsed, $asInstanceOf, addBlock, getBlock, diskUsed, equals, StorageUtils, cacheSize, blocks, asInstanceOf, rddBlocksById, initializeLogIfNecessary, offHeapMemRemaining, synchronized, offHeapMemUsed, $isInstanceOf, removeBlock, updateRddInfo, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, rddBlocks, logName, notifyAll, numRddBlocksById, isInstanceOf, maxOffHeapMem, blockManagerId, <init>, onHeapMemRemaining, containsBlock, numRddBlocks, ==, dispose, memRemaining, clone, $init$, offHeapCacheSize, toString, diskUsedByRdd, logError, !=, getClass, logWarning, memUsedByRdd, ne, maxMem, maxMemory, updateBlock, eq, maxOnHeapMem, log, ##, finalize, getRddBlockLocations, hashCode, onHeapCacheSize, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(StorageStatus, StorageUtils, asInstanceOf, synchronized, updateRddInfo, isInstanceOf, blockManagerId, <init>, ==, clone, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala: Set(onHeapMemUsed, StorageStatus, memUsed, diskUsed, offHeapMemRemaining, offHeapMemUsed, maxOffHeapMem, <init>, onHeapMemRemaining, memRemaining, maxMem, maxOnHeapMem)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala: Set(StorageUtils, asInstanceOf, <init>, ==, dispose, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(StorageStatus, blocks, asInstanceOf, removeBlock, isInstanceOf, maxOffHeapMem, blockManagerId, <init>, ==, toString, logError, !=, logWarning, ne, maxMem, eq, maxOnHeapMem, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(StorageStatus, blockManagerId, <init>, ==, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala: Set(StorageUtils, <init>, ==, dispose)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, memoryUsed, equals, memoryFree, asInstanceOf, initializeLogIfNecessary, synchronized, poolSize, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, releaseAllMemory, logName, notifyAll, releaseMemory, isInstanceOf, decrementPoolSize, <init>, setMemoryStore, incrementPoolSize, freeSpaceToShrinkPool, ==, clone, acquireMemory, $init$, memoryStore, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, StorageMemoryPool, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(memoryUsed, memoryFree, synchronized, poolSize, decrementPoolSize, <init>, incrementPoolSize, freeSpaceToShrinkPool, ==, acquireMemory, ne, StorageMemoryPool, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(memoryUsed, synchronized, poolSize, releaseAllMemory, releaseMemory, <init>, setMemoryStore, incrementPoolSize, ==, StorageMemoryPool)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(memoryFree, synchronized, poolSize, decrementPoolSize, <init>, incrementPoolSize, ==, acquireMemory, memoryStore, !=, StorageMemoryPool, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, secret, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, readUtf8, authToServer, notifyAll, isInstanceOf, writeUtf8, <init>, ==, RAuthHelper, clone, authClient, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(secret, asInstanceOf, synchronized, isInstanceOf, <init>, ==, RAuthHelper, authClient, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(secret, <init>, ==, RAuthHelper, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(secret, asInstanceOf, <init>, ==, RAuthHelper, authClient, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, offHeapStorageMemoryPool, releaseStorageMemory, wait, releaseExecutionMemory, $asInstanceOf, maxOnHeapStorageMemory, maxHeapMemory, releaseAllStorageMemory, equals, acquireUnrollMemory, acquireExecutionMemory, offHeapExecutionMemoryPool, asInstanceOf, initializeLogIfNecessary, releaseAllExecutionMemoryForTask, synchronized, $isInstanceOf, pageSizeBytes, logTrace, getExecutionMemoryUsageForTask, storageMemoryUsed, isTraceEnabled, initializeLogIfNecessary$default$2, releaseUnrollMemory, logName, notifyAll, UnifiedMemoryManager, isInstanceOf, acquireStorageMemory, <init>, setMemoryStore, apply, ==, maxOffHeapStorageMemory, clone, $init$, onHeapStorageMemoryPool, tungstenMemoryMode, offHeapStorageMemory, toString, onHeapExecutionMemoryPool, logError, !=, maxOffHeapMemory, getClass, logWarning, tungstenMemoryAllocator, ne, eq, log, executionMemoryUsed, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, UnifiedMemoryManager, isInstanceOf, <init>, apply, ==, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, executorEndpoint, canEqual, ExecutorData, notifyAll, isInstanceOf, executorHost, <init>, ==, clone, executorAddress, freeCores, totalCores, toString, !=, getClass, ne, eq, ##, finalize, logUrlMap, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, executorEndpoint, ExecutorData, isInstanceOf, executorHost, <init>, ==, executorAddress, freeCores, totalCores, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, HadoopDelegationTokenProvider, delegationTokensRequired, wait, $asInstanceOf, equals, asInstanceOf, serviceName, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, obtainDelegationTokens, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala: Set(HadoopDelegationTokenProvider, asInstanceOf, serviceName, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(HadoopDelegationTokenProvider, serviceName, isInstanceOf, ==, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala: Set(HadoopDelegationTokenProvider, asInstanceOf, serviceName, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(HadoopDelegationTokenProvider, delegationTokensRequired, serviceName, obtainDelegationTokens)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(HadoopDelegationTokenProvider, delegationTokensRequired, serviceName, obtainDelegationTokens)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(HadoopDelegationTokenProvider, delegationTokensRequired, serviceName, obtainDelegationTokens)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(HadoopDelegationTokenProvider, delegationTokensRequired, serviceName, obtainDelegationTokens)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	appSparkVersion, notify, name, wait, $asInstanceOf, equals, appName, prefix, asInstanceOf, synchronized, $isInstanceOf, pages, attachPage, basePath, notifyAll, isInstanceOf, <init>, StorageTab, ==, clone, toString, !=, getClass, headerTabs, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appSparkVersion, appName, prefix, basePath, <init>, StorageTab, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, PoolTable, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, toNodeSeq, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(PoolTable, <init>, ==, toNodeSeq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(PoolTable, <init>, toNodeSeq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorBackend.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, ExecutorBackend, isInstanceOf, statusUpdate, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, ExecutorBackend, isInstanceOf, statusUpdate, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, ExecutorBackend, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, ExecutorBackend, isInstanceOf, statusUpdate, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	used, notify, wait, $asInstanceOf, equals, taskMemoryManager, asInstanceOf, initializeLogIfNecessary, synchronized, diskBytesSpilled, allocateArray, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, partitionedIterator, iterator, maybeSpill, stop, logName, notifyAll, allocatePage, releaseMemory, insertAll, isInstanceOf, freePage, <init>, elementsRead, ==, clone, memoryBytesSpilled, destructiveIterator, acquireMemory, $init$, freeArray, _spillCount, writePartitionedFile, toString, logError, !=, freeMemory, spill, getClass, addElementsRead, ExternalSorter, logWarning, getMode, ne, protected$getUsed, eq, forceSpill, log, peakMemoryUsedBytes, ##, finalize, hashCode, getUsed, logDebug, numSpills, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(stop, insertAll, <init>, writePartitionedFile, logError, !=, ExternalSorter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, diskBytesSpilled, iterator, stop, insertAll, isInstanceOf, <init>, ==, memoryBytesSpilled, ExternalSorter, ne, peakMemoryUsedBytes)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, reduceLeftOption, synchronized, sliding, CompletionIterator, completion, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, fold, nonEmpty, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, GroupedIterator, <init>, toStream, max, buffered, apply, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, toString, copyToArray, length, seq, !=, collect, getClass, hasDefiniteSize, patch, foldLeft, contains, isEmpty, ne, withPartial, reversed, hasNext, indexOf, reduceLeft, eq, sum, ##, scanLeft, finalize, hashCode, zipAll, product.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(size, map, asInstanceOf, synchronized, CompletionIterator, mkString, nonEmpty, <init>, buffered, apply, ++, ==, foreach, exists, next, length, !=, getClass, isEmpty, ne, hasNext, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(withFilter, size, map, toMap, filterNot, asInstanceOf, synchronized, CompletionIterator, partition, mkString, toIterator, to, isInstanceOf, filter, <init>, max, apply, ++, flatMap, ==, foreach, exists, toArray, toSeq, next, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(map, asInstanceOf, CompletionIterator, isInstanceOf, <init>, apply, flatMap, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	SparkExecutorInfo, cacheSize, host, numRunningTasks, port.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(SparkExecutorInfo, host, port)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala: Set(SparkExecutorInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkExecutorInfo.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala: Set(SparkExecutorInfo, cacheSize, host, numRunningTasks, port)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(SparkExecutorInfo, host, port)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	TASK_SIZE_TO_WARN_KB, notify, maxResultSize, speculationEnabled, ser, weight, parent, partitionToIndex, name, handleTaskGettingResult, priority, wait, abortIfCompletelyBlacklisted, <init>$default$5, emittedTaskSizeWarning, $asInstanceOf, SPECULATION_MULTIPLIER, someAttemptSucceeded, epoch, myLocalityLevels, runningTasks, equals, addPendingTask, taskSet, canFetchMoreResults, stageId, asInstanceOf, initializeLogIfNecessary, getSortedTaskSetQueue, taskInfos, synchronized, schedulableQueue, $isInstanceOf, <init>$default$4, checkSpeculatableTasks, maxTaskFailures, EXCEPTION_PRINT_INTERVAL, localityWaits, logTrace, TaskSetManager, executorAdded, isTraceEnabled, pendingTasksWithNoPrefs, initializeLogIfNecessary$default$2, successfulTaskDurations, dequeueSpeculativeTask, logName, notifyAll, isInstanceOf, SPECULATION_QUANTILE, <init>, addSchedulable, runningTasksSet, getSchedulableByName, markPartitionCompleted, tasks, successful, handleFailedTask, minShare, ==, executorLost, clone, numTasks, getLocalityIndex, $init$, copiesRunning, removeSchedulable, resourceOffer, toString, taskAttempts, logError, taskSetBlacklistHelperOpt, !=, removeRunningTask, isZombie, addRunningTask, recomputeLocality, getClass, logWarning, abort$default$2, speculatableTasks, schedulingMode, ne, eq, log, env, ##, finalize, handleSuccessfulTask, hashCode, abort, logDebug, logInfo, tasksSuccessful.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, TaskSetManager, isInstanceOf, <init>, tasks, successful, ==, executorLost, toString, logError, !=, logWarning, ne, env, abort, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala: Set(TaskSetManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala: Set(weight, parent, name, priority, runningTasks, stageId, getSortedTaskSetQueue, schedulableQueue, checkSpeculatableTasks, TaskSetManager, <init>, getSchedulableByName, minShare, ==, executorLost, !=, schedulingMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(parent, name, handleTaskGettingResult, wait, abortIfCompletelyBlacklisted, someAttemptSucceeded, myLocalityLevels, runningTasks, taskSet, stageId, asInstanceOf, getSortedTaskSetQueue, synchronized, checkSpeculatableTasks, maxTaskFailures, TaskSetManager, executorAdded, isInstanceOf, <init>, runningTasksSet, markPartitionCompleted, tasks, handleFailedTask, ==, executorLost, removeSchedulable, resourceOffer, toString, logError, !=, removeRunningTask, isZombie, logWarning, schedulingMode, ne, env, handleSuccessfulTask, abort, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(name, handleTaskGettingResult, canFetchMoreResults, asInstanceOf, TaskSetManager, isInstanceOf, <init>, handleFailedTask, ==, logError, !=, ne, handleSuccessfulTask, abort, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, unpersist, addApplication, wait, $asInstanceOf, equals, addWorker, asInstanceOf, PersistenceEngine, synchronized, readPersistedData, $isInstanceOf, notifyAll, isInstanceOf, removeApplication, persist, <init>, removeDriver, ==, clone, BlackHolePersistenceEngine, toString, !=, getClass, close, ne, addDriver, eq, removeWorker, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(PersistenceEngine, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala: Set(PersistenceEngine, <init>, !=, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(PersistenceEngine, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala: Set(PersistenceEngine, <init>, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(addApplication, addWorker, asInstanceOf, PersistenceEngine, readPersistedData, isInstanceOf, removeApplication, <init>, removeDriver, ==, BlackHolePersistenceEngine, toString, !=, close, ne, addDriver, removeWorker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(PersistenceEngine, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, PartitionerAwareUnionRDD, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, parents, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, rdds, logError, !=, partitions, collect, getClass, PartitionerAwareUnionRDDPartition, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, parent, partitioner, name, PartitionerAwareUnionRDD, setName, union, map, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, <init>, id, max, toDebugString, ++, flatMap, ==, clone, foreach, zipWithIndex, first, toString, rdds, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, index, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toRDD, toScalaFunction2, pairFunToScalaFun, toScalaFunction, kClassTag, vClassTag.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, MemoryParam, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala: Set(unapply, asInstanceOf, isInstanceOf, MemoryParam, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala: Set(unapply, asInstanceOf, isInstanceOf, MemoryParam, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskNotSerializableException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskNotSerializableException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskNotSerializableException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskNotSerializableException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskNotSerializableException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, notifyAll, TaskNotSerializableException, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, TaskNotSerializableException, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, TaskNotSerializableException, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, onNetworkError, $asInstanceOf, equals, asInstanceOf, ThreadSafeRpcEndpoint, synchronized, self, RpcEnvFactory, $isInstanceOf, create, receive, stop, notifyAll, isInstanceOf, onError, ==, receiveAndReply, clone, $init$, onDisconnected, toString, !=, onConnected, getClass, onStop, RpcEndpoint, ne, onStart, rpcEnv, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, RpcEnvFactory, stop, isInstanceOf, ==, clone, toString, !=, RpcEndpoint, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, stop, isInstanceOf, ==, toString, !=, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, create, stop, isInstanceOf, ==, clone, toString, !=, getClass, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, create, stop, isInstanceOf, ==, clone, toString, !=, getClass, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, create, stop, isInstanceOf, ==, clone, toString, !=, getClass, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, create, stop, isInstanceOf, ==, toString, !=, getClass, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(self, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(clone, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, create, stop, isInstanceOf, ==, clone, toString, !=, getClass, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, create, stop, isInstanceOf, ==, toString, !=, getClass, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, toString, !=, getClass, RpcEndpoint, ne, rpcEnv, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(asInstanceOf, synchronized, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, isInstanceOf, ==, !=, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(asInstanceOf, create, isInstanceOf, ==, toString, !=, RpcEndpoint, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, !=, RpcEndpoint, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(onNetworkError, asInstanceOf, ThreadSafeRpcEndpoint, synchronized, receive, isInstanceOf, onError, ==, receiveAndReply, onDisconnected, toString, !=, onConnected, onStop, RpcEndpoint, onStart, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala: Set(==, !=, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(self, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala: Set(asInstanceOf, isInstanceOf, ==, toString, RpcEndpoint, rpcEnv, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, synchronized, self, stop, isInstanceOf, ==, toString, !=, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, self, create, stop, isInstanceOf, ==, !=, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, toString, RpcEndpoint, rpcEnv, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, create, stop, isInstanceOf, ==, clone, toString, !=, getClass, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(self, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, create, isInstanceOf, ==, !=, RpcEndpoint, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, synchronized, self, create, stop, isInstanceOf, ==, toString, !=, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(asInstanceOf, create, isInstanceOf, ==, toString, !=, RpcEndpoint, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, create, stop, isInstanceOf, ==, toString, !=, getClass, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, stop, isInstanceOf, ==, toString, RpcEndpoint, rpcEnv, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, !=, RpcEndpoint, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, stop, isInstanceOf, ==, toString, !=, ne, rpcEnv, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, RpcEnvFactory, stop, isInstanceOf, ==, clone, toString, !=, RpcEndpoint, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, toString, !=, getClass, RpcEndpoint, ne, rpcEnv, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(onNetworkError, asInstanceOf, ThreadSafeRpcEndpoint, synchronized, receive, isInstanceOf, onError, ==, receiveAndReply, onDisconnected, toString, !=, onConnected, onStop, RpcEndpoint, onStart, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, self, create, stop, isInstanceOf, ==, toString, !=, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, self, stop, isInstanceOf, ==, !=, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, isInstanceOf, ==, !=, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, stop, notifyAll, isInstanceOf, ==, toString, !=, RpcEndpoint, ne, rpcEnv, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, self, isInstanceOf, ==, toString, !=, ne, rpcEnv, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, self, create, isInstanceOf, ==, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala: Set(asInstanceOf, ==, RpcEndpoint, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(clone, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(self, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, synchronized, self, create, stop, isInstanceOf, ==, toString, !=, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, self, stop, isInstanceOf, ==, !=, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, self, create, isInstanceOf, ==, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, stop, isInstanceOf, ==, toString, !=, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, toString, !=, getClass, RpcEndpoint, ne, rpcEnv, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, create, stop, isInstanceOf, ==, toString, !=, getClass, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, stop, isInstanceOf, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, toString, !=, getClass, RpcEndpoint, ne, rpcEnv, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, stop, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, stop, notifyAll, isInstanceOf, ==, !=, ne, rpcEnv, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, stop, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, isInstanceOf, ==, !=, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, stop, notifyAll, isInstanceOf, ==, !=, ne, rpcEnv, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, create, stop, isInstanceOf, ==, clone, toString, !=, getClass, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, ThreadSafeRpcEndpoint, self, create, stop, isInstanceOf, ==, !=, RpcEndpoint, ne, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, create, isInstanceOf, ==, !=, RpcEndpoint, rpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveKeyOpenHashMap.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, find, span, flatten, toBuffer, count, move, reduceOption, sliceWithKnownDelta, wait, foldRight, takeWhile, stringPrefix, $asInstanceOf, splitAt, minBy, size, inits, zip, toSet, :\, map, takeRight, dropWhile, toMap, filterNot, equals, par, unzip3, repr, toList, isTraversableAgain, head, asInstanceOf, sameElements, unzip, reduceLeftOption, synchronized, sliding, Self, partition, aggregate, $isInstanceOf, forall, newBuilder, mkString, min, scanRight, fold, scan, nonEmpty, canEqual, tail, lastOption, dropRight, iterator, last, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, ++:, _keySet, <init>, toStream, companion, max, tails, apply, ++, grouped, flatMap, getOrElse, take, parCombiner, reduceRight, grow, groupBy, ==, maxBy, sliceWithKnownBound, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, changeValue, $init$, toSeq, zipWithIndex, toString, genericBuilder, copyToArray, seq, !=, transpose, collect, headOption, getClass, WithFilter, update, hasDefiniteSize, foldLeft, contains, toCollection, isEmpty, ne, PrimitiveKeyOpenHashMap, init, reversed, reduceLeft, eq, sum, thisCollection, ##, scanLeft, finalize, hashCode, zipAll, product, view.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, masterMetricsSystem, asInstanceOf, initializeLogIfNecessary, newShuffleBlockHandler, synchronized, $isInstanceOf, logTrace, startIfEnabled, ExternalShuffleService, isTraceEnabled, main, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, applicationRemoved, start, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, startIfEnabled, ExternalShuffleService, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, applicationRemoved, start, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getLocalProperty, isCompleted, TaskContext, wait, addTaskFailureListener, taskAttemptId, $asInstanceOf, empty, equals, taskMemoryManager, setTaskContext, unset, taskMetrics, stageId, setFetchFailed, asInstanceOf, getPartitionId, synchronized, addTaskCompletionListener, $isInstanceOf, killTaskIfInterrupted, notifyAll, stageAttemptNumber, partitionId, isInterrupted, isInstanceOf, <init>, getMetricsSources, ==, clone, registerAccumulator, getKillReason, toString, attemptNumber, isRunningLocally, !=, get, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(TaskContext, taskMetrics, stageId, asInstanceOf, synchronized, partitionId, isInstanceOf, <init>, ==, clone, toString, attemptNumber, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala: Set(TaskContext, setFetchFailed, <init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(TaskContext, taskAttemptId, asInstanceOf, stageAttemptNumber, partitionId, isInstanceOf, <init>, ==, toString, attemptNumber, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(TaskContext, stageId, asInstanceOf, partitionId, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala: Set(TaskContext, taskMetrics, <init>, ==, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(TaskContext, asInstanceOf, addTaskCompletionListener, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(getLocalProperty, TaskContext, stageId, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(TaskContext, asInstanceOf, isInstanceOf, <init>, ==, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(TaskContext, asInstanceOf, addTaskCompletionListener, partitionId, isInstanceOf, <init>, toString, attemptNumber, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(getLocalProperty, TaskContext, empty, equals, stageId, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(isCompleted, TaskContext, taskAttemptId, empty, setTaskContext, taskMetrics, stageId, asInstanceOf, addTaskCompletionListener, partitionId, isInterrupted, isInstanceOf, <init>, ==, getKillReason, attemptNumber, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(TaskContext, asInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala: Set(TaskContext, asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(TaskContext, taskMetrics, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(TaskContext, stageId, stageAttemptNumber, <init>, attemptNumber, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala: Set(TaskContext, killTaskIfInterrupted, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(TaskContext, equals, setTaskContext, asInstanceOf, isInstanceOf, <init>, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(TaskContext, asInstanceOf, synchronized, addTaskCompletionListener, isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala: Set(TaskContext, asInstanceOf, <init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(TaskContext, taskAttemptId, empty, taskMemoryManager, taskMetrics, stageId, synchronized, stageAttemptNumber, partitionId, <init>, registerAccumulator, attemptNumber, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(TaskContext, taskMemoryManager, asInstanceOf, isInstanceOf, <init>, get, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(getLocalProperty, TaskContext, empty, taskMetrics, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(TaskContext, taskMetrics, <init>, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala: Set(TaskContext, wait, taskAttemptId, empty, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(TaskContext, setTaskContext, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(TaskContext, equals, asInstanceOf, <init>, ==, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(TaskContext, taskAttemptId, empty, taskMetrics, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(TaskContext, asInstanceOf, synchronized, isInstanceOf, <init>, ==, registerAccumulator, toString, !=, get, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(TaskContext, <init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala: Set(TaskContext, asInstanceOf, partitionId, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(TaskContext, taskAttemptId, taskMemoryManager, asInstanceOf, synchronized, addTaskCompletionListener, <init>, ==, !=, get, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(TaskContext, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(TaskContext, asInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(TaskContext, stageId, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala: Set(TaskContext, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(TaskContext, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(TaskContext, equals, unset, taskMetrics, asInstanceOf, synchronized, addTaskCompletionListener, isInstanceOf, <init>, ==, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala: Set(TaskContext, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(TaskContext, taskAttemptId, taskMemoryManager, taskMetrics, asInstanceOf, synchronized, partitionId, isInstanceOf, <init>, ==, toString, !=, get, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(TaskContext, wait, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(TaskContext, asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala: Set(TaskContext, wait, asInstanceOf, synchronized, notifyAll, <init>, ==, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(TaskContext, taskMetrics, asInstanceOf, addTaskCompletionListener, isInstanceOf, <init>, ==, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(TaskContext, asInstanceOf, isInstanceOf, <init>, ==, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(TaskContext, taskMetrics, asInstanceOf, synchronized, addTaskCompletionListener, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(TaskContext, taskAttemptId, empty, taskMemoryManager, setTaskContext, unset, taskMetrics, stageId, synchronized, notifyAll, partitionId, <init>, attemptNumber, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(TaskContext, stageId, asInstanceOf, partitionId, <init>, ==, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala: Set(TaskContext, empty, asInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(TaskContext, equals, taskMetrics, asInstanceOf, isInstanceOf, <init>, ==, toString, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(TaskContext, asInstanceOf, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(TaskContext, asInstanceOf, <init>, clone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala: Set(TaskContext, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala: Set(TaskContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(TaskContext, asInstanceOf, <init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(TaskContext, taskAttemptId, empty, asInstanceOf, synchronized, addTaskCompletionListener, isInstanceOf, <init>, ==, toString, !=, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(TaskContext, asInstanceOf, isInstanceOf, <init>, ==, toString, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(TaskContext, asInstanceOf, isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(TaskContext, asInstanceOf, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(TaskContext, equals, unset, taskMetrics, stageId, asInstanceOf, synchronized, addTaskCompletionListener, isInstanceOf, <init>, ==, toString, attemptNumber, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala: Set(TaskContext, empty, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(TaskContext, addTaskFailureListener, addTaskCompletionListener, partitionId, <init>, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(getLocalProperty, TaskContext, empty, equals, stageId, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, empty, taskMemoryManager, asInstanceOf, synchronized, killTaskIfInterrupted, notifyAll, isInstanceOf, <init>, ==, attemptNumber, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(TaskContext, taskAttemptId, empty, taskMemoryManager, setTaskContext, unset, taskMetrics, stageId, synchronized, notifyAll, partitionId, <init>, attemptNumber, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, $asInstanceOf, addAccumulator, countFailedValues, equals, newAcc, +=, zero, asInstanceOf, GrowableAccumulableParam, synchronized, $isInstanceOf, setValueAny, addInPlace, toInfo, AccumulableParam, notifyAll, isInstanceOf, setValue, <init>, merge, id, ==, clone, toString, !=, getClass, ++=, ne, add, Accumulable, localValue, value, eq, value_=, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(name, asInstanceOf, AccumulableParam, <init>, Accumulable, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(name, newAcc, asInstanceOf, GrowableAccumulableParam, synchronized, AccumulableParam, isInstanceOf, <init>, id, ==, clone, toString, !=, getClass, ++=, ne, Accumulable, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(name, asInstanceOf, AccumulableParam, <init>, Accumulable, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(name, addAccumulator, countFailedValues, newAcc, zero, asInstanceOf, synchronized, addInPlace, AccumulableParam, isInstanceOf, <init>, id, ==, toString, !=, getClass, add, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(name, newAcc, asInstanceOf, GrowableAccumulableParam, synchronized, AccumulableParam, isInstanceOf, <init>, id, ==, clone, toString, !=, getClass, ++=, ne, Accumulable, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala: Set(name, countFailedValues, addInPlace, AccumulableParam, <init>, Accumulable)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, memoryUsed, equals, memoryFree, asInstanceOf, initializeLogIfNecessary, synchronized, poolSize, $isInstanceOf, acquireMemory$default$4, getMemoryUsageForTask, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, releaseMemory, isInstanceOf, decrementPoolSize, <init>, incrementPoolSize, ==, clone, acquireMemory, $init$, toString, logError, !=, getClass, logWarning, acquireMemory$default$3, ne, ExecutionMemoryPool, releaseAllMemoryForTask, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(memoryUsed, memoryFree, synchronized, poolSize, decrementPoolSize, <init>, incrementPoolSize, ==, acquireMemory, ne, ExecutionMemoryPool, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(memoryUsed, synchronized, poolSize, getMemoryUsageForTask, releaseMemory, <init>, incrementPoolSize, ==, ExecutionMemoryPool, releaseAllMemoryForTask)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(memoryFree, synchronized, poolSize, decrementPoolSize, <init>, incrementPoolSize, ==, acquireMemory, !=, ExecutionMemoryPool, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/PackedRecordPointer.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/PackedRecordPointer.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/PackedRecordPointer.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/PackedRecordPointer.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/PackedRecordPointer.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $assertionsDisabled, PARTITION_ID_END_BYTE_INDEX, equals, MAXIMUM_PARTITION_ID, set, getPartitionId, PARTITION_ID_START_BYTE_INDEX, PackedRecordPointer, notifyAll, <init>, packPointer, MAXIMUM_PAGE_SIZE_BYTES, toString, getRecordPointer, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/PackedRecordPointer.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(MAXIMUM_PARTITION_ID, PackedRecordPointer, <init>, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, fileServerSSLOptions, setModifyAclsGroups, $asInstanceOf, setAdminAcls, equals, sslSocketFactory, asInstanceOf, initializeLogIfNecessary, synchronized, setModifyAcls, getViewAcls, getViewAclsGroups, initializeAuth, $isInstanceOf, isAuthenticationEnabled, checkUIViewPermissions, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, SecurityManager, getModifyAclsGroups, logName, notifyAll, isInstanceOf, ENV_AUTH_SECRET, aclsEnabled, getModifyAcls, SPARK_AUTH_SECRET_CONF, <init>, hostnameVerifier, setAdminAclsGroups, getSSLOptions, ==, clone, getHttpUser, getSaslUser, $init$, isEncryptionEnabled, setViewAclsGroups, toString, setAcls, getSecretKey, logError, !=, checkModifyPermissions, getClass, logWarning, ioEncryptionKey, SECRET_LOOKUP_KEY, ne, SPARK_AUTH_CONF, <init>$default$2, eq, log, getIOEncryptionKey, setViewAcls, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, SecurityManager, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, getIOEncryptionKey, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, SecurityManager, <init>, ==, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==, logError, !=, logWarning, ioEncryptionKey, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(SecurityManager, <init>, ==, toString, checkModifyPermissions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, SecurityManager, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(SecurityManager, <init>, getSSLOptions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, sslSocketFactory, asInstanceOf, synchronized, isAuthenticationEnabled, logTrace, SecurityManager, isInstanceOf, <init>, hostnameVerifier, ==, getHttpUser, toString, getSecretKey, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala: Set(SecurityManager, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, SecurityManager, isInstanceOf, <init>, getSSLOptions, toString, SPARK_AUTH_CONF, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, initializeLogIfNecessary, SecurityManager, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(SecurityManager, <init>, getSSLOptions, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==, toString, !=, logWarning, getIOEncryptionKey, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, SecurityManager, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, initializeAuth, SecurityManager, isInstanceOf, <init>, ==, isEncryptionEnabled, toString, !=, getClass, logWarning, ioEncryptionKey, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(SecurityManager, <init>, getSSLOptions, ==, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(synchronized, SecurityManager, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isAuthenticationEnabled, SecurityManager, isInstanceOf, <init>, ==, clone, getSaslUser, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(asInstanceOf, checkUIViewPermissions, SecurityManager, isInstanceOf, <init>, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, isAuthenticationEnabled, logTrace, SecurityManager, isInstanceOf, <init>, ==, toString, logError, !=, getClass, logWarning, ne, getIOEncryptionKey, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala: Set(SecurityManager, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(asInstanceOf, checkUIViewPermissions, SecurityManager, isInstanceOf, <init>, ==, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(SecurityManager, <init>, ==, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala: Set(checkUIViewPermissions, SecurityManager, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(SecurityManager, <init>, ==, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(isAuthenticationEnabled, logTrace, SecurityManager, <init>, logError, !=, getClass, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, SecurityManager, notifyAll, isInstanceOf, <init>, ==, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala: Set(SecurityManager, <init>, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(setAdminAcls, asInstanceOf, synchronized, SecurityManager, isInstanceOf, <init>, setAdminAclsGroups, ==, clone, setViewAclsGroups, toString, setAcls, logError, !=, getClass, logWarning, ne, eq, log, setViewAcls, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(isAuthenticationEnabled, SecurityManager, <init>, ==, !=, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala: Set(isAuthenticationEnabled, SecurityManager, ENV_AUTH_SECRET, SPARK_AUTH_SECRET_CONF, <init>, getSecretKey, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala: Set(SecurityManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala: Set(SecurityManager, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(SecurityManager, <init>, ==, toString, checkModifyPermissions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, SPARK_AUTH_SECRET_CONF, <init>, ==, toString, !=, logWarning, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, SecurityManager, <init>, ==, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, SecurityManager, isInstanceOf, <init>, ==, logError, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(SecurityManager, <init>, ==, toString, checkModifyPermissions)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, access$800, wait, $assertionsDisabled, equals, access$702, access$502, access$602, access$202, access$900, skip, ReadAheadInputStream, notifyAll, markSupported, access$302, <init>, access$1000, access$100, access$000, reset, available, toString, getClass, mark, close, access$400, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	UnsafeSorterSpillMerger, notify, wait, equals, notifyAll, <init>, access$100, access$000, toString, getClass, getSortedIterator, addSpillIfNotEmpty, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FilterFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FilterFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FilterFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FilterFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FilterFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, FilterFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FilterFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, unpersist, addApplication, wait, $asInstanceOf, equals, addWorker, asInstanceOf, initializeLogIfNecessary, synchronized, readPersistedData, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, removeApplication, persist, dir, <init>, removeDriver, serializer, ==, clone, $init$, toString, logError, !=, getClass, logWarning, close, ne, addDriver, eq, log, removeWorker, ##, finalize, FileSystemPersistenceEngine, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(<init>, serializer, FileSystemPersistenceEngine, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toSparkContext, fromSparkContext.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, register, registerLogger, SignalUtils, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(registerLogger, SignalUtils, equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $assertionsDisabled, equals, RadixSort, notifyAll, <init>, toString, sortKeyPrefixArray, getClass, sort, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RadixSort.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partition.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, $init$, Partition, toString, !=, getClass, ne, eq, ##, finalize, index, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, Partition, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(asInstanceOf, isInstanceOf, Partition, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(asInstanceOf, Partition, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala: Set(synchronized, ==, Partition)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, Partition, toString, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, toString, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, toString, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, Partition, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, Partition, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Partition, toString, !=, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf, Partition)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala: Set(synchronized, ==, Partition)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, ==, Partition, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(asInstanceOf, Partition, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, Partition, toString, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(asInstanceOf, ==, Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala: Set(asInstanceOf, Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, Partition, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Partition, toString, !=, getClass, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(equals, asInstanceOf, ==, Partition, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala: Set(asInstanceOf, Partition, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, Partition, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, ==, Partition, toString, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(asInstanceOf, isInstanceOf, Partition, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(==, Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala: Set(==, Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, toString, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Partition, toString, !=, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala: Set(Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(asInstanceOf, Partition)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(asInstanceOf, Partition, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Partition, toString, !=, getClass, ne, eq, finalize, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(asInstanceOf, Partition)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala: Set(wait, asInstanceOf, synchronized, notifyAll, ==, Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, toString, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(asInstanceOf, ==, Partition, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala: Set(asInstanceOf, ==, Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, Partition, toString, ne, eq, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(asInstanceOf, clone, Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Partition, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf, Partition, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, toString, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, Partition, !=, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala: Set(Partition)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala: Set(Partition, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(asInstanceOf, Partition, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Partition, toString, !=, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala: Set(Partition)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getTimer, postToAll, wait, $asInstanceOf, listeners, equals, asInstanceOf, initializeLogIfNecessary, synchronized, replay, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, removeListener, <init>, ReplayListenerBus, ==, ReplayEventsFilter, clone, replay$default$3, $init$, findListenersByClass, toString, logError, !=, SELECT_ALL_FILTER, getClass, logWarning, doPostEvent, replay$default$4, addListener, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo, removeListenerOnError.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, replay, isInstanceOf, <init>, ReplayListenerBus, ==, ReplayEventsFilter, clone, toString, logError, !=, SELECT_ALL_FILTER, getClass, logWarning, addListener, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onExecutorUnblacklisted, notify, AppStatusListener, wait, $asInstanceOf, onJobEnd, onApplicationEnd, equals, onTaskEnd, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, onOtherEvent, isTraceEnabled, initializeLogIfNecessary$default$2, onTaskStart, logName, notifyAll, onExecutorRemoved, onExecutorAdded, isInstanceOf, <init>, onBlockUpdated, ==, onBlockManagerRemoved, clone, onExecutorMetricsUpdate, onEnvironmentUpdate, onBlockManagerAdded, $init$, onNodeBlacklisted, onApplicationStart, onSpeculativeTaskSubmitted, toString, logError, !=, getClass, logWarning, onExecutorBlacklisted, onUnpersistRDD, activeStages, ne, onStageSubmitted, eq, onNodeUnblacklisted, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(AppStatusListener, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(AppStatusListener, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(AppStatusListener, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, activeStages, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferInputStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferInputStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferInputStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferInputStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ByteBufferInputStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, wait, $asInstanceOf, ByteBufferInputStream, equals, asInstanceOf, synchronized, $isInstanceOf, skip, notifyAll, markSupported, isInstanceOf, <init>, ==, clone, reset, available, toString, !=, getClass, mark, close, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala: Set(ByteBufferInputStream, <init>, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(read, ByteBufferInputStream, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, close, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala: Set(ByteBufferInputStream, asInstanceOf, isInstanceOf, <init>, reset, close)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, valueOf, fromString, equals, ApplicationStatus, getDeclaringClass, RUNNING, COMPLETED, notifyAll, compareTo, ordinal, values, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala: Set(ApplicationStatus)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, DEFAULT_SHUTDOWN_PRIORITY, SparkShutdownHookManager, wait, $asInstanceOf, equals, runAll, removeShutdownHook, asInstanceOf, hasShutdownDeleteDir, initializeLogIfNecessary, inShutdown, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, registerShutdownDeleteDir, remove, addShutdownHook, ==, SPARK_CONTEXT_SHUTDOWN_PRIORITY, clone, $init$, hasRootAsShutdownDeleteDir, toString, logError, !=, getClass, logWarning, install, ShutdownHookManager, removeShutdownDeleteDir, ne, add, TEMP_DIR_SHUTDOWN_PRIORITY, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(removeShutdownHook, asInstanceOf, synchronized, isInstanceOf, <init>, remove, addShutdownHook, ==, SPARK_CONTEXT_SHUTDOWN_PRIORITY, clone, toString, logError, !=, getClass, logWarning, ShutdownHookManager, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, registerShutdownDeleteDir, ==, toString, logError, !=, getClass, logWarning, ShutdownHookManager, removeShutdownDeleteDir, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, addShutdownHook, toString, ShutdownHookManager, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(removeShutdownHook, <init>, addShutdownHook, ==, toString, logError, !=, logWarning, ShutdownHookManager, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, inShutdown, synchronized, isInstanceOf, <init>, ==, toString, !=, logWarning, ShutdownHookManager, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, inShutdown, synchronized, notifyAll, isInstanceOf, <init>, remove, ==, logError, !=, logWarning, ShutdownHookManager, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(<init>, addShutdownHook, ==, !=, ShutdownHookManager, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(removeShutdownHook, synchronized, <init>, addShutdownHook, ==, !=, logWarning, ShutdownHookManager, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(removeShutdownHook, synchronized, <init>, addShutdownHook, clone, hasRootAsShutdownDeleteDir, logError, !=, ShutdownHookManager, TEMP_DIR_SHUTDOWN_PRIORITY, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, inShutdown, synchronized, isInstanceOf, <init>, ==, toString, !=, logWarning, ShutdownHookManager, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala: Set(inShutdown, isInstanceOf, <init>, logError, ShutdownHookManager)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, JdbcRDD, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, getConnection, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, resultSetToObjectArray, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, <init>$default$7, $isInstanceOf, create, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, lower, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, upper, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, ConnectionFactory, getCreationSite, JdbcPartition, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, numPartitionsInRdd2, $isInstanceOf, compute, rdd2, s2, mapPartitions$default$2, min, getCheckpointFile, fold, rdd1, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, s1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, CartesianPartition, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, CartesianRDD, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(creationSite, zipPartitions, parent, isLocallyCheckpointed, getOrCompute, partitioner, coalesce, name, count, isCheckpointedAndMaterialized, mapPartitions, union, map, subtract, intersection, scope, asInstanceOf, context, getPreferredLocations, doCheckpoint, synchronized, aggregate, compute, rdd2, min, fold, getOutputDeterministicLevel, iterator, conf, isInstanceOf, filter, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, ++, flatMap, take, groupBy, ==, clone, distinct, retag, foreach, reduce, zipWithIndex, checkpoint, elementClassTag, sample, toString, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, clearDependencies, isEmpty, ne, getDependencies, mapPartitionsWithIndex, keyBy, getCreationSite, computeOrReadCheckpoint, dependencies, isReliablyCheckpointed, withScope, log, CartesianRDD, treeAggregate, index, takeOrdered, firstParent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, RDDInfo, diskSize, wait, fromRdd, <=, $asInstanceOf, <init>$default$6, equals, scope, asInstanceOf, <, synchronized, <init>$default$7, $isInstanceOf, >=, parentIds, notifyAll, compareTo, isInstanceOf, <init>, id, numCachedPartitions, ==, clone, isCached, $init$, toString, !=, callSite, memSize, getClass, ne, numPartitions, eq, compare, externalBlockStoreSize, storageLevel, >, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala: Set(name, RDDInfo, fromRdd, parentIds, <init>, id)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(name, RDDInfo, fromRdd, <=, asInstanceOf, <, synchronized, isInstanceOf, <init>, id, ==, clone, isCached, toString, !=, callSite, getClass, ne, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala: Set(RDDInfo, diskSize, asInstanceOf, isInstanceOf, <init>, id, numCachedPartitions, ==, toString, !=, memSize, ne, eq, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(name, RDDInfo, <, >=, <init>, id, ==, toString, !=, numPartitions, eq, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(name, RDDInfo, diskSize, <=, scope, asInstanceOf, <, synchronized, >=, isInstanceOf, <init>, id, numCachedPartitions, ==, clone, toString, !=, memSize, getClass, ne, numPartitions, externalBlockStoreSize, storageLevel, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala: Set(name, RDDInfo, <=, scope, asInstanceOf, parentIds, isInstanceOf, <init>, id, ==, isCached, toString, !=, callSite, ne, eq, storageLevel, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(name, RDDInfo, diskSize, <=, equals, scope, asInstanceOf, >=, isInstanceOf, <init>, id, ==, toString, !=, memSize, getClass, ne, storageLevel, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(name, RDDInfo, diskSize, scope, asInstanceOf, parentIds, isInstanceOf, <init>, id, numCachedPartitions, ==, toString, !=, callSite, memSize, ne, numPartitions, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ListenerBus, notify, getTimer, postToAll, wait, $asInstanceOf, listeners, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, removeListener, ==, clone, $init$, findListenersByClass, toString, logError, !=, getClass, logWarning, doPostEvent, addListener, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo, removeListenerOnError.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala: Set(postToAll, asInstanceOf, isInstanceOf, logError, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(logTrace, removeListener, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, logError, !=, getClass, logWarning, addListener, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(listeners, asInstanceOf, synchronized, isInstanceOf, removeListener, ==, findListenersByClass, logError, addListener)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, logError, !=, getClass, logWarning, addListener, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala: Set(postToAll, asInstanceOf, isInstanceOf, logError, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala: Set(ListenerBus, asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(listeners, asInstanceOf, synchronized, isInstanceOf, removeListener, ==, findListenersByClass, logError, addListener)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	assert, notify, wait, $asInstanceOf, equals, asInstanceOf, validate, synchronized, $isInstanceOf, notifyAll, isInstanceOf, messageType, <init>, ==, clone, toJson, message, toString, fromJson, !=, parseAction, getClass, doValidate, ne, action, eq, ##, finalize, hashCode, SubmitRestProtocolMessage, assertFieldIsSet.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala: Set(<init>, message, doValidate, SubmitRestProtocolMessage, assertFieldIsSet)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(asInstanceOf, validate, isInstanceOf, <init>, ==, toJson, message, fromJson, ne, SubmitRestProtocolMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(asInstanceOf, validate, isInstanceOf, messageType, <init>, ==, toJson, message, fromJson, !=, ne, SubmitRestProtocolMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, isInstanceOf, messageType, <init>, message, toString, SubmitRestProtocolMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala: Set(assert, <init>, !=, doValidate, SubmitRestProtocolMessage, assertFieldIsSet)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, isInstanceOf, messageType, <init>, message, toString, SubmitRestProtocolMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(assert, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, action, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(asInstanceOf, validate, isInstanceOf, <init>, ==, toJson, message, fromJson, ne, SubmitRestProtocolMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(asInstanceOf, validate, isInstanceOf, messageType, <init>, ==, toJson, message, fromJson, !=, ne, SubmitRestProtocolMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, isInstanceOf, messageType, <init>, message, toString, SubmitRestProtocolMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(asInstanceOf, validate, isInstanceOf, messageType, <init>, ==, toJson, message, fromJson, !=, ne, SubmitRestProtocolMessage)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, constructSubmitRequest, $asInstanceOf, RestSubmissionClientApp, equals, requestSubmissionStatus, asInstanceOf, initializeLogIfNecessary, run, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, RestSubmissionClient, logName, notifyAll, createSubmission, isInstanceOf, <init>, run$default$5, ==, killSubmission, PROTOCOL_VERSION, clone, readResponse, $init$, toString, logError, !=, getClass, logWarning, filterSystemEnvironment, start, ne, eq, log, requestSubmissionStatus$default$2, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(requestSubmissionStatus, asInstanceOf, initializeLogIfNecessary, RestSubmissionClient, isInstanceOf, <init>, ==, killSubmission, toString, !=, getClass, start, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(asInstanceOf, RestSubmissionClient, isInstanceOf, <init>, ==, PROTOCOL_VERSION, start, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, StorageLevels, MEMORY_ONLY_2, equals, create, MEMORY_AND_DISK_SER, notifyAll, MEMORY_AND_DISK_2, <init>, DISK_ONLY, NONE, OFF_HEAP, MEMORY_AND_DISK, DISK_ONLY_2, MEMORY_AND_DISK_SER_2, toString, getClass, MEMORY_ONLY_SER_2, MEMORY_ONLY_SER, MEMORY_ONLY, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/StorageLevels.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	driverDesc, ProcessBuilderLike, notify, finalException, setClock, wait, $asInstanceOf, Sleeper, finalState, equals, asInstanceOf, initializeLogIfNecessary, synchronized, sleep, $isInstanceOf, logTrace, DriverRunner, workDir, isTraceEnabled, initializeLogIfNecessary$default$2, command, logName, notifyAll, isInstanceOf, <init>, apply, ==, clone, securityManager, $init$, prepareAndRunDriver, toString, logError, !=, getClass, sparkHome, logWarning, runCommandWithRetry, start, kill, setSleeper, ne, eq, workerUrl, log, ##, finalize, hashCode, logDebug, logInfo, worker, driverId.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(driverDesc, asInstanceOf, DriverRunner, isInstanceOf, <init>, ==, toString, eq, worker, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(driverDesc, asInstanceOf, synchronized, DriverRunner, workDir, command, isInstanceOf, <init>, apply, ==, toString, logError, !=, sparkHome, logWarning, start, kill, ne, log, logDebug, logInfo, worker, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(driverDesc, finalException, finalState, DriverRunner, command, <init>, apply, ==, toString, worker, driverId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	withScope$default$2, notify, parent, name, wait, $asInstanceOf, equals, getAllScopes, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>$default$3, <init>, id, ==, clone, toJson, $init$, toString, fromJson, logError, !=, getClass, logWarning, RDDOperationScope, ne, <init>$default$2, eq, withScope, log, nextScopeId, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(parent, name, asInstanceOf, synchronized, isInstanceOf, <init>, id, ==, clone, toString, logError, !=, getClass, logWarning, RDDOperationScope, ne, withScope, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(parent, name, asInstanceOf, synchronized, isInstanceOf, <init>, id, ==, clone, toString, fromJson, !=, getClass, logWarning, RDDOperationScope, ne, withScope, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala: Set(name, getAllScopes, asInstanceOf, isInstanceOf, <init>, id, ==, toString, !=, logWarning, RDDOperationScope, ne, eq, hashCode, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala: Set(name, <init>, id, toString, RDDOperationScope)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(name, asInstanceOf, isInstanceOf, <init>, id, ==, toJson, toString, fromJson, !=, RDDOperationScope, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, commitAndGet, synchronized, $isInstanceOf, revertPartialWritesAndClose, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, open, recordWritten, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, DiskBlockObjectWriter, flush, toString, file, logError, !=, getClass, logWarning, close, ne, blockId, eq, write, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, ==, DiskBlockObjectWriter, toString, file, logError, !=, getClass, logWarning, close, ne, blockId, write, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, commitAndGet, synchronized, revertPartialWritesAndClose, <init>, ==, DiskBlockObjectWriter, flush, file, !=, getClass, logWarning, close, ne, blockId, write, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala: Set(<init>, DiskBlockObjectWriter, !=, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, commitAndGet, synchronized, revertPartialWritesAndClose, isInstanceOf, <init>, ==, DiskBlockObjectWriter, flush, toString, file, !=, logWarning, close, ne, blockId, eq, write, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, wait, equals, skip, notifyAll, NioBufferedFileInputStream, markSupported, <init>, reset, available, toString, getClass, mark, close, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(NioBufferedFileInputStream, <init>, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, doCheckpoint, synchronized, cpState, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, ReliableRDDCheckpointData, isInstanceOf, checkpointPath, <init>, isCheckpointed, ==, checkpointRDD, clone, $init$, checkpoint, toString, getCheckpointDir, logError, !=, getClass, logWarning, getPartitions, cleanCheckpoint, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, doCheckpoint, synchronized, ReliableRDDCheckpointData, isInstanceOf, <init>, isCheckpointed, ==, checkpointRDD, clone, checkpoint, toString, getCheckpointDir, !=, getClass, logWarning, getPartitions, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, ReliableRDDCheckpointData, isInstanceOf, <init>, ==, toString, logError, cleanCheckpoint, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, master, equals, asInstanceOf, metricRegistry, synchronized, $isInstanceOf, notifyAll, sourceName, isInstanceOf, <init>, ==, clone, toString, !=, getClass, MasterSource, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(master, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, MasterSource, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolResponse.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	assert, KillSubmissionResponse, notify, success, wait, $asInstanceOf, equals, asInstanceOf, validate, synchronized, driverState, $isInstanceOf, workerHostPort, submissionId, notifyAll, CreateSubmissionResponse, highestProtocolVersion, isInstanceOf, messageType, <init>, ==, ErrorResponse, clone, toJson, SubmitRestProtocolResponse, workerId, message, toString, !=, getClass, doValidate, SubmissionStatusResponse, unknownFields, ne, action, eq, ##, finalize, hashCode, serverSparkVersion, assertFieldIsSet.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(KillSubmissionResponse, success, asInstanceOf, workerHostPort, submissionId, CreateSubmissionResponse, isInstanceOf, messageType, <init>, ErrorResponse, SubmitRestProtocolResponse, workerId, message, toString, SubmissionStatusResponse, unknownFields)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(assert, asInstanceOf, isInstanceOf, <init>, ==, SubmitRestProtocolResponse, toString, !=, getClass, ne, action, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(KillSubmissionResponse, asInstanceOf, validate, submissionId, isInstanceOf, <init>, ==, ErrorResponse, toJson, SubmitRestProtocolResponse, message, SubmissionStatusResponse, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(KillSubmissionResponse, success, asInstanceOf, validate, driverState, workerHostPort, submissionId, CreateSubmissionResponse, isInstanceOf, messageType, <init>, ==, ErrorResponse, toJson, SubmitRestProtocolResponse, workerId, message, !=, SubmissionStatusResponse, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, encode, name, serializedTask, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, addedFiles, decode, executorId, notifyAll, isInstanceOf, <init>, TaskDescription, properties, ==, clone, toString, attemptNumber, !=, addedJars, getClass, ne, eq, taskId, ##, finalize, index, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(encode, serializedTask, asInstanceOf, synchronized, executorId, isInstanceOf, <init>, TaskDescription, properties, ==, toString, !=, ne, taskId, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(name, serializedTask, asInstanceOf, synchronized, addedFiles, executorId, isInstanceOf, <init>, TaskDescription, ==, toString, attemptNumber, !=, addedJars, ne, taskId, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, decode, executorId, isInstanceOf, <init>, TaskDescription, ==, !=, ne, taskId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, <init>, TaskDescription, ==, toString, eq, taskId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(name, wait, asInstanceOf, synchronized, executorId, isInstanceOf, <init>, TaskDescription, properties, ==, toString, !=, ne, taskId, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(name, serializedTask, wait, asInstanceOf, synchronized, addedFiles, executorId, notifyAll, isInstanceOf, <init>, TaskDescription, properties, ==, attemptNumber, !=, addedJars, ne, eq, taskId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	rawValueConverter, findEntry, alternatives, notify, readString, wait, $asInstanceOf, UNDEFINED, equals, fallback, asInstanceOf, ConfigEntry, synchronized, FallbackConfigEntry, $isInstanceOf, registerEntry, stringConverter, doc, notifyAll, key, isInstanceOf, isPublic, <init>, readFrom, OptionalConfigEntry, ==, clone, toString, rawStringConverter, !=, valueConverter, getClass, defaultValue, ne, defaultValueString, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala: Set(ConfigEntry, <init>, OptionalConfigEntry, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, ConfigEntry, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, ConfigEntry, synchronized, key, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, ConfigEntry, key, <init>, OptionalConfigEntry, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, ConfigEntry, synchronized, key, isInstanceOf, <init>, OptionalConfigEntry, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, ConfigEntry, isInstanceOf, <init>, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, ConfigEntry, synchronized, key, isInstanceOf, <init>, OptionalConfigEntry, ==, toString, !=, getClass, ne, defaultValueString, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala: Set(asInstanceOf, ConfigEntry, isInstanceOf, <init>, OptionalConfigEntry, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, ConfigEntry, synchronized, isInstanceOf, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, ConfigEntry, key, isInstanceOf, <init>, OptionalConfigEntry, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, ConfigEntry, synchronized, isInstanceOf, <init>, ==, !=, defaultValue, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(asInstanceOf, ConfigEntry, key, isInstanceOf, <init>, OptionalConfigEntry, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala: Set(ConfigEntry, <init>, OptionalConfigEntry)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala: Set(ConfigEntry, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala: Set(asInstanceOf, ConfigEntry, <init>, ==, !=, defaultValue)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, ConfigEntry, synchronized, key, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala: Set(ConfigEntry, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala: Set(ConfigEntry, key, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(ConfigEntry, isInstanceOf, <init>, OptionalConfigEntry, ==, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala: Set(asInstanceOf, ConfigEntry, key, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, ConfigEntry, synchronized, key, isInstanceOf, <init>, ==, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala: Set(ConfigEntry, doc, <init>, OptionalConfigEntry)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(ConfigEntry, <init>, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(asInstanceOf, ConfigEntry, isInstanceOf, <init>, OptionalConfigEntry, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, ConfigEntry, key, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala: Set(asInstanceOf, ConfigEntry, synchronized, isInstanceOf, <init>, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(ConfigEntry, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala: Set(ConfigEntry, doc, <init>, OptionalConfigEntry)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, ConfigEntry, synchronized, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala: Set(findEntry, fallback, asInstanceOf, ConfigEntry, key, isInstanceOf, <init>, ==, !=, defaultValueString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala: Set(fallback, asInstanceOf, ConfigEntry, FallbackConfigEntry, stringConverter, key, isInstanceOf, <init>, OptionalConfigEntry, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, ConfigEntry, synchronized, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, ConfigEntry, synchronized, key, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(asInstanceOf, ConfigEntry, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, ConfigEntry, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(ConfigEntry, synchronized, notifyAll, <init>, OptionalConfigEntry, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, ConfigEntry, synchronized, isInstanceOf, <init>, OptionalConfigEntry, ==, clone, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(ConfigEntry, synchronized, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(asInstanceOf, ConfigEntry, synchronized, isInstanceOf, <init>, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala: Set(asInstanceOf, ConfigEntry, isInstanceOf, <init>, ==, !=, defaultValue)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, ConfigEntry, synchronized, notifyAll, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala: Set(ConfigEntry, key, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(asInstanceOf, ConfigEntry, synchronized, isInstanceOf, <init>, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, ConfigEntry, stringConverter, key, isInstanceOf, <init>, readFrom, OptionalConfigEntry, ==, toString, rawStringConverter, !=, defaultValue, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, ConfigEntry, synchronized, key, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getPeakMemoryUsedBytes, wait, $assertionsDisabled, equals, closeAndGetSpills, cleanupResources, allocateArray, ShuffleExternalSorter, DISK_WRITE_BUFFER_SIZE, insertRecord, notifyAll, <init>, acquireMemory, freeArray, toString, freeMemory, spill, getClass, getMode, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	create$default$6, notify, numUsableCores, addFile, name, advertiseAddress, wait, copy$default$2, $asInstanceOf, copy$default$5, addJar, setupEndpointRef, productArity, equals, setupEndpointRefByURI, asInstanceOf, defaultLookupTimeout, bindAddress, synchronized, $isInstanceOf, create, copy$default$8, canEqual, copy$default$4, deserialize, productPrefix, stop, notifyAll, conf, isInstanceOf, clientMode, shutdown, <init>, port, RpcEnvConfig, validateDirectoryUri, ==, setupEndpoint, clone, fileServer, RpcEnv, copy$default$7, securityManager, $init$, asyncSetupEndpointRefByURI, copy$default$3, address, copy, RpcEnvFileServer, toString, awaitTermination, endpointRef, !=, getClass, copy$default$1, copy$default$6, ne, eq, addDirectory, productIterator, ##, finalize, productElement, hashCode, openChannel.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala: Set(name, asInstanceOf, isInstanceOf, <init>, ==, RpcEnv, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, stop, conf, isInstanceOf, <init>, ==, setupEndpoint, RpcEnv, securityManager, address, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, stop, conf, isInstanceOf, <init>, ==, RpcEnv, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(setupEndpointRefByURI, asInstanceOf, create, stop, conf, isInstanceOf, shutdown, <init>, ==, setupEndpoint, RpcEnv, asyncSetupEndpointRefByURI, address, awaitTermination, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, stop, conf, isInstanceOf, <init>, ==, RpcEnv, toString, endpointRef, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(addFile, name, addJar, asInstanceOf, synchronized, create, deserialize, stop, conf, isInstanceOf, <init>, port, ==, setupEndpoint, clone, fileServer, RpcEnv, securityManager, RpcEnvFileServer, toString, endpointRef, !=, getClass, ne, addDirectory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(conf, shutdown, <init>, port, clone, RpcEnv, address, awaitTermination, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(name, equals, asInstanceOf, synchronized, deserialize, conf, isInstanceOf, <init>, port, ==, RpcEnv, address, copy, toString, !=, getClass, ne, eq, hashCode, openChannel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, create, conf, isInstanceOf, shutdown, <init>, port, ==, setupEndpoint, RpcEnv, address, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(name, setupEndpointRef, asInstanceOf, synchronized, create, stop, conf, isInstanceOf, <init>, port, ==, setupEndpoint, RpcEnv, address, toString, awaitTermination, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(numUsableCores, name, advertiseAddress, asInstanceOf, bindAddress, synchronized, create, stop, conf, isInstanceOf, shutdown, <init>, port, ==, setupEndpoint, RpcEnv, securityManager, address, toString, awaitTermination, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, stop, conf, isInstanceOf, <init>, ==, setupEndpoint, RpcEnv, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, stop, conf, isInstanceOf, <init>, port, ==, RpcEnv, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(numUsableCores, name, advertiseAddress, asInstanceOf, bindAddress, synchronized, deserialize, stop, conf, isInstanceOf, clientMode, shutdown, <init>, port, RpcEnvConfig, ==, clone, RpcEnv, securityManager, address, RpcEnvFileServer, toString, awaitTermination, endpointRef, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(numUsableCores, asInstanceOf, synchronized, stop, conf, isInstanceOf, <init>, port, ==, setupEndpoint, RpcEnv, securityManager, copy, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala: Set(asInstanceOf, stop, <init>, RpcEnvConfig, RpcEnv, endpointRef, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala: Set(name, setupEndpointRef, conf, <init>, RpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(name, asInstanceOf, create, stop, conf, isInstanceOf, <init>, port, ==, setupEndpoint, RpcEnv, address, toString, awaitTermination, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(name, wait, asInstanceOf, synchronized, deserialize, stop, notifyAll, conf, isInstanceOf, shutdown, <init>, ==, RpcEnv, securityManager, awaitTermination, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala: Set(<init>, validateDirectoryUri, ==, address, RpcEnvFileServer, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(setupEndpointRef, asInstanceOf, stop, conf, isInstanceOf, <init>, ==, setupEndpoint, RpcEnv, address, endpointRef, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, RpcEnv, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, stop, notifyAll, conf, isInstanceOf, shutdown, <init>, ==, RpcEnv, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, RpcEnv, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala: Set(deserialize, <init>, RpcEnv)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(setupEndpointRef, asInstanceOf, create, conf, isInstanceOf, <init>, ==, setupEndpoint, RpcEnv, awaitTermination, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala: Set(asInstanceOf, <init>, ==, RpcEnv, asyncSetupEndpointRefByURI, address)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(numUsableCores, name, advertiseAddress, asInstanceOf, bindAddress, synchronized, deserialize, stop, conf, isInstanceOf, clientMode, shutdown, <init>, port, RpcEnvConfig, ==, clone, RpcEnv, securityManager, address, RpcEnvFileServer, toString, awaitTermination, endpointRef, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, address, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(numUsableCores, name, advertiseAddress, setupEndpointRefByURI, asInstanceOf, defaultLookupTimeout, bindAddress, create, conf, isInstanceOf, clientMode, <init>, port, RpcEnvConfig, ==, RpcEnv, securityManager, asyncSetupEndpointRefByURI, address, RpcEnvFileServer, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(numUsableCores, name, asInstanceOf, synchronized, stop, conf, isInstanceOf, shutdown, <init>, ==, address, awaitTermination, endpointRef, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, endpointRef, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala: Set(<init>, validateDirectoryUri, ==, address, RpcEnvFileServer, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	appSparkVersion, notify, name, wait, $asInstanceOf, equals, appName, prefix, asInstanceOf, killEnabled, synchronized, sc, $isInstanceOf, pages, attachPage, JobsTab, basePath, notifyAll, isInstanceOf, <init>, ==, clone, isFairScheduler, toString, getSparkUser, !=, getClass, headerTabs, ne, handleKillRequest, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(killEnabled, sc, JobsTab, basePath, isInstanceOf, <init>, ==, toString, getSparkUser, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(name, asInstanceOf, killEnabled, JobsTab, basePath, isInstanceOf, <init>, ==, isFairScheduler, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appSparkVersion, appName, prefix, killEnabled, sc, JobsTab, basePath, <init>, ==, getSparkUser, handleKillRequest)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CausedBy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CausedBy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CausedBy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CausedBy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CausedBy.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, wait, $asInstanceOf, equals, asInstanceOf, CausedBy, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(unapply, wait, asInstanceOf, CausedBy, synchronized, notifyAll, isInstanceOf, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Sink.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, stop, notifyAll, isInstanceOf, Sink, ==, clone, report, toString, !=, getClass, start, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala: Set(asInstanceOf, stop, isInstanceOf, Sink, ==, report, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, stop, Sink, ==, report, !=, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala: Set(asInstanceOf, stop, isInstanceOf, Sink, ==, report, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala: Set(stop, Sink, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala: Set(asInstanceOf, stop, isInstanceOf, Sink, ==, report, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala: Set(stop, Sink, report, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala: Set(asInstanceOf, stop, isInstanceOf, Sink, ==, report, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala: Set(Sink)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, stop, Sink, ==, report, !=, start)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ShuffleId, wait, $asInstanceOf, getBlockData, removeDataByMap, equals, asInstanceOf, initializeLogIfNecessary, getDataFile, synchronized, NOOP_REDUCE_ID, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, IndexShuffleBlockResolver, writeIndexFileAndCommit, toString, logError, !=, getClass, logWarning, ne, <init>$default$2, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(getDataFile, NOOP_REDUCE_ID, stop, <init>, IndexShuffleBlockResolver, writeIndexFileAndCommit, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(removeDataByMap, asInstanceOf, stop, isInstanceOf, <init>, IndexShuffleBlockResolver, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, equals, fillInStackTrace, initCause, SparkOutOfMemoryError, getCause, notifyAll, getStackTrace, <init>, getMessage, setStackTrace, getSuppressed, addSuppressed, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, SparkOutOfMemoryError, notifyAll, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, clearFailures, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, makeNewStageAttempt, parents, synchronized, latestInfo, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, rdd, findMissingPartitions, <init>, id, setActiveJob, ==, clone, numTasks, $init$, activeJob, func, details, toString, ResultStage, logError, firstJobId, !=, callSite, partitions, removeActiveJob, getClass, logWarning, jobIds, ne, numPartitions, eq, log, ##, finalize, hashCode, logDebug, makeNewStageAttempt$default$2, logInfo, fetchFailedAttemptIds.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(name, clearFailures, asInstanceOf, makeNewStageAttempt, parents, synchronized, latestInfo, logTrace, isInstanceOf, rdd, findMissingPartitions, <init>, id, setActiveJob, ==, clone, numTasks, activeJob, func, toString, ResultStage, logError, firstJobId, !=, callSite, partitions, removeActiveJob, logWarning, jobIds, ne, numPartitions, logDebug, logInfo, fetchFailedAttemptIds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf, rdd, <init>, ResultStage, callSite, partitions, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, DEFAULT_INITIAL_SER_BUFFER_SIZE, getPeakMemoryUsedBytes, wait, $assertionsDisabled, equals, stop, notifyAll, <init>, toString, closeAndWriteOutput, getClass, DEFAULT_INITIAL_SORT_BUFFER_SIZE, forceSorterToSpill, write, hashCode, UnsafeShuffleWriter, insertRecordIntoSorter.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(stop, <init>, getClass, UnsafeShuffleWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleHandle.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, shuffleId, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ShuffleHandle, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, <init>, ==, !=, ShuffleHandle, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BaseShuffleHandle.scala: Set(shuffleId, <init>, ShuffleHandle)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, shuffleId, isInstanceOf, <init>, getClass, ShuffleHandle)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(asInstanceOf, shuffleId, <init>, ShuffleHandle)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, shuffleId, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, ==, toString, ShuffleHandle, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala: Set(ShuffleHandle)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf, <init>, ShuffleHandle)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, ShuffleHandle, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(shuffleId, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, shuffleId, isInstanceOf, <init>, getClass, ShuffleHandle)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, shuffleId, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, metricRegistry, synchronized, $isInstanceOf, notifyAll, sourceName, isInstanceOf, <init>, ==, clone, toString, !=, WorkerSource, getClass, ne, eq, ##, finalize, hashCode, worker.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, WorkerSource, ne, worker)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, wait, foldRight, evictBlocksToFreeSpace, takeWhile, $asInstanceOf, minBy, size, zip, toSet, releaseUnrollMemoryForThisTask$default$2, corresponds, :\, duplicate, map, sliding$default$2, PartiallyUnrolledIterator, dropWhile, toMap, filterNot, equals, toList, reserveUnrollMemoryForThisTask, clear, dropFromMemory, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, valuesIterator, mkString, PartiallySerializedBlock, discard, min, scanRight, fold, logTrace, nonEmpty, afterDropAction, isTraceEnabled, initializeLogIfNecessary$default$2, unrollMemory, putBytes, logName, notifyAll, /:, toIterator, putIteratorAsBytes, addString, to, collectFirst, drop, isInstanceOf, filter, currentUnrollMemory, setOutputStream, GroupedIterator, <init>, toStream, max, buffered, releaseUnrollMemoryForThisTask, remove, getUnrolledChunkedByteBuffer, ++, grouped, flatMap, getValues, take, RedirectableOutputStream, reduceRight, ==, maxBy, indexWhere, clone, BlockEvictionHandler, slice, foreach, MemoryStore, exists, getBytes, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, flush, toString, copyToArray, length, seq, logError, !=, collect, getClass, getSize, logWarning, hasDefiniteSize, patch, close, foldLeft, contains, isEmpty, ne, withPartial, currentUnrollMemoryForThisTask, reversed, hasNext, indexOf, reduceLeft, eq, write, sum, log, putIteratorAsValues, ##, scanLeft, finalize, finishWritingToStream, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(withFilter, size, map, PartiallyUnrolledIterator, toMap, filterNot, clear, asInstanceOf, synchronized, partition, valuesIterator, mkString, PartiallySerializedBlock, logTrace, putBytes, toIterator, putIteratorAsBytes, to, isInstanceOf, filter, <init>, max, remove, ++, flatMap, getValues, ==, BlockEvictionHandler, foreach, MemoryStore, exists, getBytes, toArray, toSeq, next, toString, length, logError, !=, collect, getClass, getSize, logWarning, close, contains, isEmpty, ne, hasNext, write, putIteratorAsValues, finishWritingToStream, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(synchronized, min, currentUnrollMemory, <init>, max, ==, MemoryStore, !=, contains, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala: Set(evictBlocksToFreeSpace, size, synchronized, min, <init>, max, ==, MemoryStore, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, filter, <init>, releaseUnrollMemoryForThisTask, ++, MemoryStore, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(size, synchronized, min, <init>, max, ==, MemoryStore)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(withFilter, find, size, map, toMap, toList, clear, asInstanceOf, synchronized, partition, mkString, logTrace, nonEmpty, drop, isInstanceOf, filter, <init>, remove, ++, flatMap, take, ==, clone, foreach, exists, toArray, toSeq, toString, length, logError, !=, logWarning, contains, isEmpty, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(withFilter, size, toSet, map, clear, asInstanceOf, synchronized, partition, mkString, min, isInstanceOf, filter, <init>, max, remove, flatMap, ==, foreach, toIndexedSeq, toSeq, toString, logError, !=, logWarning, contains, isEmpty, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(withFilter, size, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, nonEmpty, isInstanceOf, <init>, max, remove, flatMap, ==, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, logError, !=, logWarning, contains, isEmpty, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, remove, ++, flatMap, ==, clone, foreach, exists, getBytes, toArray, toSeq, zipWithIndex, toString, length, seq, logError, !=, collect, getClass, logWarning, close, contains, isEmpty, ne, indexOf, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(size, toSet, map, toMap, asInstanceOf, mkString, logTrace, nonEmpty, to, isInstanceOf, <init>, remove, flatMap, ==, foreach, next, flush, length, logError, !=, logWarning, close, contains, ne, hasNext, write, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala: Set(size, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala: Set(map, <init>, sum)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(count, size, map, clear, asInstanceOf, min, isInstanceOf, <init>, remove, ==, exists, toArray, toString, length, !=, getSize, logWarning, close, contains, write, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ==, foreach, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(map, filterNot, asInstanceOf, synchronized, isInstanceOf, filter, <init>, ++, ==, foreach, toSeq, toString, !=, collect, getClass, logWarning, contains, isEmpty, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, <init>, ==, foreach, exists, length, logError, !=, logWarning, close, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(zip, duplicate, map, asInstanceOf, synchronized, nonEmpty, putBytes, isInstanceOf, <init>, ==, foreach, next, zipWithIndex, length, !=, close, ne, hasNext, sum, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, <init>, remove, foreach, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, isInstanceOf, filter, GroupedIterator, <init>, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, logWarning, contains, isEmpty, ne, hasNext, reduceLeft, sum, log, scanLeft, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>, exists, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(map, nonEmpty, filter, <init>, contains)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(size, map, asInstanceOf, synchronized, mkString, nonEmpty, <init>, buffered, ++, ==, foreach, exists, next, flush, length, !=, getClass, logWarning, close, isEmpty, ne, hasNext, write, scanLeft, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(withFilter, size, zip, map, clear, asInstanceOf, synchronized, mkString, isInstanceOf, filter, <init>, buffered, ++, flatMap, ==, foreach, exists, toArray, next, flush, toString, length, !=, logWarning, close, isEmpty, ne, hasNext, eq, write, scanLeft, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(withFilter, wait, size, duplicate, map, asInstanceOf, synchronized, mkString, min, nonEmpty, putBytes, notifyAll, isInstanceOf, <init>, remove, ++, ==, foreach, toArray, logError, !=, logWarning, contains, isEmpty, ne, eq, sum, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(map, asInstanceOf, isInstanceOf, <init>, flatMap, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ==, foreach, toArray, toString, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(withFilter, size, map, toMap, asInstanceOf, synchronized, mkString, logTrace, nonEmpty, isInstanceOf, filter, <init>, max, take, ==, foreach, toArray, next, toString, length, logError, !=, logWarning, close, contains, isEmpty, ne, hasNext, eq, sum, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, filter, <init>, releaseUnrollMemoryForThisTask, ++, MemoryStore, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(map, asInstanceOf, synchronized, isInstanceOf, <init>, remove, ==, foreach, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(withFilter, map, equals, asInstanceOf, isInstanceOf, <init>, ==, foreach, toArray, zipWithIndex, toString, length, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(toSet, asInstanceOf, isInstanceOf, filter, <init>, ==, foreach, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(size, map, asInstanceOf, isInstanceOf, <init>, ==, logError, !=, isEmpty, ne, sum, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(withFilter, size, map, PartiallyUnrolledIterator, toMap, filterNot, clear, asInstanceOf, synchronized, partition, valuesIterator, mkString, PartiallySerializedBlock, logTrace, putBytes, toIterator, putIteratorAsBytes, to, isInstanceOf, filter, <init>, max, remove, ++, flatMap, getValues, ==, BlockEvictionHandler, foreach, MemoryStore, exists, getBytes, toArray, toSeq, next, toString, length, logError, !=, collect, getClass, getSize, logWarning, close, contains, isEmpty, ne, hasNext, write, putIteratorAsValues, finishWritingToStream, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	handlerAdded, readArgs, notify, RBackendHandler, channelRead0, wait, $asInstanceOf, equals, handleMethodCall, userEventTriggered, findMatchedSignature, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, channelActive, channelUnregistered, logName, notifyAll, isInstanceOf, acceptInboundMessage, <init>, added, handlerRemoved, channelReadComplete, isSharable, channelInactive, ensureNotSharable, ==, clone, channelRead, $init$, toString, logError, !=, channelRegistered, exceptionCaught, getClass, logWarning, ne, eq, log, channelWritabilityChanged, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(RBackendHandler, asInstanceOf, initializeLogIfNecessary, <init>, ==, logError, !=, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, RRunner, equals, asInstanceOf, synchronized, $isInstanceOf, main, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, HadoopMapReduceWriteConfigUtil, equals, asInstanceOf, initializeLogIfNecessary, HadoopMapRedWriteConfigUtil, synchronized, $isInstanceOf, createCommitter, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, initOutputFormat, logName, notifyAll, isInstanceOf, closeWriter, <init>, ==, clone, $init$, SparkHadoopWriter, toString, assertConf, logError, !=, getClass, logWarning, createTaskAttemptContext, ne, createJobContext, eq, write, log, ##, finalize, hashCode, logDebug, initWriter, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(HadoopMapReduceWriteConfigUtil, asInstanceOf, HadoopMapRedWriteConfigUtil, isInstanceOf, <init>, ==, SparkHadoopWriter, toString, !=, logWarning, ne, write, log)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, PythonPartitioner, asInstanceOf, synchronized, $isInstanceOf, pyPartitionFunctionId, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, getPartition, ne, numPartitions, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newTaskTempFile, notify, wait, $asInstanceOf, setupJob, HadoopMapRedCommitProtocol, equals, newTaskTempFileAbsPath, setupCommitter, abortJob, abortTask, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, commitJob, <init>, commitTask, ==, clone, $init$, deleteWithJob, toString, logError, !=, getClass, logWarning, setupTask, ne, eq, log, onTaskCommit, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskLocation.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, HDFSCacheTaskLocation, wait, copy$default$2, $asInstanceOf, executorLocationTag, productArity, equals, asInstanceOf, host, synchronized, $isInstanceOf, canEqual, executorId, productPrefix, notifyAll, isInstanceOf, <init>, apply, ==, clone, $init$, copy, toString, ExecutorCacheTaskLocation, !=, getClass, copy$default$1, inMemoryLocationTag, ne, TaskLocation, HostTaskLocation, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, host, synchronized, executorId, isInstanceOf, <init>, apply, ==, clone, toString, !=, ne, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala: Set(<init>, apply, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(HDFSCacheTaskLocation, asInstanceOf, host, synchronized, executorId, isInstanceOf, <init>, apply, ==, toString, ExecutorCacheTaskLocation, !=, ne, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, <init>, apply, ==, !=, ne, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, host, synchronized, executorId, isInstanceOf, <init>, apply, ==, clone, toString, !=, getClass, ne, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, host, synchronized, executorId, isInstanceOf, <init>, apply, ==, !=, ne, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(equals, asInstanceOf, host, <init>, apply, ==, ne, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, <init>, apply, !=, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(asInstanceOf, <init>, apply, ==, ne, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(asInstanceOf, host, isInstanceOf, <init>, apply, ==, toString, ne, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(HDFSCacheTaskLocation, equals, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, toString, !=, HostTaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, host, executorId, isInstanceOf, <init>, apply, ==, toString, !=, ne, TaskLocation)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationClient.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, killExecutor, wait, $asInstanceOf, requestExecutors, equals, killExecutors, asInstanceOf, ExecutorAllocationClient, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, killExecutors$default$4, $init$, killExecutorsOnHost, toString, !=, getClass, getExecutorIds, requestTotalExecutors, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(requestExecutors, killExecutors, asInstanceOf, ExecutorAllocationClient, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, getExecutorIds, requestTotalExecutors, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(killExecutors, asInstanceOf, ExecutorAllocationClient, synchronized, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(requestExecutors, killExecutors, asInstanceOf, ExecutorAllocationClient, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, getExecutorIds, requestTotalExecutors, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(killExecutors, asInstanceOf, ExecutorAllocationClient, synchronized, isInstanceOf, ==, !=, requestTotalExecutors, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(killExecutors, asInstanceOf, ExecutorAllocationClient, isInstanceOf, ==, killExecutorsOnHost, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, ExecutorAllocationClient, synchronized, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(killExecutors, asInstanceOf, isInstanceOf, ==, toString, !=, requestTotalExecutors)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(requestExecutors, killExecutors, asInstanceOf, ExecutorAllocationClient, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, getExecutorIds, requestTotalExecutors, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, prefix, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, AllStagesPage, <init>, ==, clone, renderJson, toString, !=, getClass, render, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(AllStagesPage, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, lookup, sampleByKey, wait, combineByKeyWithClassTag, $asInstanceOf, fullOuterJoin, sampleByKeyExact$default$3, saveAsNewAPIHadoopFile$default$5, countApproxDistinctByKey$default$1, partitionBy, join, combineByKey$default$5, equals, saveAsHadoopFile, aggregateByKey, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, reduceByKey, logTrace, combineByKey$default$6, isTraceEnabled, initializeLogIfNecessary$default$2, sampleByKeyExact, saveAsHadoopFile$default$6, logName, notifyAll, countApproxDistinctByKey, keyOrdering, groupByKey, saveAsNewAPIHadoopFile, PairRDDFunctions, isInstanceOf, groupWith, countByKey, sampleByKey$default$3, <init>, combineByKey, keyClass, rightOuterJoin, ==, clone, valueClass, saveAsHadoopFile$default$5, $init$, flatMapValues, combineByKeyWithClassTag$default$5, values, toString, logError, !=, collectAsMap, getClass, logWarning, saveAsHadoopDataset, reduceByKeyLocally, leftOuterJoin, mapValues, ne, saveAsNewAPIHadoopDataset, subtractByKey, eq, combineByKeyWithClassTag$default$6, countByKeyApprox$default$2, countByKeyApprox, log, foldByKey, ##, finalize, keys, hashCode, logDebug, cogroup, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(saveAsHadoopFile, asInstanceOf, synchronized, reduceByKey, keyOrdering, groupByKey, PairRDDFunctions, isInstanceOf, countByKey, <init>, ==, clone, values, toString, !=, getClass, logWarning, ne, subtractByKey, log, foldByKey, keys, cogroup, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala: Set(reduceByKey, PairRDDFunctions, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala: Set(saveAsHadoopFile, PairRDDFunctions, <init>, keyClass, valueClass, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(lookup, sampleByKey, combineByKeyWithClassTag, fullOuterJoin, partitionBy, join, saveAsHadoopFile, aggregateByKey, asInstanceOf, reduceByKey, sampleByKeyExact, countApproxDistinctByKey, groupByKey, saveAsNewAPIHadoopFile, PairRDDFunctions, groupWith, countByKey, <init>, combineByKey, keyClass, rightOuterJoin, valueClass, flatMapValues, collectAsMap, saveAsHadoopDataset, reduceByKeyLocally, leftOuterJoin, mapValues, ne, saveAsNewAPIHadoopDataset, subtractByKey, countByKeyApprox, foldByKey, cogroup)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala: Set(PairRDDFunctions, countByKey, <init>, ==, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, saveAsHadoopFile, asInstanceOf, synchronized, logTrace, saveAsNewAPIHadoopFile, PairRDDFunctions, isInstanceOf, <init>, keyClass, ==, valueClass, values, toString, !=, getClass, logWarning, saveAsHadoopDataset, ne, saveAsNewAPIHadoopDataset, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(asInstanceOf, saveAsNewAPIHadoopFile, PairRDDFunctions, isInstanceOf, <init>, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	SubmitRestConnectionException, notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, SubmitRestProtocolException, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, SubmitRestMissingFieldException, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(asInstanceOf, isInstanceOf, getStackTrace, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(SubmitRestConnectionException, asInstanceOf, SubmitRestProtocolException, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(SubmitRestConnectionException, printStackTrace, asInstanceOf, getCause, isInstanceOf, getStackTrace, <init>, getMessage, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, isInstanceOf, <init>, SubmitRestMissingFieldException, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala: Set(SubmitRestProtocolException, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala: Set(asInstanceOf, SubmitRestProtocolException, isInstanceOf, <init>, SubmitRestMissingFieldException, ==, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, read, withPadding, find, span, toBuffer, count, reduceOption, resume, wait, isAlive, foldRight, threadLocalRandomSeed, takeWhile, $asInstanceOf, setName, minBy, size, zip, toSet, getContextClassLoader, join, corresponds, :\, handleEndOfDataSection, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, writeIteratorToStream, setPriority, handleException, parkBlocker, toList, threadLocalRandomProbe, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, run, getPriority, shutdownOnTaskCompletion, reduceLeftOption, synchronized, sliding, partition, blockedOn, aggregate, $isInstanceOf, exception, forall, compute, mkString, checkAccess, SQL_BATCHED_UDF, min, newWriterThread, scanRight, NON_UDF, TIMING_DATA, envVars, fold, suspend, logTrace, nonEmpty, pythonVer, PYTHON_EXCEPTION_THROWN, isTraceEnabled, initializeLogIfNecessary$default$2, getUncaughtExceptionHandler, getThreadGroup, stop, logName, SQL_GROUPED_MAP_PANDAS_UDF, notifyAll, START_ARROW_STREAM, /:, toIterator, getName, addString, to, collectFirst, END_OF_DATA_SECTION, isInterrupted, drop, isInstanceOf, getState, filter, getStackTrace, handlePythonException, GroupedIterator, <init>, toStream, destroy, NULL, pythonExec, max, buffered, apply, ++, grouped, flatMap, take, WriterThread, reduceRight, ==, maxBy, indexWhere, accumulator, clone, SpecialLengths, setDaemon, slice, foreach, writeCommand, exists, reduceRightOption, toVector, toIndexedSeq, PythonEvalType, copyToBuffer, toArray, inheritableThreadLocals, reduce, padTo, $init$, toSeq, ReaderIterator, next, zipWithIndex, setContextClassLoader, threadLocals, END_OF_STREAM, toString, copyToArray, length, seq, isDaemon, logError, !=, collect, handleTimingData, getClass, logWarning, hasDefiniteSize, setUncaughtExceptionHandler, countStackFrames, patch, start, foldLeft, contains, isEmpty, MonitorThread, ne, withPartial, PythonRunner, getId, reversed, hasNext, SQL_SCALAR_PANDAS_UDF, indexOf, threadLocalRandomSecondarySeed, reduceLeft, BasePythonRunner, eq, interrupt, newReaderIterator, sum, log, ##, scanLeft, finalize, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(read, wait, size, map, writeIteratorToStream, asInstanceOf, synchronized, compute, mkString, min, envVars, logTrace, pythonVer, isInstanceOf, GroupedIterator, <init>, NULL, pythonExec, apply, grouped, flatMap, ==, accumulator, SpecialLengths, setDaemon, foreach, exists, toArray, toString, length, !=, collect, getClass, logWarning, start, isEmpty, ne, PythonRunner, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	VoidFunction, call.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/VoidFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(VoidFunction, call)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, TaskCompletionListenerException, printStackTrace, onTaskCompletion, getLocalizedMessage, wait, $asInstanceOf, TaskCompletionListener, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, previousError, notifyAll, TaskFailureListener, isInstanceOf, getStackTrace, getStackTraceElement, <init>, onTaskFailure, getMessage, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, getStackTrace, <init>, setStackTrace, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala: Set(<init>, getMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(asInstanceOf, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, getMessage, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(printStackTrace, equals, initCause, asInstanceOf, synchronized, getCause, isInstanceOf, getStackTrace, <init>, getMessage, setStackTrace, ==, addSuppressed, toString, !=, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(asInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, getMessage, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(TaskCompletionListenerException, onTaskCompletion, TaskCompletionListener, synchronized, TaskFailureListener, <init>, onTaskFailure, getMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, <init>, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(equals, asInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, getMessage, ==, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala: Set(asInstanceOf, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, <init>, ==, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala: Set(wait, asInstanceOf, synchronized, notifyAll, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, <init>, addSuppressed, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(asInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala: Set(asInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(asInstanceOf, <init>, clone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/taskListeners.scala: Set(TaskCompletionListenerException, previousError, getStackTrace, <init>, getMessage, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(asInstanceOf, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(TaskCompletionListener, TaskFailureListener, <init>, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(TaskCompletionListenerException, onTaskCompletion, TaskCompletionListener, synchronized, TaskFailureListener, <init>, onTaskFailure, getMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(TaskCompletionListener, TaskFailureListener, <init>, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(printStackTrace, equals, initCause, asInstanceOf, synchronized, getCause, isInstanceOf, getStackTrace, <init>, getMessage, setStackTrace, ==, addSuppressed, toString, !=, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, <init>, addSuppressed, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationState.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationState.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	withFilter, notify, recoverWith, submitJob, cancel, isCompleted, wait, $asInstanceOf, zip, map, equals, JobSubmitter, isCancelled, asInstanceOf, ready, result, synchronized, failed, $isInstanceOf, andThen, SimpleFutureAction, notifyAll, FutureAction, isInstanceOf, filter, onSuccess, recover, <init>, flatMap, isDone, ==, clone, JavaFutureActionWrapper, foreach, $init$, mapTo, toString, !=, get, fallbackTo, collect, getClass, transformWith, onComplete, jobIds, onFailure, ne, transform, value, eq, ##, finalize, hashCode, ComplexFutureAction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(submitJob, JobSubmitter, result, SimpleFutureAction, FutureAction, <init>, flatMap, ==, foreach, get, ComplexFutureAction)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(zip, map, asInstanceOf, FutureAction, <init>, flatMap, JavaFutureActionWrapper, foreach, collect)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, submitJob, map, asInstanceOf, result, synchronized, SimpleFutureAction, isInstanceOf, filter, <init>, flatMap, ==, clone, foreach, toString, !=, get, collect, getClass, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, waitTillDataReceived, markCheckpointed, notify, register, readRDDFromInputStream, secret, read, mapPartitionsWithIndex$default$2, register$default$2, ChainedPythonFunctions, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, PythonBroadcast, coalesce, name, count, PythonException, printStackTrace, getLocalizedMessage, PythonRDD, funcs, wait, DechunkedInputStream, copy$default$2, $asInstanceOf, EncryptedPythonBroadcastServer, isCheckpointedAndMaterialized, mapPartitions, setName, path, union, coalesce$default$3, copy$default$5, zip, asJavaRDD, localCheckpoint, map, productArity, subtract, countFailedValues, equals, pipe$default$5, intersection, getResult, isZero, writeIteratorToStream, sortBy$default$3, fillInStackTrace, foreachPartition, PythonRDDServer, countApprox$default$2, scope, setupBroadcast, PythonServer, initCause, saveAsHadoopFile, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, register$default$3, sortBy, pipe$default$6, PythonFunction, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, serveIterator, aggregate, $isInstanceOf, waitTillBroadcastDataSent, compute, getCause, handleConnection, mapPartitions$default$2, PythonAccumulatorV2, min, getCheckpointFile, envVars, fold, skip, newAPIHadoopRDD, getOutputDeterministicLevel, logTrace, copyAndReset, canEqual, pythonVer, treeAggregate$default$4, toInfo, copy$default$4, broadcastVars, isTraceEnabled, pythonIncludes, initializeLogIfNecessary$default$2, command, zipWithUniqueId, productPrefix, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, saveAsNewAPIHadoopFile, markSupported, cache, getNumPartitions, isInstanceOf, filter, getStackTrace, pipe$default$3, getStackTraceElement, setValue, writeUTF, countByValueApprox$default$3, unpersist$default$1, valueOfPair, persist, checkpointData, <init>, isCheckpointed, merge, toLocalIteratorAndServe, id, pythonExec, getMessage, mapPartitionsWithIndexInternal, setStackTrace, countApproxDistinct, port, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, getSuppressed, flatMap, getWorkerBroadcasts, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, accumulator, getStackTraceDepth, groupBy$default$4, clone, distinct, retag, foreach, addSuppressed, collectAndServe, treeReduce, toLocalIterator, copy$default$7, sparkContext, reduce, reuseWorker, saveAsTextFile, hadoopRDD, $init$, bufferSize, runJob, takeSample$default$3, zipWithIndex, dechunkAndCopyToOutput, readRDDFromFile, getStorageLevel, checkpoint, first, newAPIHadoopFile, countByValue, countByValueApprox$default$2, isAtDriverSide, setupEncryptionServer, writeReplace, elementClassTag, sequenceFile, copy$default$3, sample, copy, pipe$default$7, reset, metadata, available, hadoopFile, streamToRDD, toString, mapPartitionsInternal, preferredLocations, logError, idsAndFiles, !=, isRegistered, partitions, collect, getClass, pipe, logWarning, getPartitions, copy$default$1, saveAsHadoopDataset, pipe$default$4, cartesian, mark, repartition, close, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, copy$default$6, promise, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, add, randomSplit, top, coalesce$default$2, getCreationSite, value, computeOrReadCheckpoint, setupOneConnectionServer, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, saveAsSequenceFile, toJavaRDD, eq, isReliablyCheckpointed, productIterator, withScope, log, env, ##, finalize, treeAggregate, PythonParallelizeServer, productElement, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(secret, read, ChainedPythonFunctions, PythonBroadcast, PythonException, PythonRDD, funcs, EncryptedPythonBroadcastServer, path, map, writeIteratorToStream, asInstanceOf, context, PythonFunction, waitTillBroadcastDataSent, PythonAccumulatorV2, envVars, logTrace, pythonVer, broadcastVars, pythonIncludes, command, conf, isInstanceOf, writeUTF, <init>, id, pythonExec, port, flatMap, getWorkerBroadcasts, ==, accumulator, foreach, reuseWorker, bufferSize, logError, idsAndFiles, !=, logWarning, close, ne, add, value, env, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(register, read, PythonBroadcast, name, map, asInstanceOf, skip, conf, isInstanceOf, filter, <init>, getMessage, max, ==, foreach, bufferSize, reset, logError, !=, getClass, close, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(parent, name, PythonRDD, asJavaRDD, map, asInstanceOf, context, compute, broadcastVars, iterator, <init>, ==, foreach, readRDDFromFile, toString, !=, partitions, ne, value, firstParent)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapPartitionsFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapPartitionsFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapPartitionsFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapPartitionsFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapPartitionsFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, MapPartitionsFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/MapPartitionsFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	TimeTrackingOutputStream, notify, wait, equals, notifyAll, <init>, flush, toString, getClass, close, write, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(TimeTrackingOutputStream, <init>, flush, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onExecutorUnblacklisted, notify, totalRunningTasks, setClock, wait, $asInstanceOf, onJobEnd, onApplicationEnd, listener, equals, onTaskEnd, totalPendingTasks, onStageCompleted, onTaskGettingResult, asInstanceOf, ExecutorAllocationManager, onJobStart, initializeLogIfNecessary, metricRegistry, executorAllocationManagerSource, synchronized, $isInstanceOf, isExecutorIdle, logTrace, onOtherEvent, ExecutorAllocationListener, isTraceEnabled, initializeLogIfNecessary$default$2, onTaskStart, stop, logName, notifyAll, onExecutorRemoved, sourceName, onExecutorAdded, isInstanceOf, <init>, onBlockUpdated, ==, onBlockManagerRemoved, pendingSpeculativeTasks, clone, onExecutorMetricsUpdate, onEnvironmentUpdate, onBlockManagerAdded, ExecutorAllocationManagerSource, $init$, onNodeBlacklisted, onApplicationStart, reset, onSpeculativeTaskSubmitted, toString, logError, !=, getClass, logWarning, start, onExecutorBlacklisted, onUnpersistRDD, updateExecutorPlacementHints, ne, onStageSubmitted, eq, onNodeUnblacklisted, log, pendingTasks, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(listener, asInstanceOf, ExecutorAllocationManager, executorAllocationManagerSource, synchronized, stop, isInstanceOf, <init>, ==, clone, ExecutorAllocationManagerSource, toString, logError, !=, getClass, logWarning, start, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, ConsoleProgressBar, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, finishAll, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, stop, ConsoleProgressBar, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, ne, log, finishAll, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/CoGroupFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/CoGroupFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/CoGroupFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/CoGroupFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/CoGroupFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, CoGroupFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/CoGroupFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	setExecutorCpuTime, notify, register, registered, resultSerializationTime, wait, createTempShuffleReadMetrics, $asInstanceOf, TaskMetrics, empty, mergeShuffleReadMetrics, equals, peakExecutionMemory, inputMetrics, asInstanceOf, initializeLogIfNecessary, fromAccumulatorInfos, shuffleWriteMetrics, nameToAccums, accumulators, synchronized, diskBytesSpilled, setUpdatedBlockStatuses, $isInstanceOf, internalAccums, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, setJvmGCTime, incMemoryBytesSpilled, logName, notifyAll, setExecutorRunTime, incDiskBytesSpilled, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, setExecutorDeserializeTime, jvmGCTime, testAccum, setResultSize, <init>, setResultSerializationTime, incUpdatedBlockStatuses, ==, outputMetrics, clone, memoryBytesSpilled, $init$, incPeakExecutionMemory, registerAccumulator, externalAccums, toString, logError, !=, executorCpuTime, getClass, setExecutorDeserializeCpuTime, logWarning, ne, executorRunTime, updatedBlockStatuses, shuffleReadMetrics, eq, log, nonZeroInternalAccums, fromAccumulators, ##, finalize, resultSize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(TaskMetrics, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, ==, clone, toString, logError, !=, logWarning, ne, fromAccumulators, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala: Set(TaskMetrics, empty, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala: Set(TaskMetrics, <init>, ==, outputMetrics)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(TaskMetrics, empty, asInstanceOf, diskBytesSpilled, logTrace, incMemoryBytesSpilled, incDiskBytesSpilled, isInstanceOf, <init>, ==, memoryBytesSpilled, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(TaskMetrics, asInstanceOf, diskBytesSpilled, incMemoryBytesSpilled, incDiskBytesSpilled, isInstanceOf, <init>, ==, memoryBytesSpilled, incPeakExecutionMemory, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(TaskMetrics, empty, synchronized, <init>, registerAccumulator, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(resultSerializationTime, TaskMetrics, peakExecutionMemory, inputMetrics, shuffleWriteMetrics, diskBytesSpilled, executorDeserializeTime, executorDeserializeCpuTime, jvmGCTime, <init>, ==, outputMetrics, memoryBytesSpilled, toString, !=, executorCpuTime, executorRunTime, shuffleReadMetrics, eq, resultSize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(register, TaskMetrics, empty, asInstanceOf, isInstanceOf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(TaskMetrics, empty, inputMetrics, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(TaskMetrics, shuffleWriteMetrics, <init>, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(TaskMetrics, empty, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, incUpdatedBlockStatuses, ==, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala: Set(TaskMetrics, empty, asInstanceOf, isInstanceOf, <init>, ==, memoryBytesSpilled, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(TaskMetrics, equals, inputMetrics, asInstanceOf, fromAccumulatorInfos, shuffleWriteMetrics, isInstanceOf, jvmGCTime, <init>, ==, toString, !=, getClass, ne, shuffleReadMetrics, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(TaskMetrics, equals, inputMetrics, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(TaskMetrics, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(TaskMetrics, asInstanceOf, shuffleWriteMetrics, synchronized, diskBytesSpilled, incMemoryBytesSpilled, incDiskBytesSpilled, isInstanceOf, <init>, ==, memoryBytesSpilled, incPeakExecutionMemory, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(setExecutorCpuTime, resultSerializationTime, wait, TaskMetrics, empty, mergeShuffleReadMetrics, inputMetrics, asInstanceOf, shuffleWriteMetrics, accumulators, synchronized, diskBytesSpilled, setJvmGCTime, notifyAll, setExecutorRunTime, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, setExecutorDeserializeTime, jvmGCTime, <init>, setResultSerializationTime, ==, outputMetrics, memoryBytesSpilled, logError, !=, executorCpuTime, setExecutorDeserializeCpuTime, logWarning, ne, executorRunTime, shuffleReadMetrics, eq, resultSize, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(createTempShuffleReadMetrics, TaskMetrics, mergeShuffleReadMetrics, asInstanceOf, diskBytesSpilled, incMemoryBytesSpilled, incDiskBytesSpilled, isInstanceOf, <init>, ==, memoryBytesSpilled, incPeakExecutionMemory, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(TaskMetrics, <init>, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(createTempShuffleReadMetrics, TaskMetrics, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(registered, TaskMetrics, empty, synchronized, notifyAll, <init>, externalAccums, !=, nonZeroInternalAccums)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(TaskMetrics, asInstanceOf, shuffleWriteMetrics, isInstanceOf, <init>, ==, toString, !=, ne, executorRunTime, shuffleReadMetrics, eq, resultSize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(TaskMetrics, equals, asInstanceOf, diskBytesSpilled, incMemoryBytesSpilled, incDiskBytesSpilled, isInstanceOf, <init>, ==, memoryBytesSpilled, incPeakExecutionMemory, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(TaskMetrics, equals, inputMetrics, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(setExecutorCpuTime, resultSerializationTime, createTempShuffleReadMetrics, TaskMetrics, empty, mergeShuffleReadMetrics, inputMetrics, asInstanceOf, shuffleWriteMetrics, accumulators, diskBytesSpilled, setUpdatedBlockStatuses, setJvmGCTime, incMemoryBytesSpilled, setExecutorRunTime, incDiskBytesSpilled, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, setExecutorDeserializeTime, jvmGCTime, setResultSize, <init>, setResultSerializationTime, ==, outputMetrics, memoryBytesSpilled, toString, !=, executorCpuTime, setExecutorDeserializeCpuTime, ne, executorRunTime, updatedBlockStatuses, shuffleReadMetrics, resultSize)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createWithExistingInMemorySorter, closeCurrentPage, notify, access$802, access$800, getPeakMemoryUsedBytes, wait, getRecordLength, $assertionsDisabled, equals, access$702, cleanupResources, access$900, allocateArray, create, UnsafeExternalSorter, access$200, getSpillSize, insertRecord, access$500, UnsafeExternalSorter$SpillableIterator, notifyAll, UnsafeExternalSorter$ChainedIterator, access$300, <init>, merge, access$1000, hasSpaceForAnotherRecord, access$600, access$100, this$0, acquireMemory, getKeyPrefix, access$000, freeArray, toString, loadNext, getBaseOffset, freeMemory, access$700, getBaseObject, spill, getClass, insertKVRecord, getMode, getNumRecords, hasNext, getSortedIterator, access$400, access$1100, getNumberOfAllocatedPages, getSortTimeNanos, getIterator, hashCode, access$1002.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, Function.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(call, Function)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala: Set(call, Function)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(call, Function)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(call, Function)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(Function)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(call, Function)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/Source.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, metricRegistry, synchronized, $isInstanceOf, notifyAll, sourceName, isInstanceOf, Source, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, metricRegistry, sourceName, Source, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, Source, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala: Set(metricRegistry, sourceName, Source)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala: Set(metricRegistry, sourceName, Source)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, metricRegistry, synchronized, sourceName, isInstanceOf, Source, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, Source, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(synchronized, sourceName, Source)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, metricRegistry, synchronized, sourceName, isInstanceOf, Source, ==, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, metricRegistry, synchronized, sourceName, isInstanceOf, Source, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, Source, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, Source, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala: Set(metricRegistry, sourceName, Source)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(Source, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(asInstanceOf, metricRegistry, sourceName, isInstanceOf, Source, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(Source, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/JvmSource.scala: Set(metricRegistry, sourceName, Source)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala: Set(metricRegistry, sourceName, Source, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala: Set(equals, metricRegistry, sourceName, Source)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleServiceSource.scala: Set(metricRegistry, sourceName, Source)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/source/StaticSources.scala: Set(metricRegistry, sourceName, Source)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala: Set(metricRegistry, sourceName, Source, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, Source, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(!=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, Source, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, metricRegistry, synchronized, sourceName, isInstanceOf, Source, ==, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, Source, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, Source, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala: Set(metricRegistry, sourceName, Source)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(!=)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, ==, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, Source, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, Source, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, metricRegistry, synchronized, sourceName, isInstanceOf, Source, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(metricRegistry, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, Source, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala: Set(asInstanceOf, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, Source, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, Source, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(Source, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, metricRegistry, sourceName, Source, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, Source, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryMode.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryMode.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryMode.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryMode.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryMode.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, valueOf, equals, getDeclaringClass, ON_HEAP, notifyAll, compareTo, ordinal, OFF_HEAP, values, toString, getClass, MemoryMode, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryMode.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala: Set(OFF_HEAP, MemoryMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(values, toString, getClass, MemoryMode, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(MemoryMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala: Set(wait, notifyAll, values, MemoryMode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(MemoryMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala: Set(MemoryMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(notifyAll, MemoryMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(MemoryMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(values, toString, MemoryMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(MemoryMode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createPathFromString, notify, initHadoopOutputMetrics, wait, $asInstanceOf, disableOutputSpecValidation, maybeUpdateOutputMetrics, createJobTrackerID, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, createJobID, toString, isOutputSpecValidationEnabled, !=, getClass, SparkHadoopWriterUtils, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(initHadoopOutputMetrics, maybeUpdateOutputMetrics, createJobTrackerID, asInstanceOf, isInstanceOf, ==, toString, isOutputSpecValidationEnabled, !=, getClass, SparkHadoopWriterUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(createPathFromString, asInstanceOf, isInstanceOf, ==, toString, !=, SparkHadoopWriterUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala: Set(asInstanceOf, isInstanceOf, createJobID, toString, SparkHadoopWriterUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, shuffleMetrics, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, fetchBlocks, synchronized, $isInstanceOf, hostName, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, uploadBlockSync, isInstanceOf, <init>, port, ==, clone, uploadBlock, $init$, toString, logError, !=, fetchBlockSync, getClass, logWarning, close, ne, init, eq, BlockTransferService, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(hostName, logTrace, <init>, port, logError, !=, getClass, close, BlockTransferService, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(shuffleMetrics, asInstanceOf, synchronized, hostName, logTrace, uploadBlockSync, isInstanceOf, <init>, port, ==, toString, logError, !=, fetchBlockSync, getClass, logWarning, close, ne, init, BlockTransferService, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, port, ==, toString, !=, getClass, logWarning, ne, BlockTransferService, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, port, ==, toString, !=, getClass, logWarning, ne, BlockTransferService, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, killExecutors, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, StandaloneAppClient, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, start, requestTotalExecutors, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(killExecutors, asInstanceOf, stop, StandaloneAppClient, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, start, requestTotalExecutors, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $assertionsDisabled, equals, getKey, newKey, notifyAll, allocate, <init>, swap, toString, copyElement, getClass, ShuffleSortDataFormat, hashCode, copyRange.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	GRAPHITE_KEY_PORT, notify, GRAPHITE_KEY_PERIOD, registry, wait, $asInstanceOf, equals, reporter, pollUnit, prefix, asInstanceOf, host, synchronized, $isInstanceOf, GRAPHITE_DEFAULT_PREFIX, stop, notifyAll, isInstanceOf, GraphiteSink, <init>, GRAPHITE_KEY_HOST, port, GRAPHITE_DEFAULT_PERIOD, GRAPHITE_DEFAULT_UNIT, GRAPHITE_KEY_PROTOCOL, pollPeriod, ==, clone, GRAPHITE_KEY_PREFIX, report, toString, property, !=, getClass, propertyToOption, start, GRAPHITE_KEY_UNIT, graphite, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, SparkConfigProvider, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, ConfigProvider, notifyAll, EnvProvider, isInstanceOf, <init>, MapProvider, ==, clone, SystemProvider, toString, !=, get, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(SparkConfigProvider, asInstanceOf, ConfigProvider, isInstanceOf, <init>, ==, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigReader.scala: Set(asInstanceOf, ConfigProvider, EnvProvider, isInstanceOf, <init>, MapProvider, ==, SystemProvider, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala: Set(<init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala: Set(<init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala: Set(<init>, ==, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(synchronized, <init>, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(<init>, clone, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala: Set(<init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala: Set(synchronized, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala: Set(<init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(<init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala: Set(<init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala: Set(asInstanceOf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(<init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala: Set(<init>, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala: Set(<init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, <init>, get, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala: Set(<init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala: Set(<init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(<init>, ==, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(synchronized, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(isInstanceOf, <init>, ==, toString, get, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asInstanceOf, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala: Set(asInstanceOf, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(<init>, !=, get, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala: Set(<init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, <init>, ==, !=, get, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala: Set(asInstanceOf, <init>, ==, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala: Set(SparkConfigProvider, ConfigProvider, EnvProvider, <init>, MapProvider, SystemProvider, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(synchronized, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala: Set(<init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(<init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, get, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(<init>, ==, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala: Set(<init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala: Set(<init>, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(<init>, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(<init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala: Set(isInstanceOf, <init>, !=, get, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(<init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala: Set(ConfigProvider, <init>, clone, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala: Set(<init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(<init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(synchronized, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala: Set(<init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, get, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, ==, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala: Set(asInstanceOf, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala: Set(synchronized, <init>, ==, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, <init>, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala: Set(asInstanceOf, isInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, get, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(<init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, isInstanceOf, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(synchronized, <init>, ==, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, get, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(asInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, ==, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala: Set(<init>, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(asInstanceOf, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala: Set(asInstanceOf, isInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(asInstanceOf, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(isInstanceOf, <init>, ==, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(<init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, <init>, ==, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(synchronized, <init>, clone, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(<init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	listStatus, notify, isSplitable, wait, getRecordLength, $asInstanceOf, FixedLengthBinaryInputFormat, RECORD_LENGTH_PROPERTY, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, getFormatMinSplitSize, getSplits, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, getBlockIndex, logName, notifyAll, computeSplitSize, isInstanceOf, addInputPathRecursively, <init>, ==, createRecordReader, makeSplit, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(FixedLengthBinaryInputFormat, RECORD_LENGTH_PROPERTY, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala: Set(getRecordLength, FixedLengthBinaryInputFormat, asInstanceOf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, weight, parent, name, priority, decreaseRunningTasks, wait, $asInstanceOf, runningTasks, equals, stageId, asInstanceOf, initializeLogIfNecessary, getSortedTaskSetQueue, synchronized, schedulableQueue, $isInstanceOf, checkSpeculatableTasks, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, schedulableNameToSchedulable, <init>, addSchedulable, getSchedulableByName, minShare, ==, poolName, executorLost, clone, $init$, removeSchedulable, toString, logError, !=, getClass, logWarning, schedulingMode, Pool, ne, eq, log, increaseRunningTasks, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala: Set(Pool)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(weight, parent, name, priority, decreaseRunningTasks, runningTasks, stageId, asInstanceOf, synchronized, checkSpeculatableTasks, isInstanceOf, <init>, minShare, ==, toString, logError, !=, logWarning, Pool, ne, increaseRunningTasks, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(parent, name, stageId, asInstanceOf, synchronized, schedulableQueue, isInstanceOf, schedulableNameToSchedulable, <init>, ==, clone, toString, logError, !=, getClass, logWarning, schedulingMode, Pool, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala: Set(weight, name, <init>, addSchedulable, getSchedulableByName, minShare, ==, poolName, logError, !=, logWarning, schedulingMode, Pool, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala: Set(Pool)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(parent, name, wait, runningTasks, stageId, asInstanceOf, getSortedTaskSetQueue, synchronized, checkSpeculatableTasks, isInstanceOf, <init>, ==, executorLost, removeSchedulable, toString, logError, !=, logWarning, schedulingMode, Pool, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/VersionUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/VersionUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/VersionUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/VersionUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/VersionUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, majorVersion, asInstanceOf, VersionUtils, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, minorVersion, toString, !=, getClass, majorMinorVersion, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SimpleDateParam.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, timestamp, originalValue, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, SimpleDateParam, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala: Set(timestamp, <init>, SimpleDateParam)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingMode.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingMode.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingMode.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingMode.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingMode.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingMode.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, clearFailures, wait, $asInstanceOf, equals, ShuffleMapStage, shuffleDep, asInstanceOf, initializeLogIfNecessary, makeNewStageAttempt, parents, synchronized, latestInfo, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, addActiveJob, notifyAll, isInstanceOf, rdd, findMissingPartitions, mapStageJobs, <init>, id, pendingPartitions, ==, clone, numTasks, $init$, details, toString, isAvailable, logError, firstJobId, !=, callSite, removeActiveJob, getClass, logWarning, jobIds, numAvailableOutputs, ne, numPartitions, eq, log, ##, finalize, hashCode, logDebug, makeNewStageAttempt$default$2, logInfo, fetchFailedAttemptIds.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(name, clearFailures, ShuffleMapStage, shuffleDep, asInstanceOf, makeNewStageAttempt, parents, synchronized, latestInfo, logTrace, addActiveJob, isInstanceOf, rdd, findMissingPartitions, mapStageJobs, <init>, id, pendingPartitions, ==, clone, numTasks, toString, isAvailable, logError, firstJobId, !=, callSite, removeActiveJob, logWarning, jobIds, numAvailableOutputs, ne, numPartitions, logDebug, logInfo, fetchFailedAttemptIds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(ShuffleMapStage, asInstanceOf, isInstanceOf, rdd, <init>, callSite, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SparkRDefaults.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SparkRDefaults.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SparkRDefaults.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SparkRDefaults.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SparkRDefaults.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, DEFAULT_HEARTBEAT_INTERVAL, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, DEFAULT_NUM_RBACKEND_THREADS, ==, clone, toString, !=, getClass, SparkRDefaults, ne, DEFAULT_CONNECTION_TIMEOUT, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(DEFAULT_HEARTBEAT_INTERVAL, asInstanceOf, isInstanceOf, ==, !=, getClass, SparkRDefaults, DEFAULT_CONNECTION_TIMEOUT)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, SparkRDefaults, DEFAULT_CONNECTION_TIMEOUT)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(==, toString, !=, SparkRDefaults, ne, DEFAULT_CONNECTION_TIMEOUT)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(asInstanceOf, DEFAULT_NUM_RBACKEND_THREADS, ==, !=, SparkRDefaults, ne, DEFAULT_CONNECTION_TIMEOUT)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, WorkerPage, prefix, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, executorRow, clone, renderJson, toString, driverRow, !=, getClass, render, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(WorkerPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	taskSucceeded, notify, jobFailed, cancel, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, jobFinished, <init>, ==, clone, jobId, $init$, toString, completionFuture, logError, !=, getClass, logWarning, JobWaiter, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala: Set(cancel, asInstanceOf, synchronized, isInstanceOf, jobFinished, <init>, jobId, completionFuture, JobWaiter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(taskSucceeded, jobFailed, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, ==, clone, jobId, toString, completionFuture, logError, !=, logWarning, JobWaiter, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, jobId, toString, logError, !=, getClass, logWarning, JobWaiter, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, metadata$1, name, internal, wait, copy$default$2, $asInstanceOf, copy$default$5, productArity, countFailedValues, equals, asInstanceOf, apply$default$7, synchronized, <init>$default$7, $isInstanceOf, canEqual, copy$default$4, productPrefix, notifyAll, isInstanceOf, countFailedValues$1, <init>, id, AccumulableInfo, apply, ==, clone, copy$default$7, $init$, copy$default$3, copy, metadata, toString, !=, getClass, copy$default$1, update, internal$1, copy$default$6, ne, value, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(unapply, name, internal, asInstanceOf, synchronized, isInstanceOf, <init>, id, AccumulableInfo, apply, ==, clone, toString, !=, update, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala: Set(name, <init>, id, AccumulableInfo, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(name, internal, <init>, id, AccumulableInfo, apply, ==, metadata, toString, !=, update, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(name, internal, countFailedValues, asInstanceOf, synchronized, isInstanceOf, <init>, id, AccumulableInfo, apply, ==, copy, metadata, toString, !=, getClass, update, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(name, internal, equals, asInstanceOf, isInstanceOf, <init>, id, AccumulableInfo, apply, ==, toString, !=, getClass, update, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(name, internal, asInstanceOf, synchronized, <init>, AccumulableInfo, apply, ==, metadata, update, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(asInstanceOf, isInstanceOf, <init>, AccumulableInfo, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(name, internal, wait, asInstanceOf, synchronized, isInstanceOf, <init>, id, AccumulableInfo, apply, ==, toString, !=, update, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(unapply, name, internal, wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, AccumulableInfo, apply, ==, !=, update, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala: Set(<init>, AccumulableInfo, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala: Set(internal, asInstanceOf, isInstanceOf, <init>, AccumulableInfo, apply, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(name, countFailedValues, asInstanceOf, <init>, id, AccumulableInfo, apply, ==, copy, toString, update, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(unapply, name, internal, countFailedValues, asInstanceOf, isInstanceOf, <init>, id, AccumulableInfo, apply, ==, metadata, toString, !=, update, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, resume, wait, isAlive, threadLocalRandomSeed, $asInstanceOf, setName, <init>$default$6, RRunner, getContextClassLoader, join, equals, setPriority, readData, parkBlocker, threadLocalRandomProbe, asInstanceOf, initializeLogIfNecessary, run, getPriority, synchronized, blockedOn, <init>$default$7, $isInstanceOf, compute, checkAccess, getLines, suspend, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, getUncaughtExceptionHandler, getThreadGroup, stop, DATAFRAME_GAPPLY, logName, notifyAll, getName, isInterrupted, isInstanceOf, getState, getStackTrace, createRWorker, <init>$default$8, <init>, destroy, ==, clone, setDaemon, inheritableThreadLocals, $init$, RDD, setContextClassLoader, RRunnerModes, threadLocals, toString, BufferedStreamThread, isDaemon, logError, !=, <init>$default$9, getClass, logWarning, setUncaughtExceptionHandler, lineIdx, DATAFRAME_DAPPLY, countStackFrames, start, lines, ne, getId, threadLocalRandomSecondarySeed, eq, interrupt, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(RRunner, asInstanceOf, compute, <init>, ==, RDD, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, registry, wait, $asInstanceOf, equals, reporter, pollUnit, asInstanceOf, SLF4J_DEFAULT_UNIT, synchronized, $isInstanceOf, stop, notifyAll, SLF4J_KEY_PERIOD, isInstanceOf, <init>, pollPeriod, ==, clone, SLF4J_DEFAULT_PERIOD, SLF4J_KEY_UNIT, report, toString, property, !=, getClass, start, ne, eq, Slf4jSink, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, delegationTokensRequired, wait, $asInstanceOf, HadoopFSDelegationTokenProvider, equals, asInstanceOf, initializeLogIfNecessary, serviceName, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, obtainDelegationTokens, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(delegationTokensRequired, HadoopFSDelegationTokenProvider, serviceName, <init>, obtainDelegationTokens, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Command.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Command.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Command.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Command.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Command.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, classPathEntries, wait, copy$default$2, $asInstanceOf, copy$default$5, productArity, equals, asInstanceOf, Command, synchronized, $isInstanceOf, javaOpts, canEqual, copy$default$4, productPrefix, mainClass, notifyAll, isInstanceOf, libraryPathEntries, <init>, ==, clone, environment, $init$, copy$default$3, copy, toString, !=, getClass, copy$default$1, copy$default$6, ne, eq, productIterator, ##, finalize, productElement, hashCode, arguments.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(Command, <init>, ==, toString, arguments)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(classPathEntries, asInstanceOf, Command, javaOpts, isInstanceOf, libraryPathEntries, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ApplicationDescription.scala: Set(asInstanceOf, Command, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(Command, <init>, ==, toString, arguments)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala: Set(Command, <init>, toString, arguments)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, Command, synchronized, javaOpts, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/WorkerCommandBuilder.scala: Set(classPathEntries, Command, javaOpts, <init>, environment)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(Command, <init>, ==, environment, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala: Set(asInstanceOf, Command, mainClass, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, Command, mainClass, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala: Set(classPathEntries, Command, javaOpts, mainClass, libraryPathEntries, <init>, environment, ne, arguments)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, Command, javaOpts, mainClass, isInstanceOf, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(Command, synchronized, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(classPathEntries, asInstanceOf, Command, javaOpts, mainClass, isInstanceOf, libraryPathEntries, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	inferDefaultCores, masters, notify, wait, $asInstanceOf, equals, checkWorkerMemory, cores, asInstanceOf, host, synchronized, $isInstanceOf, webUiPort, workDir, notifyAll, isInstanceOf, <init>, WorkerArguments, port, inferDefaultMemory, ==, clone, printUsageAndExit, toString, !=, getClass, ne, eq, ##, finalize, hashCode, propertiesFile, memory.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(masters, cores, asInstanceOf, host, synchronized, webUiPort, workDir, isInstanceOf, <init>, WorkerArguments, port, ==, toString, !=, ne, memory)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, listLeafDirStatuses, getFSBytesWrittenOnThreadCallback, wait, runAsSparkUser, $asInstanceOf, loginUserFromKeytab, getDateOfNextUpdate, SPARK_YARN_CREDS_COUNTER_DELIM, equals, substituteHadoopVariables, asInstanceOf, initializeLogIfNecessary, tokenToString, SPARK_HADOOP_CONF_FILE, listFilesSorted, synchronized, $isInstanceOf, getSuffixForCredentialsPath, logTrace, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, logName, notifyAll, conf, appendSparkHadoopConfigs, isInstanceOf, <init>, UPDATE_INPUT_METRICS_INTERVAL_RECORDS, isGlobPath, addCurrentUserCredentials, ==, getFSBytesReadOnThreadCallback, isProxyUser, clone, SPARK_YARN_CREDS_TEMP_EXTENSION, $init$, globPathIfNecessary, dumpTokens, transferCredentials, globPath, toString, logError, !=, get, listLeafStatuses, getClass, logWarning, addCredentials, newConfiguration, ne, createSparkUser, addDelegationTokens, serialize, getConfBypassingFSCache, appendS3AndSparkHadoopConfigurations, eq, SparkHadoopUtil, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, logError, !=, get, getClass, logWarning, addCredentials, ne, SparkHadoopUtil, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala: Set(getFSBytesWrittenOnThreadCallback, conf, <init>, ==, get, SparkHadoopUtil)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(runAsSparkUser, asInstanceOf, conf, isInstanceOf, <init>, ==, logError, !=, get, logWarning, ne, addDelegationTokens, SparkHadoopUtil, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, deserialize, conf, isInstanceOf, <init>, ==, clone, toString, logError, !=, get, getClass, logWarning, addCredentials, newConfiguration, ne, SparkHadoopUtil, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, deserialize, conf, isInstanceOf, <init>, ==, toString, logError, !=, get, getClass, logWarning, newConfiguration, ne, serialize, eq, SparkHadoopUtil, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(loginUserFromKeytab, asInstanceOf, synchronized, conf, isInstanceOf, <init>, toString, get, SparkHadoopUtil, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(loginUserFromKeytab, asInstanceOf, initializeLogIfNecessary, conf, isInstanceOf, <init>, ==, toString, !=, get, getClass, newConfiguration, ne, eq, SparkHadoopUtil)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, !=, get, newConfiguration, SparkHadoopUtil, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(conf, isInstanceOf, <init>, ==, isProxyUser, toString, get, getClass, logWarning, SparkHadoopUtil, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, conf, isInstanceOf, <init>, UPDATE_INPUT_METRICS_INTERVAL_RECORDS, ==, getFSBytesReadOnThreadCallback, toString, !=, get, logWarning, SparkHadoopUtil, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, get, addCredentials, SparkHadoopUtil, hashCode, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, logError, !=, get, logWarning, newConfiguration, ne, SparkHadoopUtil, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(asInstanceOf, logName, conf, isInstanceOf, <init>, ==, toString, !=, get, getClass, logWarning, newConfiguration, ne, SparkHadoopUtil, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, deserialize, notifyAll, conf, isInstanceOf, <init>, ==, logError, !=, get, logWarning, newConfiguration, ne, serialize, eq, SparkHadoopUtil, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, conf, isInstanceOf, <init>, ==, clone, toString, logError, !=, get, getClass, logWarning, newConfiguration, ne, eq, SparkHadoopUtil, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, conf, <init>, ==, !=, get, logWarning, newConfiguration, SparkHadoopUtil, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, conf, isInstanceOf, <init>, UPDATE_INPUT_METRICS_INTERVAL_RECORDS, ==, getFSBytesReadOnThreadCallback, toString, !=, get, logWarning, addCredentials, SparkHadoopUtil, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, compressedOutputStream, $asInstanceOf, equals, DEFAULT_COMPRESSION_CODEC, supportsConcatenationOfSerializedStreams, asInstanceOf, compressedInputStream, getShortName, getCodecName, synchronized, ZStdCompressionCodec, $isInstanceOf, notifyAll, isInstanceOf, ALL_COMPRESSION_CODECS, version, <init>, ==, LZFCompressionCodec, clone, CompressionCodec, FALLBACK_COMPRESSION_CODEC, toString, !=, getClass, LZ4CompressionCodec, ne, SnappyCompressionCodec, eq, ##, createCodec, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, getShortName, getCodecName, synchronized, isInstanceOf, <init>, ==, clone, CompressionCodec, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(compressedOutputStream, asInstanceOf, compressedInputStream, isInstanceOf, <init>, CompressionCodec, toString, !=, ne, createCodec)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(compressedOutputStream, asInstanceOf, compressedInputStream, synchronized, isInstanceOf, <init>, ==, CompressionCodec, !=, ne, createCodec)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(compressedOutputStream, asInstanceOf, compressedInputStream, isInstanceOf, <init>, ==, CompressionCodec, createCodec)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala: Set(compressedOutputStream, asInstanceOf, compressedInputStream, isInstanceOf, <init>, ==, CompressionCodec, toString, createCodec)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(compressedOutputStream, asInstanceOf, compressedInputStream, getShortName, isInstanceOf, <init>, ==, CompressionCodec, toString, !=, getClass, ne, createCodec)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, getShortName, synchronized, isInstanceOf, version, <init>, ==, clone, CompressionCodec, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, LocalCheckpointRDD, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(map, iterator, filter, <init>, id, LocalCheckpointRDD, sparkContext, getStorageLevel, partitions)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSet.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, stageAttemptId, priority, wait, $asInstanceOf, equals, stageId, TaskSet, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, id, tasks, properties, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(stageAttemptId, stageId, TaskSet, asInstanceOf, synchronized, isInstanceOf, <init>, id, tasks, properties, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(stageAttemptId, priority, stageId, TaskSet, asInstanceOf, synchronized, isInstanceOf, <init>, id, tasks, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala: Set(TaskSet)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(stageId, TaskSet, asInstanceOf, isInstanceOf, <init>, properties, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(stageAttemptId, wait, stageId, TaskSet, asInstanceOf, synchronized, isInstanceOf, <init>, id, tasks, properties, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, mkdir, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, newClient, isInstanceOf, deleteRecursive, newClient$default$2, ==, clone, SparkCuratorUtil, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala: Set(synchronized, newClient, ==, SparkCuratorUtil, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala: Set(mkdir, newClient, SparkCuratorUtil, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(newClient, deleteRecursive, ==, SparkCuratorUtil, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, readExternal, BlockManagerId, wait, apply$default$4, writeExternal, $asInstanceOf, equals, hostPort, asInstanceOf, host, synchronized, $isInstanceOf, executorId, notifyAll, isInstanceOf, getCachedBlockManagerId, port, apply, topologyInfo, ==, clone, isDriver, blockManagerIdCache, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(BlockManagerId, asInstanceOf, host, synchronized, executorId, isInstanceOf, apply, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(BlockManagerId, writeExternal, asInstanceOf, executorId, isInstanceOf, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala: Set(BlockManagerId, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(BlockManagerId, asInstanceOf, host, synchronized, executorId, isInstanceOf, apply, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(BlockManagerId, asInstanceOf, host, synchronized, executorId, isInstanceOf, port, apply, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala: Set(BlockManagerId, hostPort, asInstanceOf, isInstanceOf, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(BlockManagerId, hostPort, asInstanceOf, host, executorId, isInstanceOf, port, apply, ==, isDriver, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(BlockManagerId, apply, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(BlockManagerId, asInstanceOf, host, synchronized, executorId, isInstanceOf, port, apply, topologyInfo, ==, isDriver, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala: Set(BlockManagerId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(BlockManagerId, equals, hostPort, asInstanceOf, host, executorId, isInstanceOf, apply, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala: Set(BlockManagerId, asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(BlockManagerId, asInstanceOf, executorId, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(BlockManagerId, executorId, apply, ==, isDriver, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala: Set(BlockManagerId, host, apply, topologyInfo, ==, !=, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(BlockManagerId, wait, hostPort, asInstanceOf, host, synchronized, executorId, isInstanceOf, apply, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(BlockManagerId, wait, asInstanceOf, synchronized, executorId, notifyAll, isInstanceOf, apply, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(BlockManagerId, asInstanceOf, isInstanceOf, apply, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(BlockManagerId, hostPort, asInstanceOf, host, synchronized, executorId, isInstanceOf, port, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(BlockManagerId, asInstanceOf, isInstanceOf, apply, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala: Set(readExternal, BlockManagerId, writeExternal, asInstanceOf, isInstanceOf, apply, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(BlockManagerId, wait, hostPort, asInstanceOf, host, synchronized, executorId, notifyAll, isInstanceOf, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala: Set(BlockManagerId, asInstanceOf, isInstanceOf, apply, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(BlockManagerId, asInstanceOf, executorId, isInstanceOf, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, apply, ==, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(BlockManagerId, asInstanceOf, host, executorId, isInstanceOf, port, apply, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newTaskTempFile, notify, wait, $asInstanceOf, setupJob, equals, newTaskTempFileAbsPath, abortJob, abortTask, asInstanceOf, synchronized, $isInstanceOf, instantiate$default$4, obj, notifyAll, instantiate, isInstanceOf, EmptyTaskCommitMessage, commitJob, <init>, commitTask, ==, clone, TaskCommitMessage, deleteWithJob, toString, !=, getClass, setupTask, FileCommitProtocol, ne, eq, onTaskCommit, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(setupJob, abortJob, abortTask, asInstanceOf, instantiate, isInstanceOf, commitJob, <init>, commitTask, ==, TaskCommitMessage, toString, !=, getClass, setupTask, FileCommitProtocol, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(setupJob, abortJob, abortTask, asInstanceOf, instantiate, isInstanceOf, commitJob, <init>, commitTask, ==, TaskCommitMessage, toString, !=, getClass, setupTask, FileCommitProtocol, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala: Set(setupJob, abortJob, abortTask, asInstanceOf, obj, isInstanceOf, commitJob, <init>, commitTask, TaskCommitMessage, toString, setupTask, FileCommitProtocol, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onExecutorUnblacklisted, notify, wait, onJobEnd, onApplicationEnd, equals, onTaskEnd, onStageCompleted, onTaskGettingResult, onJobStart, onOtherEvent, SparkFirehoseListener, onTaskStart, notifyAll, onExecutorRemoved, onExecutorAdded, onEvent, <init>, onBlockUpdated, onBlockManagerRemoved, onExecutorMetricsUpdate, onEnvironmentUpdate, onBlockManagerAdded, onNodeBlacklisted, onApplicationStart, onSpeculativeTaskSubmitted, toString, getClass, onExecutorBlacklisted, onUnpersistRDD, onStageSubmitted, onNodeUnblacklisted, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, BlockStoreShuffleReader, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, <init>, BlockStoreShuffleReader, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorFailuresInTaskSet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorFailuresInTaskSet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorFailuresInTaskSet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorFailuresInTaskSet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorFailuresInTaskSet.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, taskToFailureCountAndFailureTime, getNumTaskFailures, equals, asInstanceOf, synchronized, updateWithFailure, $isInstanceOf, notifyAll, isInstanceOf, <init>, numUniqueTasksWithFailures, ==, clone, ExecutorFailuresInTaskSet, toString, node, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala: Set(getNumTaskFailures, updateWithFailure, <init>, numUniqueTasksWithFailures, ExecutorFailuresInTaskSet, node)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(taskToFailureCountAndFailureTime, asInstanceOf, isInstanceOf, <init>, ==, ExecutorFailuresInTaskSet, toString, node, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, ExecutorFailuresInTaskSet, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, BlockException, wait, copy$default$2, $asInstanceOf, productArity, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, canEqual, productPrefix, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, $init$, copy, message, toString, !=, getClass, copy$default$1, ne, blockId, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(BlockException, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, ne, blockId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	HEADER_MEM_SPILL, notify, hasBytesSpilled, HEADER_DISK_SPILL, taskCount, prevPageSizeFormField, lastStageNameAndDescription, wait, pageLink, HEADER_INPUT_SIZE, $asInstanceOf, COLUMN_TO_INDEX, pageSize, HEADER_SHUFFLE_WRITE_SIZE, pageSizeFormField, StagePage, HEADER_TASK_INDEX, pageNumberFormField, equals, prefix, asInstanceOf, HEADER_LOCALITY, ApiHelper, synchronized, HEADER_HOST, pageData, HEADER_OUTPUT_SIZE, $isInstanceOf, HEADER_EXECUTOR, TaskPagedTable, row, hasShuffleRead, tableCssClass, HEADER_ACCUMULATORS, dataSource, HEADER_LAUNCH_TIME, notifyAll, goButtonFormPath, isInstanceOf, HEADER_SHUFFLE_TOTAL_READS, HEADER_ERROR, <init>, tasks, HEADER_PEAK_MEM, makeTimeline, executorLogs, ==, HEADER_SHUFFLE_REMOTE_READS, sliceData, clone, HEADER_DESER_TIME, hasShuffleWrite, HEADER_ATTEMPT, indexName, HEADER_SHUFFLE_WRITE_TIME, $init$, HEADER_STATUS, renderJson, tableId, toString, HEADER_SHUFFLE_READ_TIME, !=, HEADER_SER_TIME, getClass, headers, pageNavigation, render, hasInput, HEADER_ID, HEADER_GC_TIME, hasOutput, ne, dataSize, eq, totalBytesRead, ##, finalize, HEADER_GETTING_RESULT_TIME, table, HEADER_SCHEDULER_DELAY, HEADER_DURATION, hashCode, hasAccumulators, TaskDataSource.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(lastStageNameAndDescription, pageSize, pageSizeFormField, pageNumberFormField, ApiHelper, dataSource, isInstanceOf, <init>, makeTimeline, ==, toString, !=, ne, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(StagePage, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(hasBytesSpilled, ApiHelper, hasShuffleRead, <init>, executorLogs, hasShuffleWrite, toString, hasInput, hasOutput, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, partitionKeyComparator, wait, $asInstanceOf, insert, WritablePartitionedIterator, equals, asInstanceOf, WritablePartitionedPairCollection, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, nextPartition, partitionedDestructiveSortedIterator, partitionComparator, $init$, toString, !=, getClass, ne, destructiveSortedWritablePartitionedIterator, hasNext, eq, writeNext, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(!=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(insert, WritablePartitionedIterator, asInstanceOf, WritablePartitionedPairCollection, synchronized, isInstanceOf, ==, nextPartition, partitionedDestructiveSortedIterator, toString, !=, ne, destructiveSortedWritablePartitionedIterator, hasNext, eq, writeNext, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(insert, WritablePartitionedIterator, asInstanceOf, WritablePartitionedPairCollection, synchronized, isInstanceOf, ==, nextPartition, partitionedDestructiveSortedIterator, toString, !=, ne, destructiveSortedWritablePartitionedIterator, hasNext, eq, writeNext, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala: Set(partitionKeyComparator, WritablePartitionedPairCollection, partitionComparator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(insert, WritablePartitionedIterator, asInstanceOf, WritablePartitionedPairCollection, synchronized, isInstanceOf, ==, nextPartition, partitionedDestructiveSortedIterator, toString, !=, ne, destructiveSortedWritablePartitionedIterator, hasNext, eq, writeNext, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala: Set(partitionKeyComparator, asInstanceOf, WritablePartitionedPairCollection, ==, partitionComparator, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	name, submissionTime, stageId, currentAttemptId, SparkStageInfo, numActiveTasks, numTasks, numFailedTasks, numCompletedTasks.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(name, submissionTime, stageId, SparkStageInfo, numActiveTasks, numTasks, numFailedTasks)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkStageInfo.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala: Set(stageId, SparkStageInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala: Set(name, submissionTime, stageId, currentAttemptId, SparkStageInfo, numActiveTasks, numTasks, numFailedTasks, numCompletedTasks)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(name, submissionTime, stageId, SparkStageInfo, numActiveTasks, numTasks, numFailedTasks)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	parameterOtherTable, shuffleRead, notify, StagePagedTable, shuffleWriteWithUnit, duration, stage, prevPageSizeFormField, submissionTime, wait, pageLink, $asInstanceOf, pageSize, pageSizeFormField, missingStageRow, schedulingPool, inputRead, pageNumberFormField, equals, parameterStageSortColumn, stageSortColumn, currentTime, inputReadWithUnit, stageId, outputWrite, descriptionOption, parameterStagePrevPageSize, asInstanceOf, parameterStagePage, page, allParameters, synchronized, option, pageData, parameterStageSortDesc, outputWriteWithUnit, $isInstanceOf, shuffleReadWithUnit, row, tableCssClass, dataSource, notifyAll, goButtonFormPath, isInstanceOf, <init>, attemptId, ==, sliceData, toNodeSeq, clone, shuffleWrite, parameterPath, $init$, tableId, stagePage, StageDataSource, formattedSubmissionTime, toString, !=, getClass, headers, pageNavigation, StageTableBase, stagePrevPageSize, stagePageSize, stageSortDesc, ne, dataSize, formattedDuration, StageTableRowData, MissingStageTableRowData, eq, ##, finalize, table, hashCode, parameterStagePageSize.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(<init>, ==, toNodeSeq, StageTableBase)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(<init>, toNodeSeq, StageTableBase)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(stage, submissionTime, stageId, asInstanceOf, isInstanceOf, <init>, attemptId, ==, toNodeSeq, toString, !=, StageTableBase)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, fromStage$default$5, name, fromStage$default$3, submissionTime, wait, $asInstanceOf, StageInfo, failureReason, equals, taskMetrics, stageId, asInstanceOf, synchronized, stageFailed, $isInstanceOf, parentIds, notifyAll, isInstanceOf, completionTime, <init>$default$8, <init>, attemptId, ==, taskLocalityPreferences, clone, numTasks, fromStage$default$4, details, toString, attemptNumber, !=, <init>$default$9, getClass, accumulables, ne, eq, fromStage, ##, finalize, getStatusString, rddInfos, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(name, submissionTime, StageInfo, failureReason, taskMetrics, stageId, asInstanceOf, synchronized, stageFailed, isInstanceOf, <init>, ==, clone, numTasks, toString, attemptNumber, !=, accumulables, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(name, StageInfo, stageId, asInstanceOf, synchronized, isInstanceOf, <init>, ==, taskLocalityPreferences, numTasks, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(name, submissionTime, StageInfo, failureReason, stageId, completionTime, <init>, attemptId, ==, numTasks, details, toString, attemptNumber, !=, accumulables, eq, rddInfos)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(name, StageInfo, asInstanceOf, isInstanceOf, <init>, ==, taskLocalityPreferences, numTasks, details, !=, fromStage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala: Set(StageInfo, stageId, <init>, attemptId, details)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala: Set(name, StageInfo, stageId, asInstanceOf, parentIds, isInstanceOf, <init>, ==, toString, attemptNumber, !=, ne, eq, rddInfos, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala: Set(name, StageInfo, taskMetrics, stageId, asInstanceOf, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(name, submissionTime, StageInfo, failureReason, equals, taskMetrics, stageId, asInstanceOf, isInstanceOf, completionTime, <init>, attemptId, ==, numTasks, details, toString, attemptNumber, !=, getClass, ne, rddInfos)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(StageInfo, taskMetrics, stageId, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(name, submissionTime, StageInfo, failureReason, taskMetrics, stageId, asInstanceOf, isInstanceOf, completionTime, <init>, ==, numTasks, toString, attemptNumber, !=, ne, eq, getStatusString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(name, submissionTime, StageInfo, failureReason, taskMetrics, stageId, asInstanceOf, parentIds, isInstanceOf, completionTime, <init>, attemptId, ==, numTasks, details, toString, attemptNumber, !=, accumulables, ne, rddInfos)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, application, wait, $asInstanceOf, state, equals, ExecutorDesc, cores, asInstanceOf, copyState, synchronized, $isInstanceOf, notifyAll, isInstanceOf, fullId, <init>, id, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode, worker, memory.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(application, state, ExecutorDesc, cores, <init>, id, ==, worker, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala: Set(application, state, ExecutorDesc, cores, fullId, <init>, id, ==, memory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala: Set(state, ExecutorDesc, cores, asInstanceOf, isInstanceOf, <init>, id, ==, !=, worker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(application, state, ExecutorDesc, cores, asInstanceOf, copyState, isInstanceOf, fullId, <init>, id, ==, toString, !=, ne, worker, memory)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorExitCode.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorExitCode.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorExitCode.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorExitCode.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorExitCode.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	EXTERNAL_BLOCK_STORE_FAILED_TO_INITIALIZE, notify, wait, $asInstanceOf, equals, asInstanceOf, explainExitCode, synchronized, $isInstanceOf, DISK_STORE_FAILED_TO_CREATE_DIR, HEARTBEAT_FAILURE, EXTERNAL_BLOCK_STORE_FAILED_TO_CREATE_DIR, notifyAll, isInstanceOf, ==, clone, toString, !=, ExecutorExitCode, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorLossReason.scala: Set(asInstanceOf, explainExitCode, isInstanceOf, ==, toString, ExecutorExitCode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, HEARTBEAT_FAILURE, notifyAll, isInstanceOf, ==, !=, ExecutorExitCode, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(synchronized, DISK_STORE_FAILED_TO_CREATE_DIR, clone, !=, ExecutorExitCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	SparkListenerLogStart, onExecutorUnblacklisted, notify, execId, stageAttemptId, stageInfo, wait, SparkListenerTaskGettingResult, copy$default$2, $asInstanceOf, onJobEnd, onApplicationEnd, copy$default$5, SparkListenerExecutorUnblacklisted, productArity, equals, onTaskEnd, SparkListenerBlockManagerRemoved, taskMetrics, stageId, appName, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, SparkListenerJobEnd, synchronized, $isInstanceOf, sparkVersion, SparkListenerEnvironmentUpdate, logEvent, onOtherEvent, canEqual, copy$default$4, executorId, onTaskStart, productPrefix, reason, driverLogs, notifyAll, onExecutorRemoved, SparkListenerSpeculativeTaskSubmitted, SparkListenerInterface, SparkListener, SparkListenerEvent, SparkListenerUnpersistRDD, SparkListenerTaskEnd, onExecutorAdded, SparkListenerNodeUnblacklisted, isInstanceOf, maxOffHeapMem, SparkListenerApplicationEnd, blockManagerId, <init>, SparkListenerExecutorBlacklisted, SparkListenerApplicationStart, onBlockUpdated, executorFailures, properties, executorInfo, blockUpdatedInfo, jobResult, ==, onBlockManagerRemoved, SparkListenerExecutorMetricsUpdate, taskFailures, appAttemptId, rddId, clone, onExecutorMetricsUpdate, onEnvironmentUpdate, stageIds, SparkListenerTaskStart, hostId, jobId, onBlockManagerAdded, SparkListenerExecutorRemoved, $init$, onNodeBlacklisted, onApplicationStart, accumUpdates, copy$default$3, copy, onSpeculativeTaskSubmitted, taskType, toString, !=, environmentDetails, time, getClass, SparkListenerJobStart, SparkListenerStageCompleted, copy$default$1, SparkListenerNodeBlacklisted, appId, sparkUser, stageInfos, onExecutorBlacklisted, onUnpersistRDD, SparkListenerBlockUpdated, copy$default$6, ne, onStageSubmitted, maxMem, SparkListenerStageSubmitted, SparkListenerExecutorAdded, eq, onNodeUnblacklisted, SparkListenerBlockManagerAdded, productIterator, maxOnHeapMem, taskInfo, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(stageId, appName, asInstanceOf, synchronized, SparkListenerEnvironmentUpdate, executorId, reason, SparkListenerInterface, SparkListenerEvent, SparkListenerUnpersistRDD, isInstanceOf, SparkListenerApplicationEnd, blockManagerId, <init>, SparkListenerApplicationStart, ==, rddId, clone, jobId, toString, !=, environmentDetails, getClass, sparkUser, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, synchronized, SparkListenerInterface, SparkListenerEvent, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(SparkListenerLogStart, appName, asInstanceOf, synchronized, sparkVersion, SparkListenerEnvironmentUpdate, SparkListenerInterface, SparkListener, SparkListenerEvent, isInstanceOf, SparkListenerApplicationEnd, <init>, SparkListenerApplicationStart, ==, appAttemptId, clone, copy, toString, !=, environmentDetails, time, getClass, appId, sparkUser, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(stageId, appName, asInstanceOf, synchronized, SparkListenerEnvironmentUpdate, executorId, reason, SparkListenerInterface, SparkListenerEvent, SparkListenerUnpersistRDD, isInstanceOf, SparkListenerApplicationEnd, blockManagerId, <init>, SparkListenerApplicationStart, ==, rddId, clone, jobId, toString, !=, environmentDetails, getClass, sparkUser, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(stageAttemptId, stageId, asInstanceOf, executorId, isInstanceOf, <init>, ==, rddId, stageIds, jobId, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(execId, stageAttemptId, SparkListenerTaskGettingResult, taskMetrics, stageId, asInstanceOf, SparkListenerJobEnd, synchronized, executorId, reason, SparkListenerSpeculativeTaskSubmitted, SparkListenerEvent, SparkListenerTaskEnd, isInstanceOf, blockManagerId, <init>, properties, ==, SparkListenerExecutorMetricsUpdate, clone, stageIds, SparkListenerTaskStart, jobId, accumUpdates, toString, !=, time, SparkListenerJobStart, SparkListenerStageCompleted, stageInfos, ne, SparkListenerStageSubmitted, taskInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, executorId, reason, SparkListenerEvent, isInstanceOf, <init>, properties, executorInfo, ==, SparkListenerExecutorRemoved, toString, !=, ne, SparkListenerExecutorAdded)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(stageId, appName, asInstanceOf, synchronized, SparkListenerEnvironmentUpdate, executorId, reason, SparkListenerInterface, SparkListenerEvent, SparkListenerUnpersistRDD, isInstanceOf, SparkListenerApplicationEnd, blockManagerId, <init>, SparkListenerApplicationStart, ==, rddId, clone, jobId, toString, !=, environmentDetails, getClass, sparkUser, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala: Set(asInstanceOf, SparkListenerEvent, isInstanceOf, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(stageInfo, stageId, asInstanceOf, synchronized, executorId, reason, onExecutorRemoved, SparkListenerSpeculativeTaskSubmitted, SparkListenerInterface, SparkListener, SparkListenerTaskEnd, onExecutorAdded, isInstanceOf, <init>, ==, SparkListenerTaskStart, SparkListenerExecutorRemoved, !=, SparkListenerStageCompleted, ne, SparkListenerStageSubmitted, SparkListenerExecutorAdded, taskInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(stageAttemptId, SparkListenerExecutorUnblacklisted, stageId, asInstanceOf, executorId, SparkListenerEvent, SparkListenerNodeUnblacklisted, isInstanceOf, <init>, SparkListenerExecutorBlacklisted, ==, toString, SparkListenerNodeBlacklisted, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala: Set(onExecutorUnblacklisted, SparkListenerTaskGettingResult, onJobEnd, onApplicationEnd, SparkListenerExecutorUnblacklisted, onTaskEnd, SparkListenerBlockManagerRemoved, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, SparkListenerJobEnd, SparkListenerEnvironmentUpdate, onOtherEvent, onTaskStart, onExecutorRemoved, SparkListenerSpeculativeTaskSubmitted, SparkListenerInterface, SparkListenerEvent, SparkListenerUnpersistRDD, SparkListenerTaskEnd, onExecutorAdded, SparkListenerNodeUnblacklisted, isInstanceOf, SparkListenerApplicationEnd, SparkListenerExecutorBlacklisted, SparkListenerApplicationStart, onBlockUpdated, onBlockManagerRemoved, SparkListenerExecutorMetricsUpdate, onExecutorMetricsUpdate, onEnvironmentUpdate, SparkListenerTaskStart, onBlockManagerAdded, SparkListenerExecutorRemoved, onNodeBlacklisted, onApplicationStart, onSpeculativeTaskSubmitted, SparkListenerJobStart, SparkListenerStageCompleted, SparkListenerNodeBlacklisted, onExecutorBlacklisted, onUnpersistRDD, SparkListenerBlockUpdated, onStageSubmitted, SparkListenerStageSubmitted, SparkListenerExecutorAdded, onNodeUnblacklisted, SparkListenerBlockManagerAdded)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, reason, SparkListenerEvent, isInstanceOf, <init>, ==, toString, appId, SparkListenerExecutorAdded, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appName, SparkListener, <init>, ==, appId, sparkUser)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(execId, SparkListenerBlockManagerRemoved, asInstanceOf, executorId, SparkListenerEvent, isInstanceOf, maxOffHeapMem, blockManagerId, <init>, ==, rddId, toString, !=, time, SparkListenerBlockUpdated, ne, maxMem, eq, SparkListenerBlockManagerAdded, maxOnHeapMem, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala: Set(SparkListener)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(SparkListenerInterface, SparkListenerEvent, <init>, !=, time, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala: Set(stageInfo, taskMetrics, stageId, asInstanceOf, SparkListenerJobEnd, SparkListenerInterface, SparkListener, SparkListenerTaskEnd, isInstanceOf, <init>, ==, copy, SparkListenerStageCompleted, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(SparkListenerLogStart, execId, stageAttemptId, stageInfo, SparkListenerTaskGettingResult, SparkListenerExecutorUnblacklisted, equals, onTaskEnd, SparkListenerBlockManagerRemoved, taskMetrics, stageId, appName, asInstanceOf, SparkListenerJobEnd, sparkVersion, SparkListenerEnvironmentUpdate, executorId, reason, driverLogs, SparkListener, SparkListenerEvent, SparkListenerUnpersistRDD, SparkListenerTaskEnd, SparkListenerNodeUnblacklisted, isInstanceOf, maxOffHeapMem, SparkListenerApplicationEnd, blockManagerId, <init>, SparkListenerExecutorBlacklisted, SparkListenerApplicationStart, properties, executorInfo, blockUpdatedInfo, jobResult, ==, SparkListenerExecutorMetricsUpdate, appAttemptId, rddId, stageIds, SparkListenerTaskStart, hostId, jobId, SparkListenerExecutorRemoved, accumUpdates, toString, !=, environmentDetails, time, getClass, SparkListenerJobStart, SparkListenerStageCompleted, SparkListenerNodeBlacklisted, appId, sparkUser, stageInfos, SparkListenerBlockUpdated, ne, maxMem, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerBlockManagerAdded, maxOnHeapMem, taskInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkFirehoseListener.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, synchronized, SparkListenerInterface, SparkListenerEvent, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(SparkListenerLogStart, SparkListenerTaskGettingResult, SparkListenerExecutorUnblacklisted, SparkListenerBlockManagerRemoved, asInstanceOf, SparkListenerJobEnd, SparkListenerEnvironmentUpdate, logEvent, SparkListener, SparkListenerEvent, SparkListenerUnpersistRDD, SparkListenerTaskEnd, SparkListenerNodeUnblacklisted, isInstanceOf, SparkListenerApplicationEnd, <init>, SparkListenerExecutorBlacklisted, SparkListenerApplicationStart, ==, SparkListenerExecutorMetricsUpdate, appAttemptId, SparkListenerTaskStart, SparkListenerExecutorRemoved, toString, !=, environmentDetails, getClass, SparkListenerJobStart, SparkListenerStageCompleted, SparkListenerNodeBlacklisted, appId, SparkListenerBlockUpdated, ne, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerBlockManagerAdded)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(stageInfo, taskMetrics, stageId, asInstanceOf, SparkListener, SparkListenerTaskEnd, isInstanceOf, <init>, ==, toString, !=, SparkListenerStageCompleted, ne, eq, taskInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(SparkListenerLogStart, appName, asInstanceOf, synchronized, sparkVersion, SparkListenerEnvironmentUpdate, SparkListenerInterface, SparkListener, SparkListenerEvent, isInstanceOf, SparkListenerApplicationEnd, <init>, SparkListenerApplicationStart, ==, appAttemptId, clone, copy, toString, !=, environmentDetails, time, getClass, appId, sparkUser, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, executorId, SparkListenerInterface, SparkListener, isInstanceOf, blockManagerId, <init>, ==, SparkListenerExecutorRemoved, accumUpdates, toString, !=, ne, SparkListenerExecutorAdded, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(SparkListenerLogStart, execId, stageAttemptId, stageInfo, SparkListenerTaskGettingResult, SparkListenerBlockManagerRemoved, taskMetrics, stageId, appName, asInstanceOf, SparkListenerJobEnd, sparkVersion, SparkListenerEnvironmentUpdate, executorId, reason, driverLogs, SparkListenerEvent, SparkListenerUnpersistRDD, SparkListenerTaskEnd, isInstanceOf, maxOffHeapMem, SparkListenerApplicationEnd, blockManagerId, <init>, SparkListenerApplicationStart, properties, executorInfo, blockUpdatedInfo, jobResult, ==, SparkListenerExecutorMetricsUpdate, appAttemptId, rddId, stageIds, SparkListenerTaskStart, jobId, SparkListenerExecutorRemoved, accumUpdates, taskType, toString, !=, environmentDetails, time, SparkListenerJobStart, SparkListenerStageCompleted, appId, sparkUser, stageInfos, SparkListenerBlockUpdated, ne, maxMem, SparkListenerStageSubmitted, SparkListenerExecutorAdded, SparkListenerBlockManagerAdded, maxOnHeapMem, taskInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(SparkListenerLogStart, appName, asInstanceOf, synchronized, sparkVersion, SparkListenerEnvironmentUpdate, SparkListenerInterface, SparkListener, SparkListenerEvent, isInstanceOf, SparkListenerApplicationEnd, <init>, SparkListenerApplicationStart, ==, appAttemptId, clone, copy, toString, !=, environmentDetails, time, getClass, appId, sparkUser, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(stageId, appName, asInstanceOf, synchronized, SparkListenerEnvironmentUpdate, executorId, reason, SparkListenerInterface, SparkListenerEvent, SparkListenerUnpersistRDD, isInstanceOf, SparkListenerApplicationEnd, blockManagerId, <init>, SparkListenerApplicationStart, ==, rddId, clone, jobId, toString, !=, environmentDetails, getClass, sparkUser, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, executorId, reason, notifyAll, isInstanceOf, blockManagerId, <init>, properties, ==, accumUpdates, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(stageId, appName, asInstanceOf, synchronized, SparkListenerEnvironmentUpdate, executorId, reason, SparkListenerInterface, SparkListenerEvent, SparkListenerUnpersistRDD, isInstanceOf, SparkListenerApplicationEnd, blockManagerId, <init>, SparkListenerApplicationStart, ==, rddId, clone, jobId, toString, !=, environmentDetails, getClass, sparkUser, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryPool.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, memoryUsed, equals, memoryFree, asInstanceOf, synchronized, poolSize, $isInstanceOf, notifyAll, isInstanceOf, decrementPoolSize, <init>, incrementPoolSize, ==, clone, MemoryPool, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(memoryUsed, memoryFree, synchronized, poolSize, decrementPoolSize, <init>, incrementPoolSize, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala: Set(wait, memoryFree, synchronized, poolSize, notifyAll, <init>, ==, MemoryPool)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(memoryFree, synchronized, poolSize, decrementPoolSize, <init>, incrementPoolSize, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala: Set(memoryUsed, memoryFree, synchronized, poolSize, <init>, ==, MemoryPool)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(memoryUsed, synchronized, poolSize, <init>, incrementPoolSize, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(memoryUsed, memoryFree, synchronized, poolSize, decrementPoolSize, <init>, incrementPoolSize, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(memoryUsed, synchronized, poolSize, <init>, incrementPoolSize, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(memoryFree, synchronized, poolSize, decrementPoolSize, <init>, incrementPoolSize, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(memoryUsed, memoryFree, synchronized, poolSize, decrementPoolSize, <init>, incrementPoolSize, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(memoryUsed, synchronized, poolSize, <init>, incrementPoolSize, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(memoryFree, synchronized, poolSize, decrementPoolSize, <init>, incrementPoolSize, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/Optional.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/Optional.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/Optional.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/Optional.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/Optional.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, empty, orNull, equals, Optional, isPresent, absent, or, orElse, notifyAll, ofNullable, toString, get, getClass, fromNullable, of, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(Optional)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/Optional.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(Optional)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(Optional)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaUtils.scala: Set(empty, Optional, get, of, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	executeCommand$default$3, executeAndGetOutput$default$3, copyFileStreamNIO, executeAndGetOutput$default$2, processStreamByLine, truncatedString, setupSecureURLConnection, notify, doFetchFile, initDaemon, unapply, localCanonicalHostName, tryWithSafeFinally, windowsDrive, localHostNameForURI, getOrCreateLocalRootDirs, getDynamicAllocationInitialExecutors, random, portMaxRetries, getLocalUserJarsForShell, getCallSite$default$1, timeIt, DEFAULT_DRIVER_MEM_MB, truncatedString$default$5, resume, tryWithSafeFinallyAndFailureCallbacks$default$2, wait, isAlive, tryWithSafeFinallyAndFailureCallbacks, <init>$default$5, copyStream$default$4, threadLocalRandomSeed, callerContextSupported, RedirectThread, classIsLoadable, copy$default$2, updateSparkConfigFromProperties, $asInstanceOf, empty, setName, <init>$default$6, randomizeInPlace$default$2, isBindCollision, loadDefaultSparkProperties$default$2, writeByteBuffer, resolveURI, getContextClassLoader, join, isLocalMaster, setCurrentContext, getDefaultPropertiesFile, CircularBuffer, productArity, localHostName, equals, tryOrIOException, getThreadDump, nonLocalPaths$default$2, setPriority, deserializeViaNestedStream, createTempDir$default$1, isMac, nanSafeCompareFloats, startServiceOnPort$default$4, parkBlocker, responseFromBackup, unionFileLists, threadLocalRandomProbe, fetchHcfsFile, tryWithResource, asInstanceOf, initializeLogIfNecessary, isDynamicAllocationEnabled, stringToSeq, executeAndGetOutput$default$4, isSymlink, getContextOrSparkClassLoader, checkHostPort, exceptionString, run, getPriority, msDurationToString, memoryStringToMb, timeStringAsSeconds, getThreadDumpForThread, isFatalError, synchronized, executeCommand$default$2, getUsedTimeMs, isWindows, nonNegativeMod, isTesting, blockedOn, <init>$default$7, $isInstanceOf, configTestLog4j, getStderr, <init>$default$4, copyStream$default$3, checkAccess, tryWithSafeFinallyAndFailureCallbacks$default$3, checkAndGetK8sMasterUrl, getIteratorZipWithIndex, tempFileWith, EMPTY_USER_GROUPS, suspend, logTrace, createSecret, DEFAULT_MAX_TO_STRING_FIELDS, canEqual, resolveURIs, createTempDir$default$2, isTraceEnabled, deserialize, REDACTION_REPLACEMENT_TEXT, initializeLogIfNecessary$default$2, getUserJars, getSystemProperties, loadDefaultSparkProperties, classForName, getUncaughtExceptionHandler, productPrefix, getThreadGroup, recursiveList, fetchFile, stop, longForm, logName, notifyAll, clearLocalRootDirs, getCurrentUserGroups, getName, getFileLength, getFilePath, doesDirectoryContainAnyNewFiles, isInterrupted, isInstanceOf, userPort, megabytesToString, getState, getSparkClassLoader, getStackTrace, shortForm, sparkJavaOpts$default$2, nanSafeCompareDoubles, executeCommand$default$4, getHadoopFileSystem, <init>$default$3, terminateProcess, CallerContext, <init>$default$8, byteStringAsGb, byteStringAsMb, constructURIForAuthentication, <init>, checkHost, bytesToString, destroy, validateURL, decodeFileNameInURI, extractHostPortFromSparkUrl, apply, loadExtensions, sparkJavaOpts, getMaxResultSize, invoke, getProcessName, nonNegativeHash, getLocalDir, ==, SHORT_FORM, randomizeInPlace, clone, CallSite, setDaemon, deserializeLongValue, createTempDir, BACKUP_STANDALONE_MASTER_PREFIX, withDummyCallSite, inheritableThreadLocals, times, isRunningInYarnContainer, $init$, serializeViaNestedStream, setContextClassLoader, getIteratorSize, tryLogNonFatalError, copy, threadLocals, flush, toString, createDirectory, Utils, redact, parseStandaloneMasterUrls, isDaemon, parseHostPort, executeAndGetOutput, logError, !=, <init>$default$9, getClass, logWarning, copy$default$1, setUncaughtExceptionHandler, countStackFrames, start, close, getSimpleName, fetchHcfsFile$default$7, symlink, createDirectory$default$2, LONG_FORM, stripDirectory, startServiceOnPort, nonLocalPaths, ne, isInDirectory, serialize, libraryPathEnvName, getConfiguredLocalDirs, setCustomHostname, tryOrExit, getId, setLogLevel, copyStream, timeIt$default$3, logUncaughtExceptions, emptyJson, getCurrentUserName, threadLocalRandomSecondarySeed, getCallSite, chmod700, timeStringAsMs, offsetBytes, getSparkOrYarnConfig, deleteRecursively, getPropertiesFromFile, <init>$default$2, eq, interrupt, productIterator, write, encodeFileNameToURIRawPath, byteStringAsKb, log, executeCommand, splitCommandString, tryLog, getFormattedClassName, ##, tryOrStopSparkContext, jsonOption, finalize, byteStringAsBytes, productElement, hashCode, libraryPathEnvPrefix, randomize, getDefaultPropertiesFile$default$1, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala: Set(RedirectThread, setName, join, isWindows, createSecret, getName, <init>, apply, ==, setDaemon, createTempDir, copy, Utils, !=, start, nonLocalPaths, logUncaughtExceptions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(megabytesToString, <init>, apply, ==, toString, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf, <init>, CallSite)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(unapply, asInstanceOf, exceptionString, synchronized, logTrace, stop, isInstanceOf, getStackTrace, shortForm, <init>, apply, ==, clone, CallSite, toString, Utils, logError, !=, logWarning, start, ne, serialize, getFormattedClassName, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(empty, asInstanceOf, synchronized, stop, isInstanceOf, <init>, apply, ==, tryLogNonFatalError, toString, Utils, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(tryWithSafeFinally, <init>, ==, flush, Utils, logError, !=, start, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(tryOrIOException, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, Utils, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala: Set(exceptionString, <init>, apply, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(unapply, empty, asInstanceOf, synchronized, getName, isInstanceOf, <init>, bytesToString, apply, getMaxResultSize, ==, toString, Utils, logError, !=, logWarning, ne, serialize, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(empty, asInstanceOf, isDynamicAllocationEnabled, stop, isInstanceOf, megabytesToString, <init>, apply, sparkJavaOpts, ==, toString, Utils, logError, !=, logWarning, start, close, splitCommandString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(tryWithSafeFinallyAndFailureCallbacks, asInstanceOf, getName, isInstanceOf, <init>, apply, ==, toString, Utils, logError, !=, getClass, logWarning, close, getSimpleName, ne, write, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(empty, asInstanceOf, stop, isInstanceOf, getStackTrace, <init>, apply, ==, setDaemon, Utils, start, startServiceOnPort, ne, write, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala: Set(empty, getContextClassLoader, asInstanceOf, getName, isInstanceOf, <init>, apply, ==, clone, toString, Utils, !=, getClass, logWarning, ne, serialize, copyStream, eq, log, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, classForName, stop, <init>, apply, ==, Utils, logError, !=, logWarning, start, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(RedirectThread, synchronized, getStderr, getStackTrace, <init>, destroy, apply, setDaemon, flush, toString, Utils, logError, !=, logWarning, start, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(initDaemon, unapply, asInstanceOf, run, classForName, stop, isInstanceOf, <init>, checkHost, apply, invoke, ==, Utils, logError, !=, logWarning, start, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala: Set(unapply, empty, asInstanceOf, synchronized, deserialize, getName, isInstanceOf, <init>, apply, ==, toString, Utils, !=, logWarning, ne, getId, getCurrentUserName, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala: Set(equals, run, synchronized, <init>, apply, Utils, logError, !=, logUncaughtExceptions, deleteRecursively, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(isAlive, empty, megabytesToString, <init>, apply, ==, toString, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(unapply, tryWithSafeFinally, responseFromBackup, asInstanceOf, run, isInstanceOf, <init>, apply, ==, Utils, parseStandaloneMasterUrls, logError, !=, logWarning, close, ne, write, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(unapply, setName, resolveURI, isLocalMaster, getThreadDump, asInstanceOf, isDynamicAllocationEnabled, getContextOrSparkClassLoader, memoryStringToMb, synchronized, deserialize, getUserJars, fetchFile, stop, longForm, getName, isInstanceOf, shortForm, <init>, validateURL, apply, loadExtensions, ==, SHORT_FORM, clone, CallSite, setDaemon, tryLogNonFatalError, toString, Utils, logError, !=, getClass, logWarning, start, close, LONG_FORM, nonLocalPaths, ne, setLogLevel, getCurrentUserName, getCallSite, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala: Set(tryOrIOException, asInstanceOf, <init>, toString, Utils, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(localHostNameForURI, localHostName, <init>, apply, clone, Utils, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(tryOrIOException, deserializeViaNestedStream, asInstanceOf, isInstanceOf, <init>, apply, ==, serializeViaNestedStream, Utils, start, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(unapply, tryWithSafeFinally, asInstanceOf, getName, isInstanceOf, <init>, apply, toString, Utils, !=, logWarning, close, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala: Set(unapply, getSparkClassLoader, <init>, ==, Utils, logError, !=, logWarning, close, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala: Set(<init>, Utils, logWarning, getPropertiesFromFile, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(initDaemon, unapply, asInstanceOf, synchronized, classForName, stop, getName, isInstanceOf, <init>, apply, toString, Utils, write, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(empty, asInstanceOf, logTrace, isInterrupted, isInstanceOf, <init>, apply, ==, setDaemon, flush, Utils, logError, !=, logWarning, start, close, ne, logUncaughtExceptions, interrupt, write, tryLog, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala: Set(<init>, apply, Utils, executeAndGetOutput, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(classIsLoadable, empty, setName, resolveURI, getContextClassLoader, asInstanceOf, initializeLogIfNecessary, stringToSeq, isTesting, checkAndGetK8sMasterUrl, resolveURIs, classForName, getName, isInstanceOf, getStackTrace, <init>, apply, ==, createTempDir, setContextClassLoader, toString, Utils, redact, !=, getClass, start, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala: Set(asInstanceOf, isInstanceOf, getSparkClassLoader, <init>, apply, ==, toString, Utils, logError, !=, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(RedirectThread, run, getName, <init>, apply, ==, toString, !=, start, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(tryOrIOException, asInstanceOf, <init>, apply, ==, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala: Set(tryOrIOException, asInstanceOf, <init>, apply, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala: Set(tryWithSafeFinally, getName, <init>, Utils, !=, logWarning, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala: Set(tryWithSafeFinally, RedirectThread, empty, resolveURI, getName, <init>, apply, ==, createTempDir, copy, toString, Utils, logError, !=, logWarning, start, close, copyStream, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(tryWithResource, asInstanceOf, getName, isInstanceOf, <init>, bytesToString, apply, ==, toString, Utils, !=, logWarning, close, write, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(getContextClassLoader, localHostName, asInstanceOf, classForName, isInstanceOf, <init>, apply, invoke, ==, setContextClassLoader, Utils, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala: Set(empty, writeByteBuffer, tryOrIOException, asInstanceOf, deserialize, isInstanceOf, <init>, ==, toString, Utils, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(unapply, getDynamicAllocationInitialExecutors, empty, asInstanceOf, synchronized, isTesting, getName, isInstanceOf, <init>, apply, ==, Utils, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, Utils, logError, logWarning, ne, timeStringAsMs, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala: Set(getDynamicAllocationInitialExecutors, isDynamicAllocationEnabled, <init>, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala: Set(random, setName, asInstanceOf, <init>, apply, toString, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(initDaemon, unapply, getOrCreateLocalRootDirs, asInstanceOf, synchronized, stop, getName, doesDirectoryContainAnyNewFiles, isInstanceOf, megabytesToString, <init>, checkHost, apply, ==, tryLogNonFatalError, toString, createDirectory, Utils, logError, !=, logWarning, start, ne, tryOrExit, chmod700, deleteRecursively, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, getName, isInstanceOf, <init>, destroy, apply, toString, Utils, logError, !=, start, close, symlink, ne, deleteRecursively, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala: Set(random, timeIt, asInstanceOf, <init>, apply, Utils, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala: Set(asInstanceOf, isInstanceOf, <init>, extractHostPortFromSparkUrl, ==, toString, Utils, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(empty, exceptionString, isInstanceOf, <init>, apply, ==, toString, Utils, !=, start, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(run, stop, <init>, apply, ==, createTempDir, flush, toString, Utils, logError, !=, logWarning, ne, write, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala: Set(empty, asInstanceOf, exceptionString, isInstanceOf, <init>, bytesToString, apply, ==, Utils, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(empty, asInstanceOf, synchronized, getSystemProperties, classForName, stop, getName, isInstanceOf, <init>, apply, getLocalDir, ==, createTempDir, toString, Utils, !=, getClass, logWarning, start, ne, deleteRecursively, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(tryWithSafeFinally, synchronized, tempFileWith, <init>, apply, ==, Utils, logError, !=, logWarning, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(tryWithSafeFinally, tryOrIOException, asInstanceOf, synchronized, getUsedTimeMs, isInstanceOf, <init>, apply, ==, Utils, !=, close, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(asInstanceOf, checkHostPort, isInstanceOf, <init>, checkHost, ==, toString, Utils, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala: Set(asInstanceOf, <init>, apply, CallSite)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala: Set(createSecret, getCurrentUserGroups, <init>, apply, ==, toString, Utils, !=, logWarning, close, ne, getCurrentUserName, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala: Set(tryOrIOException, asInstanceOf, deserialize, isInstanceOf, <init>, apply, ==, Utils, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala: Set(<init>, ==, Utils, close, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala: Set(tryOrIOException, asInstanceOf, isInstanceOf, <init>, checkHost, apply, ==, Utils, !=, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(empty, asInstanceOf, longForm, isInstanceOf, shortForm, <init>, ==, CallSite, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(random, empty, asInstanceOf, synchronized, getIteratorZipWithIndex, isInstanceOf, shortForm, <init>, bytesToString, apply, ==, randomizeInPlace, clone, CallSite, getIteratorSize, toString, Utils, !=, getClass, logWarning, getSimpleName, ne, serialize, getCallSite, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala: Set(memoryStringToMb, <init>, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(stop, <init>, apply, ==, Utils, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(empty, asInstanceOf, classForName, stop, getName, isInstanceOf, <init>, bytesToString, apply, ==, toString, Utils, logError, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(tempFileWith, stop, <init>, apply, Utils, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(unapply, isInstanceOf, <init>, apply, ==, tryLogNonFatalError, toString, Utils, getClass, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(unapply, tryWithSafeFinallyAndFailureCallbacks, asInstanceOf, synchronized, deserialize, stop, isInstanceOf, <init>, apply, ==, clone, toString, Utils, logError, !=, logWarning, close, startServiceOnPort, ne, serialize, eq, write, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isWindows, isInstanceOf, <init>, apply, ==, setDaemon, flush, toString, Utils, logError, !=, start, close, write, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala: Set(tryOrIOException, <init>, ==, Utils, !=, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(equals, tryOrIOException, asInstanceOf, <init>, apply, ==, Utils, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(unapply, empty, setName, join, asInstanceOf, synchronized, getUsedTimeMs, logTrace, classForName, stop, getName, isInstanceOf, <init>, apply, ==, setDaemon, copy, toString, Utils, logError, !=, getClass, logWarning, start, close, ne, getSparkOrYarnConfig, interrupt, write, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(tryOrIOException, asInstanceOf, synchronized, getName, isInstanceOf, <init>, apply, ==, copy, toString, Utils, !=, getClass, logWarning, getSimpleName, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(<init>, apply, getIteratorSize, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(join, logTrace, stop, <init>, setDaemon, Utils, logError, !=, getClass, logWarning, start, tryOrStopSparkContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala: Set(asInstanceOf, classForName, getName, isInstanceOf, <init>, apply, ==, flush, Utils, close, ne, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala: Set(empty, asInstanceOf, exceptionString, isInstanceOf, <init>, apply, ==, toString, Utils, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala: Set(asInstanceOf, classForName, <init>, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, getName, <init>, bytesToString, apply, ==, flush, Utils, !=, getClass, logWarning, start, close, ne, write, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala: Set(unapply, asInstanceOf, getContextOrSparkClassLoader, <init>, invoke, ==, toString, Utils, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(setName, asInstanceOf, stop, isInstanceOf, userPort, getSparkClassLoader, <init>, apply, ==, setDaemon, toString, Utils, !=, logWarning, start, startServiceOnPort, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(empty, logName, <init>, bytesToString, apply, toString, Utils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala: Set(empty, asInstanceOf, run, getName, isInstanceOf, <init>, apply, ==, createTempDir, copy, Utils, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(setName, tryWithResource, asInstanceOf, getName, isInstanceOf, <init>, apply, ==, toString, Utils, !=, close, ne, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(stop, terminateProcess, <init>, apply, ==, toString, Utils, logError, !=, logWarning, start, interrupt, write, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, toString, Utils, getClass, close, eq, write, tryLog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala: Set(unapply, DEFAULT_DRIVER_MEM_MB, localHostName, asInstanceOf, memoryStringToMb, loadDefaultSparkProperties, isInstanceOf, <init>, checkHost, invoke, ==, Utils, parseStandaloneMasterUrls, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(stop, <init>, apply, ==, Utils, logError, getFormattedClassName, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(random, setName, <init>, apply, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(random, setName, join, asInstanceOf, <init>, apply, Utils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala: Set(writeByteBuffer, <init>, Utils, close, ne, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(<init>, bytesToString, apply, ==, toString, Utils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala: Set(createSecret, <init>, ==, flush, Utils, !=, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, CallSite, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(asInstanceOf, exceptionString, classForName, getName, isInstanceOf, <init>, apply, invoke, ==, flush, Utils, logError, !=, getClass, logWarning, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(<init>, apply, ==, CallSite, getCallSite)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala: Set(localCanonicalHostName, <init>, apply, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(logTrace, getName, <init>, apply, Utils, logError, !=, getClass, start, close, startServiceOnPort, serialize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, <init>, ==, CallSite, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala: Set(DEFAULT_DRIVER_MEM_MB, resolveURI, getDefaultPropertiesFile, tryWithResource, isTesting, resolveURIs, classForName, isInstanceOf, <init>, apply, invoke, ==, flush, toString, Utils, redact, !=, stripDirectory, ne, getPropertiesFromFile, byteStringAsBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala: Set(join, getName, isInstanceOf, <init>, tryLogNonFatalError, Utils, !=, getClass, start, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(random, join, asInstanceOf, deserialize, isInstanceOf, <init>, apply, ==, toString, Utils, !=, logWarning, ne, serialize, write, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala: Set(unapply, localHostName, asInstanceOf, loadDefaultSparkProperties, isInstanceOf, <init>, checkHost, ==, Utils, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(empty, asInstanceOf, exceptionString, logName, isInstanceOf, <init>, bytesToString, apply, ==, toString, Utils, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala: Set(tryWithSafeFinally, asInstanceOf, isInstanceOf, <init>, ==, flush, toString, Utils, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala: Set(tryOrIOException, <init>, checkHost, ==, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala: Set(<init>, apply, Utils, redact)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala: Set(<init>, checkHost, apply, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala: Set(shortForm, <init>, destroy, CallSite, toString, Utils, getCallSite, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, bytesToString, apply, ==, flush, toString, Utils, !=, logWarning, start, close, ne, eq, write, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala: Set(initializeLogIfNecessary, createSecret, <init>, apply, ==, Utils, logError, !=, start, close, write, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala: Set(empty, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, Utils, executeAndGetOutput, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(random, empty, tryOrIOException, deserializeViaNestedStream, asInstanceOf, nonNegativeMod, isInstanceOf, <init>, apply, ==, serializeViaNestedStream, Utils, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(tryWithSafeFinally, wait, tryOrIOException, asInstanceOf, synchronized, logTrace, classForName, isInstanceOf, <init>, checkHost, apply, getLocalDir, ==, setDaemon, deserializeLongValue, flush, toString, Utils, !=, getClass, logWarning, start, close, ne, copyStream, eq, write, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(initDaemon, asInstanceOf, classForName, stop, isInstanceOf, megabytesToString, <init>, checkHost, apply, ==, BACKUP_STANDALONE_MASTER_PREFIX, tryLogNonFatalError, toString, Utils, logError, !=, logWarning, start, close, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(empty, asInstanceOf, isInstanceOf, <init>, bytesToString, apply, ==, toString, Utils, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, stop, isInstanceOf, <init>, apply, ==, toString, Utils, logError, !=, logWarning, start, ne, tryOrStopSparkContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(asInstanceOf, logName, getName, isInstanceOf, getHadoopFileSystem, <init>, ==, flush, toString, Utils, redact, !=, getClass, logWarning, close, ne, write, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(unapply, random, wait, empty, setName, getContextClassLoader, asInstanceOf, getContextOrSparkClassLoader, run, getThreadDumpForThread, isFatalError, synchronized, deserialize, classForName, fetchFile, stop, notifyAll, getName, isInstanceOf, <init>, checkHost, bytesToString, apply, getMaxResultSize, ==, setDaemon, setContextClassLoader, Utils, parseHostPort, logError, !=, logWarning, ne, serialize, setCustomHostname, getId, logUncaughtExceptions, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala: Set(getName, <init>, apply, ==, Utils, !=, encodeFileNameToURIRawPath)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala: Set(asInstanceOf, loadDefaultSparkProperties, isInstanceOf, <init>, ==, Utils, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(random, asInstanceOf, <init>, apply, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala: Set(asInstanceOf, nonNegativeMod, isInstanceOf, <init>, ==, Utils, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, getUsedTimeMs, logTrace, isInstanceOf, <init>, bytesToString, apply, ==, toString, Utils, logError, !=, logWarning, close, ne, copyStream, eq, randomize, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(empty, setCurrentContext, synchronized, deserialize, notifyAll, CallerContext, <init>, apply, tryLogNonFatalError, Utils, !=, serialize, interrupt)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala: Set(equals, asInstanceOf, initializeLogIfNecessary, synchronized, isTraceEnabled, classForName, logName, getName, isInstanceOf, getSparkClassLoader, <init>, apply, invoke, ==, Utils, !=, getClass, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(asInstanceOf, isInstanceOf, <init>, bytesToString, apply, ==, toString, Utils, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala: Set(getContextClassLoader, tryOrIOException, asInstanceOf, getName, isInstanceOf, <init>, apply, flush, Utils, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(isAlive, setName, join, tryWithResource, asInstanceOf, getContextOrSparkClassLoader, synchronized, isTesting, getName, isInstanceOf, <init>, apply, ==, clone, setDaemon, tryLogNonFatalError, copy, toString, Utils, logError, !=, getClass, logWarning, setUncaughtExceptionHandler, start, close, getSimpleName, ne, tryOrExit, deleteRecursively, eq, interrupt, write, log, tryLog, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala: Set(tryOrIOException, <init>, Utils, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(initDaemon, loadDefaultSparkProperties, stop, <init>, apply, ==, Utils, !=, start, close, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(setName, join, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, setDaemon, toString, Utils, logError, start, eq, interrupt, tryOrStopSparkContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala: Set(empty, asInstanceOf, getIteratorZipWithIndex, <init>, apply, ==, getIteratorSize, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, tryOrIOException, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, Utils, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala: Set(empty, <init>, apply, Utils, start, ne, libraryPathEnvName, copyStream, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala: Set(unapply, DEFAULT_DRIVER_MEM_MB, asInstanceOf, isInstanceOf, <init>, apply, ==, Utils, parseStandaloneMasterUrls, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(empty, asInstanceOf, memoryStringToMb, isInstanceOf, <init>, apply, sparkJavaOpts, toString, Utils, splitCommandString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, getFileLength, isInstanceOf, <init>, apply, ==, toString, Utils, logError, ne, isInDirectory, offsetBytes, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(getThreadDump, asInstanceOf, isInstanceOf, <init>, apply, ==, Utils, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala: Set(unapply, asInstanceOf, stop, isInstanceOf, <init>, ==, Utils, logError, !=, getClass, eq, getFormattedClassName)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala: Set(memoryStringToMb, <init>, apply, Utils, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(random, asInstanceOf, <init>, apply, clone, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(tryWithResource, asInstanceOf, synchronized, isInstanceOf, <init>, bytesToString, ==, toString, Utils, getClass, logWarning, eq, write, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala: Set(tryOrIOException, asInstanceOf, isInstanceOf, <init>, apply, ==, Utils, !=, write, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala: Set(tryOrIOException, <init>, Utils, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(asInstanceOf, initializeLogIfNecessary, run, <init>, apply, ==, setDaemon, Utils, logError, !=, start, close, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(unapply, getContextClassLoader, asInstanceOf, getContextOrSparkClassLoader, run, deserialize, isInstanceOf, <init>, apply, ==, Utils, logError, !=, ne, logUncaughtExceptions, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(unapply, tryWithSafeFinally, wait, empty, asInstanceOf, run, synchronized, stop, notifyAll, isInstanceOf, <init>, destroy, apply, ==, tryLogNonFatalError, toString, Utils, logError, !=, start, close, ne, eq, write, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(empty, asInstanceOf, synchronized, isInstanceOf, <init>, bytesToString, apply, ==, copy, flush, toString, Utils, !=, logWarning, close, eq, write, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(tryOrIOException, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, Utils, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala: Set(empty, asInstanceOf, exceptionString, getName, isInstanceOf, getStackTrace, <init>, apply, ==, toString, Utils, getClass, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala: Set(shortForm, <init>, bytesToString, apply, CallSite, toString, Utils, getFormattedClassName)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, timeStringAsSeconds, getSystemProperties, getName, isInstanceOf, byteStringAsGb, byteStringAsMb, <init>, apply, ==, toString, Utils, !=, logWarning, ne, timeStringAsMs, eq, byteStringAsKb, byteStringAsBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala: Set(asInstanceOf, timeStringAsSeconds, isInstanceOf, <init>, apply, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, logTrace, isInstanceOf, <init>, apply, ==, tryLogNonFatalError, toString, Utils, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(tryOrIOException, asInstanceOf, <init>, apply, Utils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(doFetchFile, resolveURI, asInstanceOf, stringToSeq, isTesting, getName, isInstanceOf, <init>, apply, ==, createTempDir, toString, Utils, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(unapply, tryWithSafeFinally, join, isInstanceOf, <init>, ==, setDaemon, flush, Utils, logError, !=, logWarning, start, close, ne, logUncaughtExceptions, write, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(<init>, bytesToString, ==, Utils, !=, getId, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, fetchFile, terminateProcess, <init>, apply, ==, Utils, !=, logWarning, start, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(empty, synchronized, getName, <init>, apply, nonNegativeHash, clone, createDirectory, Utils, logError, !=, getConfiguredLocalDirs, deleteRecursively, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala: Set(asInstanceOf, classForName, getName, isInstanceOf, <init>, apply, ==, Utils, getClass, ne, getFormattedClassName)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala: Set(tryOrIOException, asInstanceOf, isInstanceOf, <init>, apply, ==, Utils, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala: Set(asInstanceOf, classForName, getName, isInstanceOf, <init>, apply, ==, clone, copy, toString, Utils, logError, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(empty, localHostName, responseFromBackup, asInstanceOf, isInstanceOf, <init>, apply, sparkJavaOpts, ==, Utils, logError, logWarning, start, ne, splitCommandString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(unapply, getContextClassLoader, asInstanceOf, isTesting, deserialize, classForName, isInstanceOf, <init>, apply, invoke, ==, setContextClassLoader, flush, Utils, logError, !=, getClass, close, ne, serialize, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(unapply, empty, asInstanceOf, classForName, isInstanceOf, getStackTrace, <init>, apply, ==, toString, Utils, !=, ne, emptyJson, getFormattedClassName, jsonOption)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(megabytesToString, <init>, apply, ==, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala: Set(tryOrIOException, <init>, Utils)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, makeDriverRef, asInstanceOf, synchronized, numRetries, $isInstanceOf, maxMessageSizeBytes, askRpcTimeout, notifyAll, RpcUtils, isInstanceOf, ==, clone, toString, lookupRpcTimeout, !=, getClass, retryWaitMs, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, maxMessageSizeBytes, askRpcTimeout, RpcUtils, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, askRpcTimeout, RpcUtils, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(askRpcTimeout, RpcUtils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(asInstanceOf, RpcUtils, isInstanceOf, ==, toString, lookupRpcTimeout, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(makeDriverRef, asInstanceOf, synchronized, RpcUtils, isInstanceOf, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala: Set(numRetries, askRpcTimeout, RpcUtils, retryWaitMs)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(askRpcTimeout, RpcUtils, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, makeDriverRef, asInstanceOf, synchronized, maxMessageSizeBytes, notifyAll, RpcUtils, isInstanceOf, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, askRpcTimeout, RpcUtils, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, maxMessageSizeBytes, notifyAll, RpcUtils, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	StratifiedSamplingUtils, notify, wait, $asInstanceOf, numAccepted, getSeqOp, equals, getAcceptanceResults, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, acceptBound, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, AcceptanceResult, notifyAll, isInstanceOf, <init>, merge, waitList, ==, clone, $init$, getBernoulliSamplingFunction, computeThresholdByKey, toString, logError, !=, getClass, logWarning, numItems, areBoundsEmpty, waitListBound, ne, getPoissonSamplingFunction, eq, getCombOp, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(StratifiedSamplingUtils, asInstanceOf, isInstanceOf, <init>, ==, getBernoulliSamplingFunction, toString, !=, logWarning, ne, getPoissonSamplingFunction, log)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, RpcEndpointNotFoundException, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, RpcEndpointNotFoundException, isInstanceOf, <init>, getMessage, ==, clone, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	vClassTag, kClassTag.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getNewLastScanTime, onExecutorUnblacklisted, notify, logDir, FsHistoryProviderMetadata, viewAcls, lastProcessed, wait, attempts, copy$default$2, $asInstanceOf, onJobEnd, onApplicationEnd, getListing, copy$default$5, checkForLogs, onUIDetached, initThread, adminAclsGroups, productArity, equals, onTaskEnd, isFsInSafeMode, oldestAttempt, getEmptyListingHtml, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, initializeLogIfNecessary, FsHistoryProvider, synchronized, $isInstanceOf, ApplicationInfoWrapper, logTrace, onOtherEvent, canEqual, listing, copy$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, onTaskStart, startSafeModeCheckThread, productPrefix, stop, info, logName, notifyAll, onExecutorRemoved, initialize, onExecutorAdded, isInstanceOf, CURRENT_LISTING_VERSION, uiVersion, endTime, toApplicationInfo, version, getLastUpdatedTime, <init>, id, attemptId, onBlockUpdated, getApplicationInfo, getEventLogsUnderProcess, cleanLogs, getAttempt, adminAcls, ==, fileSize, onBlockManagerRemoved, clone, onExecutorMetricsUpdate, onEnvironmentUpdate, writeEventLogs, onBlockManagerAdded, $init$, onNodeBlacklisted, onApplicationStart, copy$default$3, copy, LogInfo, onSpeculativeTaskSubmitted, toString, AttemptInfoWrapper, logError, !=, getClass, logWarning, copy$default$1, appId, AppListingListener, onExecutorBlacklisted, applicationInfo, onUnpersistRDD, getAppUI, ne, getConfig, onStageSubmitted, fs, eq, onNodeUnblacklisted, mergeApplicationListing, productIterator, log, isBlacklisted, logPath, ##, finalize, productElement, hashCode, viewAclsGroups, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, CommitDeniedException, $isInstanceOf, getCause, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, toTaskCommitDeniedReason, ==, getStackTraceDepth, clone, addSuppressed, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(CommitDeniedException, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, CommitDeniedException, notifyAll, isInstanceOf, <init>, toTaskCommitDeniedReason, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, stopWorker, lastActivity, wait, idleWorkers, $asInstanceOf, equals, daemonWorkers, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, create, daemon, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, releaseWorker, ==, useDaemon, daemonHost, clone, PythonWorkerFactory, $init$, pythonPath, daemonPort, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, simpleWorkers, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(stopWorker, asInstanceOf, synchronized, create, stop, isInstanceOf, <init>, releaseWorker, ==, PythonWorkerFactory, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	FloatAccumulatorParam, LongAccumulatorParam, IntAccumulatorParam, DoubleAccumulatorParam.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulator.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	estimateSize, toIterable, withFilter, toTraversable, notify, find, span, flatten, toBuffer, count, reduceOption, sliceWithKnownDelta, wait, foldRight, takeWhile, stringPrefix, $asInstanceOf, splitAt, minBy, size, inits, zip, toSet, :\, insert, map, takeRight, dropWhile, toMap, filterNot, equals, par, unzip3, repr, toList, resetSamples, isTraversableAgain, head, asInstanceOf, sameElements, unzip, reduceLeftOption, synchronized, sliding, atGrowThreshold, Self, partition, aggregate, $isInstanceOf, forall, newBuilder, mkString, afterUpdate, min, scanRight, fold, scan, nonEmpty, canEqual, destructiveSortedIterator, tail, lastOption, dropRight, iterator, last, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, ++:, <init>, toStream, companion, max, tails, apply, ++, grouped, flatMap, take, parCombiner, reduceRight, groupBy, ==, maxBy, sliceWithKnownBound, clone, partitionedDestructiveSortedIterator, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, changeValue, $init$, toSeq, zipWithIndex, growTable, toString, genericBuilder, copyToArray, seq, !=, transpose, collect, headOption, getClass, WithFilter, update, hasDefiniteSize, foldLeft, toCollection, isEmpty, ne, PartitionedAppendOnlyMap, init, reversed, destructiveSortedWritablePartitionedIterator, reduceLeft, eq, sum, thisCollection, ##, scanLeft, finalize, hashCode, zipAll, product, view.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(estimateSize, withFilter, flatten, size, zip, insert, map, head, asInstanceOf, synchronized, mkString, iterator, isInstanceOf, filter, <init>, apply, ++, flatMap, ==, partitionedDestructiveSortedIterator, foreach, exists, toArray, changeValue, toString, !=, update, isEmpty, ne, PartitionedAppendOnlyMap, destructiveSortedWritablePartitionedIterator, eq, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	currentExecutorIdCounter, notify, executorRemoved, killExecutor, wait, onNetworkError, localityAwareTasks, defaultParallelism, fetchHadoopDelegationTokens, $asInstanceOf, workerRemoved, requestExecutors, shutdownCallback, equals, killExecutors, DriverEndpoint, disableExecutor, addressToExecutorId, asInstanceOf, initializeLogIfNecessary, applicationId, dead, synchronized, executorsPendingLossReason, self, $isInstanceOf, receive, logTrace, executorAdded, isTraceEnabled, initializeLogIfNecessary$default$2, removeExecutor, StandaloneSchedulerBackend, stop, logName, killTask, notifyAll, conf, isReady, createDriverEndpoint, isInstanceOf, reviveOffers, <init>, onError, doRequestTotalExecutors, doKillExecutors, applicationAttemptId, createDriverEndpointRef, ==, stopExecutors, receiveAndReply, clone, totalCoreCount, killExecutors$default$4, $init$, disconnected, onDisconnected, reset, killExecutorsOnHost, toString, minRegisteredRatio, logError, driverEndpoint, !=, connected, onConnected, hostToLocalTaskCount, getClass, logWarning, onStop, getExecutorIds, start, getDriverLogUrls, requestTotalExecutors, ne, onStart, totalRegisteredExecutors, rpcEnv, eq, log, removeWorker, ##, finalize, hashCode, sufficientResourcesRegistered, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(localityAwareTasks, defaultParallelism, requestExecutors, killExecutors, asInstanceOf, applicationId, synchronized, StandaloneSchedulerBackend, stop, conf, isInstanceOf, <init>, applicationAttemptId, ==, clone, toString, logError, !=, hostToLocalTaskCount, getClass, logWarning, getExecutorIds, start, getDriverLogUrls, requestTotalExecutors, ne, rpcEnv, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	access$102, notify, lookup, safeLookup, numKeys, access$800, access$1400, getPeakMemoryUsedBytes, BytesToBytesMap$Location, wait, $assertionsDisabled, equals, getTaskMemoryManager, getKeyBase, getNumDataPages, getMemoryPage, isDefined, access$1500, access$900, allocateArray, getValueLength, access$200, access$1900, getKeyLength, access$1508, access$500, iterator, access$1600, notifyAll, getPageSizeBytes, access$300, access$1902, BytesToBytesMap, getTotalMemoryConsumption, getArray, <init>, access$1000, MAX_CAPACITY, remove, access$1300, access$600, access$100, nextValue, BytesToBytesMap$MapIterator, this$0, getValueBase, destructiveIterator, acquireMemory, access$1800, getAverageProbesPerLookup, next, freeArray, reset, toString, access$2000, access$002, forEachRemaining, numValues, freeMemory, access$700, spill, getClass, getValueOffset, access$1700, getMode, growAndRehash, hasNext, getKeyOffset, access$400, access$1100, access$2108, free, hashCode, access$1602, append.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, registry, STATSD_KEY_PERIOD, wait, $asInstanceOf, equals, reporter, pollUnit, StatsdSink, prefix, STATSD_DEFAULT_PREFIX, asInstanceOf, initializeLogIfNecessary, host, synchronized, $isInstanceOf, STATSD_DEFAULT_HOST, STATSD_DEFAULT_PERIOD, logTrace, STATSD_KEY_PORT, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, STATSD_DEFAULT_UNIT, isInstanceOf, STATSD_DEFAULT_PORT, <init>, port, STATSD_KEY_PREFIX, pollPeriod, ==, clone, STATSD_KEY_HOST, $init$, report, toString, property, logError, !=, getClass, logWarning, STATSD_KEY_UNIT, start, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InterruptibleIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, context, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, fold, nonEmpty, notifyAll, /:, toIterator, delegate, addString, to, collectFirst, drop, isInstanceOf, filter, GroupedIterator, <init>, toStream, InterruptibleIterator, max, buffered, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, toString, copyToArray, length, seq, !=, collect, getClass, hasDefiniteSize, patch, foldLeft, contains, isEmpty, ne, withPartial, reversed, hasNext, indexOf, reduceLeft, eq, sum, ##, scanLeft, finalize, hashCode, zipAll, product.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(map, context, synchronized, <init>, foreach, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, context, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, delegate, isInstanceOf, filter, GroupedIterator, <init>, InterruptibleIterator, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, context, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, delegate, isInstanceOf, filter, GroupedIterator, <init>, InterruptibleIterator, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(withFilter, map, asInstanceOf, context, drop, isInstanceOf, <init>, InterruptibleIterator, take, ==, slice, foreach, toArray, toSeq, zipWithIndex, length, seq, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(size, toSet, map, toMap, asInstanceOf, context, mkString, nonEmpty, to, isInstanceOf, <init>, InterruptibleIterator, flatMap, ==, foreach, next, length, !=, contains, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, context, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, delegate, isInstanceOf, filter, GroupedIterator, <init>, InterruptibleIterator, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(withFilter, map, toMap, asInstanceOf, context, forall, isInstanceOf, filter, <init>, InterruptibleIterator, flatMap, ==, foreach, reduce, toString, length, !=, collect, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(size, equals, asInstanceOf, context, synchronized, partition, isInstanceOf, filter, <init>, InterruptibleIterator, ==, foreach, toString, !=, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(map, asInstanceOf, context, isInstanceOf, <init>, InterruptibleIterator, flatMap, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(withFilter, map, equals, asInstanceOf, context, isInstanceOf, <init>, InterruptibleIterator, ==, foreach, toArray, zipWithIndex, toString, length, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(size, map, equals, asInstanceOf, context, synchronized, partition, isInstanceOf, filter, <init>, InterruptibleIterator, flatMap, ==, foreach, next, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, context, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, delegate, isInstanceOf, filter, GroupedIterator, <init>, InterruptibleIterator, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf, <init>, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(withFilter, find, size, map, toMap, toList, asInstanceOf, synchronized, partition, mkString, nonEmpty, drop, isInstanceOf, filter, <init>, ++, flatMap, take, ==, clone, foreach, exists, toArray, toSeq, toString, length, !=, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala: Set(map, synchronized, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala: Set(map, <init>, ++)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, context, isInstanceOf, <init>, ==, next, toString, !=, getClass, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(toSet, asInstanceOf, context, partition, <init>, ==, toSeq, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(map, asInstanceOf, context, <init>, toArray, next, length, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(withFilter, map, asInstanceOf, context, drop, isInstanceOf, <init>, InterruptibleIterator, take, ==, slice, foreach, toArray, toSeq, zipWithIndex, length, seq, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(map, toList, asInstanceOf, context, nonEmpty, isInstanceOf, filter, <init>, flatMap, foreach, exists, zipWithIndex, toString, length, !=, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(map, asInstanceOf, context, forall, <init>, ==, reduce, length, isEmpty)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala: Set(map, asInstanceOf, context, <init>, ++, flatMap, foreach, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala: Set(map, asInstanceOf, filter, <init>, toString, collect)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, context, mkString, isInstanceOf, <init>, foreach, next, toString, !=, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(size, map, toList, mkString, to, filter, <init>, ==, foreach, toString, !=, collect, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala: Set(asInstanceOf, filter, <init>, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, length, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(size, zip, toSet, map, equals, asInstanceOf, context, forall, nonEmpty, <init>, flatMap, ==, maxBy, toArray, toSeq, length, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(map, nonEmpty, filter, <init>, contains)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala: Set(map, asInstanceOf, context, filter, <init>, zipWithIndex, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala: Set(withFilter, count, size, map, to, <init>, ==, foreach, length, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala: Set(map, context, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(map, context, partition, <init>, toArray, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(asInstanceOf, min, to, isInstanceOf, filter, <init>, max, length, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(withFilter, map, asInstanceOf, context, partition, mkString, <init>, ==, foreach, toString, length, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(map, min, filter, <init>, max, collect, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(map, toMap, asInstanceOf, filter, <init>, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(size, asInstanceOf, isInstanceOf, <init>, foreach, toArray, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(size, context, min, <init>, max, flatMap, take, ==, foreach, toArray, toSeq, next, length, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, filter, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala: Set(context, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(withFilter, map, toMap, asInstanceOf, context, forall, isInstanceOf, filter, <init>, InterruptibleIterator, flatMap, ==, foreach, reduce, toString, length, !=, collect, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala: Set(withFilter, count, size, partition, aggregate, filter, <init>, flatMap, ==, foreach, reduce, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(size, equals, asInstanceOf, context, synchronized, partition, isInstanceOf, filter, <init>, InterruptibleIterator, ==, foreach, toString, !=, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(map, asInstanceOf, <init>, ++, toArray, toSeq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(map, asInstanceOf, context, <init>, foreach, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(size, asInstanceOf, <init>, foreach, toArray, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(size, map, asInstanceOf, sameElements, context, partition, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, ==, maxBy, foreach, exists, toArray, length, collect, contains, isEmpty, ne, sum, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, size, map, asInstanceOf, context, synchronized, mkString, min, isInstanceOf, GroupedIterator, <init>, grouped, flatMap, ==, foreach, exists, toArray, toString, length, !=, collect, getClass, isEmpty, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(count, zip, map, asInstanceOf, context, aggregate, min, fold, <init>, max, flatMap, take, foreach, toArray, reduce, toSeq, zipWithIndex, collect, isEmpty)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala: Set(wait, asInstanceOf, synchronized, notifyAll, <init>, ==, foreach, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ==, foreach, toArray, toString, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(toSet, asInstanceOf, context, partition, <init>, ==, toSeq, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(map, asInstanceOf, synchronized, isInstanceOf, <init>, ==, foreach, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala: Set(size, map, equals, asInstanceOf, synchronized, isInstanceOf, GroupedIterator, <init>, ++, grouped, flatMap, take, ==, exists, toArray, toSeq, next, toString, length, !=, getClass, isEmpty, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala: Set(map, asInstanceOf, context, <init>, ==, length, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(withFilter, map, equals, asInstanceOf, context, isInstanceOf, <init>, InterruptibleIterator, ==, foreach, toArray, zipWithIndex, toString, length, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(map, context, synchronized, <init>, foreach, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(span, count, foldRight, map, asInstanceOf, context, min, fold, isInstanceOf, <init>, max, ==, foreach, toArray, reduce, next, length, !=, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala: Set(map, asInstanceOf, context, synchronized, isInstanceOf, <init>, flatMap, foreach)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(map, asInstanceOf, context, <init>, clone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(withFilter, count, wait, size, zip, map, asInstanceOf, context, synchronized, min, nonEmpty, notifyAll, isInstanceOf, filter, <init>, ++, take, ==, foreach, toArray, toSeq, zipWithIndex, toString, length, !=, contains, isEmpty, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf, context, partition, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(withFilter, count, size, map, asInstanceOf, context, partition, min, nonEmpty, to, isInstanceOf, filter, <init>, flatMap, ==, foreach, exists, toArray, toSeq, next, zipWithIndex, toString, length, contains, isEmpty, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(map, asInstanceOf, context, partition, isInstanceOf, <init>, ==, foreach, toArray, zipWithIndex, length, seq, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala: Set(toMap, toList, asInstanceOf, mkString, filter, <init>, foreach, toArray, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala: Set(size, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala: Set(map, <init>, toString, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ==, foreach, toSeq, next, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(withFilter, map, asInstanceOf, context, <init>, foreach, zipWithIndex, length, seq, ne, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(size, map, equals, asInstanceOf, context, synchronized, partition, isInstanceOf, filter, <init>, InterruptibleIterator, flatMap, ==, foreach, next, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ==, clone, foreach, toString, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala: Set(map, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(size, map, asInstanceOf, sameElements, context, partition, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, ==, maxBy, foreach, exists, toArray, length, collect, contains, isEmpty, ne, sum, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(asInstanceOf, min, to, isInstanceOf, filter, <init>, max, length, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(map, nonEmpty, filter, <init>, contains)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, context, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, delegate, isInstanceOf, filter, GroupedIterator, <init>, InterruptibleIterator, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(withFilter, map, toMap, asInstanceOf, context, forall, isInstanceOf, filter, <init>, InterruptibleIterator, flatMap, ==, foreach, reduce, toString, length, !=, collect, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(size, asInstanceOf, isInstanceOf, <init>, foreach, toArray, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(map, asInstanceOf, <init>, ++, toArray, toSeq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(size, asInstanceOf, <init>, foreach, toArray, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala: Set(map, synchronized, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(map, toList, asInstanceOf, context, nonEmpty, isInstanceOf, filter, <init>, flatMap, foreach, exists, zipWithIndex, toString, length, !=, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, context, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, delegate, isInstanceOf, filter, GroupedIterator, <init>, InterruptibleIterator, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(map, nonEmpty, filter, <init>, contains)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(map, context, partition, <init>, toArray, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(map, context, synchronized, <init>, foreach, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(size, toSet, map, toMap, asInstanceOf, context, mkString, nonEmpty, to, isInstanceOf, <init>, InterruptibleIterator, flatMap, ==, foreach, next, length, !=, contains, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(withFilter, map, asInstanceOf, isInstanceOf, filter, <init>, max, ==, foreach, !=, getClass, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(withFilter, map, asInstanceOf, context, partition, mkString, <init>, ==, foreach, toString, length, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, context, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, delegate, isInstanceOf, filter, GroupedIterator, <init>, InterruptibleIterator, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(map, asInstanceOf, context, partition, isInstanceOf, <init>, ==, foreach, toArray, zipWithIndex, length, seq, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(withFilter, map, toMap, asInstanceOf, context, forall, isInstanceOf, filter, <init>, InterruptibleIterator, flatMap, ==, foreach, reduce, toString, length, !=, collect, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, context, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, delegate, isInstanceOf, filter, GroupedIterator, <init>, InterruptibleIterator, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, context, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, delegate, isInstanceOf, filter, GroupedIterator, <init>, InterruptibleIterator, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(withFilter, map, toMap, asInstanceOf, context, forall, isInstanceOf, filter, <init>, InterruptibleIterator, flatMap, ==, foreach, reduce, toString, length, !=, collect, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(asInstanceOf, min, to, isInstanceOf, filter, <init>, max, length, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, context, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, delegate, isInstanceOf, filter, GroupedIterator, <init>, InterruptibleIterator, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(withFilter, map, toMap, asInstanceOf, context, forall, isInstanceOf, filter, <init>, InterruptibleIterator, flatMap, ==, foreach, reduce, toString, length, !=, collect, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, context, isInstanceOf, <init>, ==, next, toString, !=, getClass, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, context, mkString, isInstanceOf, <init>, foreach, next, toString, !=, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(size, equals, asInstanceOf, context, synchronized, partition, isInstanceOf, filter, <init>, InterruptibleIterator, ==, foreach, toString, !=, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(map, asInstanceOf, <init>, ++, toArray, toSeq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(map, asInstanceOf, <init>, ++, toArray, toSeq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $assertionsDisabled, equals, getKey, newKey, notifyAll, allocate, <init>, swap, toString, copyElement, getClass, UnsafeSortDataFormat, hashCode, copyRange.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSortDataFormat.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileRecordReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileRecordReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileRecordReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileRecordReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileRecordReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileRecordReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getConf, wait, rrClass, $asInstanceOf, WholeTextFileRecordReader, rrConstructor, curReader, equals, asInstanceOf, context, synchronized, $isInstanceOf, getProgress, notifyAll, Configurable, initialize, isInstanceOf, getCurrentValue, <init>, progress, nextKeyValue, ==, split, clone, ConfigurableCombineFileRecordReader, $init$, idx, toString, !=, getCurrentKey, setConf, getClass, close, ne, initNextRecordReader, fs, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(getConf, asInstanceOf, Configurable, <init>, toString, setConf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(getConf, asInstanceOf, synchronized, initialize, isInstanceOf, <init>, ==, split, clone, toString, !=, getClass, close, ne, fs)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(getConf, asInstanceOf, Configurable, <init>, toString, setConf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala: Set(getConf, context, Configurable, <init>, ==, split, ConfigurableCombineFileRecordReader, setConf, fs)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InternalAccumulator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InternalAccumulator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InternalAccumulator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InternalAccumulator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/InternalAccumulator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	shuffleRead, BYTES_READ, notify, EXECUTOR_DESERIALIZE_TIME, SHUFFLE_READ_METRICS_PREFIX, wait, REMOTE_BYTES_READ_TO_DISK, $asInstanceOf, BYTES_WRITTEN, equals, MEMORY_BYTES_SPILLED, asInstanceOf, PEAK_EXECUTION_MEMORY, EXECUTOR_CPU_TIME, REMOTE_BYTES_READ, UPDATED_BLOCK_STATUSES, RECORDS_READ, synchronized, SHUFFLE_WRITE_METRICS_PREFIX, INPUT_METRICS_PREFIX, METRICS_PREFIX, $isInstanceOf, LOCAL_BLOCKS_FETCHED, OUTPUT_METRICS_PREFIX, notifyAll, REMOTE_BLOCKS_FETCHED, isInstanceOf, LOCAL_BYTES_READ, FETCH_WAIT_TIME, EXECUTOR_RUN_TIME, DISK_BYTES_SPILLED, TEST_ACCUM, JVM_GC_TIME, ==, clone, shuffleWrite, RESULT_SERIALIZATION_TIME, RESULT_SIZE, toString, WRITE_TIME, !=, RECORDS_WRITTEN, getClass, output, ne, EXECUTOR_DESERIALIZE_CPU_TIME, input, eq, InternalAccumulator, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(asInstanceOf, synchronized, METRICS_PREFIX, isInstanceOf, ==, toString, !=, getClass, eq, InternalAccumulator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(shuffleRead, BYTES_READ, EXECUTOR_DESERIALIZE_TIME, REMOTE_BYTES_READ_TO_DISK, BYTES_WRITTEN, MEMORY_BYTES_SPILLED, asInstanceOf, PEAK_EXECUTION_MEMORY, EXECUTOR_CPU_TIME, REMOTE_BYTES_READ, UPDATED_BLOCK_STATUSES, RECORDS_READ, synchronized, LOCAL_BLOCKS_FETCHED, REMOTE_BLOCKS_FETCHED, LOCAL_BYTES_READ, FETCH_WAIT_TIME, EXECUTOR_RUN_TIME, DISK_BYTES_SPILLED, TEST_ACCUM, JVM_GC_TIME, ==, shuffleWrite, RESULT_SERIALIZATION_TIME, RESULT_SIZE, WRITE_TIME, RECORDS_WRITTEN, output, ne, EXECUTOR_DESERIALIZE_CPU_TIME, input, InternalAccumulator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, RESULT_SIZE, !=, ne, InternalAccumulator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(asInstanceOf, METRICS_PREFIX, ==, toString, InternalAccumulator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, METRICS_PREFIX, isInstanceOf, ==, toString, !=, ne, InternalAccumulator)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getPackages, addClass, notify, findResource, assertionLock, wait, $asInstanceOf, setDefaultAssertionStatus, findSystemClass, getURLs, equals, desiredAssertionStatus, asInstanceOf, getPermissions, synchronized, $isInstanceOf, clearAssertionStatus, setSigners, getParent, classAssertionStatus, getResource, ChildFirstURLClassLoader, findLoadedClass, findLibrary, notifyAll, addURL, MutableURLClassLoader, getResourceAsStream, isInstanceOf, loadClass, getPackage, <init>, defineClass, findClass, setClassAssertionStatus, ==, clone, findResources, resolveClass, isAncestor, toString, !=, setPackageAssertionStatus, getClass, getResources, getClassLoadingLock, close, ne, eq, definePackage, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(asInstanceOf, MutableURLClassLoader, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, ChildFirstURLClassLoader, addURL, MutableURLClassLoader, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, getURLs, asInstanceOf, synchronized, ChildFirstURLClassLoader, notifyAll, addURL, MutableURLClassLoader, isInstanceOf, <init>, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, ChildFirstURLClassLoader, MutableURLClassLoader, isInstanceOf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ACTIVE, name, wait, valueOf, fromString, equals, getDeclaringClass, notifyAll, StageStatus, compareTo, COMPLETE, PENDING, ordinal, values, toString, getClass, FAILED, SKIPPED, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala: Set(name, StageStatus, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(name, StageStatus, values, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala: Set(StageStatus)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(name, StageStatus, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(name, equals, StageStatus, values, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(name, StageStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(name, StageStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala: Set(name, StageStatus, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(name, StageStatus, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(StageStatus, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, initLock, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, uninitialize, isInstanceOf, ==, clone, $init$, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala: Set(asInstanceOf, synchronized, ==, Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, ==, Logging, !=, getClass, logWarning, ne, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala: Set(!=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala: Set(==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, isInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(synchronized, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(==, Logging, !=, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, initializeLogIfNecessary, uninitialize, isInstanceOf, ==, Logging, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(isInstanceOf, ==, Logging, toString, getClass, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, hashCode, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(asInstanceOf, logName, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, ==, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(==, Logging, !=, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, ==, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(synchronized, clone, Logging, logError, !=, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala: Set(isInstanceOf, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(Logging, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, initializeLogIfNecessary, uninitialize, isInstanceOf, ==, Logging, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(asInstanceOf, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala: Set(synchronized, ==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(asInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala: Set(isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala: Set(asInstanceOf, isInstanceOf, ==, !=, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala: Set(asInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(equals, asInstanceOf, ==, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(logTrace, Logging, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala: Set(asInstanceOf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala: Set(==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala: Set(Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, ==, Logging, toString, !=, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, ==, Logging, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(asInstanceOf, isInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(asInstanceOf, synchronized, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(asInstanceOf, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, isInstanceOf, ==, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala: Set(==, Logging, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala: Set(asInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, Logging, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala: Set(asInstanceOf, synchronized, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala: Set(asInstanceOf, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(asInstanceOf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala: Set(toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, Logging, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala: Set(==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(==, Logging, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala: Set(asInstanceOf, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, eq, log, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, ==, Logging, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(synchronized, Logging, toString, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala: Set(equals, synchronized, Logging, logError, !=, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala: Set(asInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(clone, Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala: Set(==, Logging, logError, !=, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala: Set(Logging, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala: Set(Logging, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, initializeLogIfNecessary, uninitialize, isInstanceOf, ==, Logging, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(asInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala: Set(Logging, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala: Set(==, Logging, toString, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala: Set(asInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, toString, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala: Set(asInstanceOf, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcAddress.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, ==, Logging, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala: Set(==, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala: Set(asInstanceOf, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerId.scala: Set(asInstanceOf, isInstanceOf, ==, !=, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MemoryParam.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(Logging, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(isInstanceOf, ==, Logging, toString, getClass, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableBuffer.scala: Set(==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(equals, asInstanceOf, ==, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(logTrace, Logging, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/PagedTable.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/FileCommitProtocol.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, ==, Logging, !=, getClass, logWarning, ne, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala: Set(asInstanceOf, ==, Logging, toString, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(logName, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala: Set(asInstanceOf, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(==, Logging, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(asInstanceOf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala: Set(ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala: Set(==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(logTrace, logError, !=, getClass, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala: Set(isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala: Set(isInstanceOf, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, logName, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala: Set(asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala: Set(Logging, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala: Set(initializeLogIfNecessary, ==, Logging, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Benchmark.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, isInstanceOf, ==, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(asInstanceOf, logName, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala: Set(==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala: Set(asInstanceOf, isInstanceOf, ==, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/Logging.scala: Set(equals, asInstanceOf, initializeLogIfNecessary, synchronized, initLock, isTraceEnabled, logName, isInstanceOf, ==, Logging, !=, getClass, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableConfiguration.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(==, Logging, !=, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala: Set(asInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala: Set(Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ClientArguments.scala: Set(asInstanceOf, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, isInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala: Set(!=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(asInstanceOf, clone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, getClass, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala: Set(asInstanceOf, isInstanceOf, ==, !=, log)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(asInstanceOf, initializeLogIfNecessary, ==, Logging, logError, !=, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, toString, logError, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, getClass, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala: Set(toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(asInstanceOf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, ==, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(synchronized, clone, Logging, logError, !=, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala: Set(asInstanceOf, isInstanceOf, ==, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala: Set(asInstanceOf, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala: Set(asInstanceOf, isInstanceOf, ==, clone, Logging, toString, logError, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(Logging, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(!=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(logTrace, logError, !=, getClass, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala: Set(isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, ==, Logging, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala: Set(asInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, initializeLogIfNecessary, uninitialize, isInstanceOf, ==, Logging, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala: Set(isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(logName, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, logName, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(!=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(clone, Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala: Set(Logging, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, ==, Logging, !=, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala: Set(asInstanceOf, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, eq, log, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(synchronized, Logging, toString, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkFiles.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(Logging, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, toString, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala: Set(asInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, ==, Logging, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, Logging, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(Logging, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, ==, Logging, !=, getClass, logWarning, ne, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala: Set(==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala: Set(asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, isInstanceOf, ==, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(asInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala: Set(asInstanceOf, isInstanceOf, ==, !=, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(Logging, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, Logging, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala: Set(asInstanceOf, isInstanceOf, Logging, logError, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(logTrace, Logging, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala: Set(==, Logging, logError, !=, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, ==, Logging, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/GraphiteSink.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/JmxSink.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, initializeLogIfNecessary, uninitialize, isInstanceOf, ==, Logging, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(synchronized, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(asInstanceOf, isInstanceOf, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(==, Logging, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(logTrace, logError, !=, getClass, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/Slf4jSink.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/CsvSink.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(==, Logging, !=, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala: Set(Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, ==, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(asInstanceOf, initializeLogIfNecessary, ==, Logging, logError, !=, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala: Set(synchronized, ==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala: Set(Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala: Set(synchronized, ==)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, ==, Logging, !=, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(asInstanceOf, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(asInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala: Set(asInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, toString, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(equals, asInstanceOf, ==, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala: Set(asInstanceOf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala: Set(==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala: Set(Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, ==, Logging, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(asInstanceOf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(asInstanceOf, isInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala: Set(==, Logging, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(asInstanceOf, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, isInstanceOf, ==, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala: Set(wait, asInstanceOf, synchronized, notifyAll, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(asInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala: Set(asInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, Logging, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala: Set(asInstanceOf, synchronized, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(asInstanceOf, clone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, toString, logError, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala: Set(asInstanceOf, !=)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala: Set(toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(asInstanceOf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala: Set(asInstanceOf, isInstanceOf, ==, clone, Logging, toString, logError, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala: Set(isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala: Set(isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(asInstanceOf, isInstanceOf, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, ==, Logging, toString, !=, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, logName, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(asInstanceOf, synchronized, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, toString, logError, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, Logging, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(Logging, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala: Set(==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, ==, Logging, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, ==, Logging, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, Logging, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(Logging, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, ==, Logging, !=, getClass, logWarning, ne, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, ==, Logging, toString, !=, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, logName, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(synchronized, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(asInstanceOf, synchronized, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, getClass, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(asInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, isInstanceOf, ==, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(Logging, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(==, Logging, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala: Set(==, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(==, Logging, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala: Set(toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala: Set(asInstanceOf, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, isInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, toString, logError, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, ==, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala: Set(asInstanceOf, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala: Set(asInstanceOf, ==, Logging, logError, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(asInstanceOf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala: Set(isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(asInstanceOf, isInstanceOf, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, logName, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(!=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(synchronized, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(Logging, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, ==, Logging, toString, !=, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(asInstanceOf, initializeLogIfNecessary, ==, Logging, logError, !=, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala: Set(toString, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryRecordReader.scala: Set(asInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala: Set(==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala: Set(Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(asInstanceOf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala: Set(==, Logging, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(asInstanceOf, isInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(asInstanceOf, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(synchronized, ==, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(synchronized, ==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(synchronized, ==, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(logTrace, Logging, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(logName, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapRedCommitProtocol.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala: Set(synchronized, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, Logging, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, ==, Logging, !=, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(synchronized, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, ==, Logging, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(asInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, toString, logError, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala: Set(asInstanceOf, isInstanceOf, ==, clone, Logging, toString, logError, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(Logging, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(synchronized, ==, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(synchronized, ==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(synchronized, ==, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, ==, Logging, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(clone, Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, Logging, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, ==, Logging, toString, !=, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, getClass, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala: Set(Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(==, Logging, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, ==, Logging, !=, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala: Set(isInstanceOf, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala: Set(asInstanceOf, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, eq, log, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, ==, Logging, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(synchronized, Logging, toString, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(asInstanceOf, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala: Set(equals, synchronized, Logging, logError, !=, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(clone, Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala: Set(==, Logging, logError, !=, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala: Set(synchronized, ==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala: Set(Logging, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala: Set(asInstanceOf, isInstanceOf, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala: Set(Logging, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, initializeLogIfNecessary, uninitialize, isInstanceOf, ==, Logging, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala: Set(asInstanceOf, isInstanceOf, Logging, logError, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala: Set(Logging, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala: Set(==, Logging, toString, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(Logging, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, toString, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala: Set(toString, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, ==, Logging, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala: Set(==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(synchronized, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, Logging, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala: Set(==, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala: Set(==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(synchronized, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(Logging, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(isInstanceOf, ==, Logging, toString, getClass, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala: Set(wait, asInstanceOf, synchronized, logTrace, notifyAll, isInstanceOf, ==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(equals, asInstanceOf, ==, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(logTrace, Logging, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, ==, Logging, !=, getClass, logWarning, ne, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala: Set(asInstanceOf, ==, Logging, toString, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala: Set(Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(synchronized, ==, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, ==, Logging, toString, !=, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, ==, Logging, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(==, Logging, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(asInstanceOf, synchronized, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(logTrace, logError, !=, getClass, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala: Set(Logging, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(Logging, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/FixedLengthBinaryInputFormat.scala: Set(==, Logging, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala: Set(==, Logging, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, hashCode, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala: Set(wait, synchronized, notifyAll, ==, Logging, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala: Set(Logging, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala: Set(==, Logging, !=, logWarning, ne, hashCode, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(synchronized, ==, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala: Set(initializeLogIfNecessary, ==, Logging, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SignalUtils.scala: Set(synchronized, ==, Logging, logWarning, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala: Set(synchronized, ==, Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(asInstanceOf, logName, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala: Set(synchronized, ==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala: Set(==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(==, Logging, !=, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/PeriodicCheckpointer.scala: Set(==, Logging, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, Logging, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala: Set(Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala: Set(asInstanceOf, synchronized, ==, Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(synchronized, ==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ListenerBus.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, getClass, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(asInstanceOf, initializeLogIfNecessary, ==, Logging, logError, !=, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, toString, logError, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, getClass, logWarning, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala: Set(Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, getClass, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala: Set(asInstanceOf, logTrace, isInstanceOf, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, ==, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(synchronized, clone, Logging, logError, !=, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala: Set(asInstanceOf, isInstanceOf, ==, clone, Logging, toString, logError, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala: Set(isInstanceOf, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala: Set(asInstanceOf, ==, Logging, logError, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, Logging, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, ==, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala: Set(asInstanceOf, synchronized, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(synchronized, ==, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(synchronized, ==, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala: Set(asInstanceOf, isInstanceOf, Logging, logError, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, Logging, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(synchronized, ==, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala: Set(synchronized, ==, Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(synchronized, ==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, ==, Logging, toString, !=, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(logTrace, logError, !=, getClass, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala: Set(==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala: Set(Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriterUtils.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(asInstanceOf, ==, Logging, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(synchronized, Logging, toString, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(clone, Logging, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala: Set(==, Logging, logError, !=, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala: Set(synchronized, ==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/TopologyMapper.scala: Set(Logging, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, Logging, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, initializeLogIfNecessary, uninitialize, isInstanceOf, ==, Logging, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(Logging, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala: Set(asInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala: Set(toString, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, ==, Logging, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, Logging, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala: Set(==, Logging, toString, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala: Set(==, Logging, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(synchronized, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(Logging, logError, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(isInstanceOf, ==, Logging, toString, getClass, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala: Set(asInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(logTrace, Logging, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala: Set(==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, ==, Logging, !=, getClass, logWarning, ne, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HBaseDelegationTokenProvider.scala: Set(asInstanceOf, ==, Logging, toString, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SSLOptions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(synchronized, ==, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala: Set(==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, ==, Logging, toString, !=, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala: Set(Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala: Set(asInstanceOf, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, ==, Logging, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(==, Logging, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala: Set(==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala: Set(getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(logTrace, logError, !=, getClass, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala: Set(Logging, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala: Set(isInstanceOf, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, logError)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, logName, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala: Set(asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala: Set(clone, toString)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(synchronized, ==, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonGatewayServer.scala: Set(initializeLogIfNecessary, ==, Logging, logError, !=, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(asInstanceOf, isInstanceOf, ==, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(asInstanceOf, logName, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala: Set(synchronized, ==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala: Set(==, Logging, !=, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, Logging, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, isInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(synchronized, ==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, getClass, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala: Set(asInstanceOf, isInstanceOf, ==, !=, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(asInstanceOf, initializeLogIfNecessary, ==, Logging, logError, !=, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, toString, logError, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala: Set(Logging, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(asInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(asInstanceOf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, ==, Logging, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(synchronized, clone, Logging, logError, !=, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(==, Logging, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala: Set(toString, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, ==, Logging, !=, getClass, logWarning, ne, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, toString, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, ==, Logging, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, ==, Logging, !=, getClass, logWarning, ne, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, toString, logError, !=, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(asInstanceOf, isInstanceOf, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, !=, getClass, logWarning, ne, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(==, Logging, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, logName, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(==)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, !=, logWarning, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, Logging, toString, logError, !=, getClass, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, logTrace, isInstanceOf, ==, Logging, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, ==, Logging, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(==, Logging)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/ConfigBuilder.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	TypedConfigBuilder, notify, bytesConf, parent, createWithDefault, timeConf, _public, internal, wait, $asInstanceOf, regexConf, productArity, equals, _onCreate, asInstanceOf, synchronized, $isInstanceOf, booleanConf, fallbackConf, canEqual, productPrefix, stringConverter, doc, notifyAll, checkValue, ConfigBuilder, key, isInstanceOf, onCreate, createWithDefaultString, doubleConf, <init>, intConf, ==, clone, longConf, createOptional, $init$, copy, toString, !=, getClass, copy$default$1, converter, _doc, _alternatives, withAlternative, ne, createWithDefaultFunction, stringConf, toSequence, transform, checkValues, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/config.scala: Set(TypedConfigBuilder, bytesConf, createWithDefault, timeConf, internal, doc, ConfigBuilder, createWithDefaultString, <init>, intConf, createOptional, stringConf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/config.scala: Set(TypedConfigBuilder, createWithDefault, timeConf, internal, booleanConf, ConfigBuilder, createWithDefaultString, <init>, intConf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala: Set(TypedConfigBuilder, bytesConf, createWithDefault, timeConf, internal, regexConf, booleanConf, fallbackConf, doc, checkValue, ConfigBuilder, createWithDefaultString, doubleConf, <init>, intConf, longConf, createOptional, withAlternative, stringConf, toSequence, checkValues)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toRDD.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, Partitioner, wait, $asInstanceOf, determineBounds, equals, asInstanceOf, synchronized, $isInstanceOf, <init>$default$4, notifyAll, RangePartitioner, isInstanceOf, <init>$default$3, <init>, ==, HashPartitioner, clone, sketch, toString, defaultPartitioner, !=, getClass, samplePointsPerPartitionHint, getPartition, ne, numPartitions, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, HashPartitioner, clone, toString, defaultPartitioner, !=, getClass, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, HashPartitioner, clone, toString, defaultPartitioner, !=, getClass, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, HashPartitioner, clone, toString, defaultPartitioner, !=, getClass, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, isInstanceOf, <init>, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala: Set(synchronized, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(asInstanceOf, <init>, !=, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(Partitioner, asInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala: Set(Partitioner, asInstanceOf, <init>, toString, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(<init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala: Set(asInstanceOf, <init>, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(Partitioner, equals, asInstanceOf, <init>, ==, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala: Set(asInstanceOf, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala: Set(<init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala: Set(<init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(<init>, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(Partitioner, asInstanceOf, RangePartitioner, isInstanceOf, <init>, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, <init>, ==, toString, !=, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(Partitioner, <init>, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(Partitioner, asInstanceOf, <init>, HashPartitioner, defaultPartitioner, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(asInstanceOf, <init>, ==, !=, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala: Set(Partitioner, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, HashPartitioner, toString, defaultPartitioner, !=, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala: Set(<init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(Partitioner, equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(Partitioner, asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(asInstanceOf, <init>, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(Partitioner, determineBounds, asInstanceOf, RangePartitioner, isInstanceOf, <init>, ==, HashPartitioner, sketch, samplePointsPerPartitionHint, ne, numPartitions, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(Partitioner, wait, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(Partitioner, asInstanceOf, <init>, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala: Set(wait, asInstanceOf, synchronized, notifyAll, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(asInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala: Set(asInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(Partitioner, equals, asInstanceOf, isInstanceOf, <init>, ==, toString, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(Partitioner, asInstanceOf, <init>, clone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(Partitioner, wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, ==, toString, !=, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(Partitioner, asInstanceOf, <init>, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, !=, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala: Set(asInstanceOf, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala: Set(<init>, toString, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala: Set(asInstanceOf, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(Partitioner, equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, clone, toString, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(Partitioner, determineBounds, asInstanceOf, RangePartitioner, isInstanceOf, <init>, ==, HashPartitioner, sketch, samplePointsPerPartitionHint, ne, numPartitions, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(Partitioner, asInstanceOf, RangePartitioner, isInstanceOf, <init>, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, HashPartitioner, clone, toString, defaultPartitioner, !=, getClass, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, HashPartitioner, toString, defaultPartitioner, !=, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala: Set(asInstanceOf, <init>, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala: Set(synchronized, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, HashPartitioner, clone, toString, defaultPartitioner, !=, getClass, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(<init>, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala: Set(Partitioner, asInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala: Set(Partitioner, asInstanceOf, <init>, toString, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, getClass, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, HashPartitioner, clone, toString, defaultPartitioner, !=, getClass, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(Partitioner, <init>, !=, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala: Set(Partitioner, equals, asInstanceOf, <init>, ==, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(Partitioner, asInstanceOf, RangePartitioner, isInstanceOf, <init>, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(Partitioner, <init>, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(Partitioner, asInstanceOf, <init>, HashPartitioner, defaultPartitioner, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala: Set(Partitioner, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, HashPartitioner, toString, defaultPartitioner, !=, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(Partitioner, equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getPartition, ne, numPartitions, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(Partitioner, asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(Partitioner, wait, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(Partitioner, asInstanceOf, <init>, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonPartitioner.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, numPartitions, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(Partitioner, equals, asInstanceOf, isInstanceOf, <init>, ==, toString, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(Partitioner, asInstanceOf, <init>, clone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(Partitioner, wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, ==, toString, !=, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(Partitioner, asInstanceOf, <init>, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, !=, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(Partitioner, equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, <init>, ==, toString, !=, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, HashPartitioner, clone, toString, defaultPartitioner, !=, getClass, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, !=, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, HashPartitioner, toString, defaultPartitioner, !=, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, HashPartitioner, clone, toString, defaultPartitioner, !=, getClass, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, HashPartitioner, clone, toString, defaultPartitioner, !=, getClass, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, HashPartitioner, toString, defaultPartitioner, !=, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(Partitioner, asInstanceOf, RangePartitioner, isInstanceOf, <init>, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, HashPartitioner, clone, toString, defaultPartitioner, !=, getClass, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(Partitioner, asInstanceOf, isInstanceOf, <init>, ==, HashPartitioner, toString, defaultPartitioner, !=, getPartition, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(Partitioner, equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(Partitioner, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, LoadedAppUI, getListing, onUIDetached, productArity, equals, getEmptyListingHtml, asInstanceOf, synchronized, invalidate, $isInstanceOf, lock, canEqual, ApplicationHistoryProvider, productPrefix, stop, notifyAll, isInstanceOf, getLastUpdatedTime, <init>, ui, getApplicationInfo, getEventLogsUnderProcess, ==, clone, writeEventLogs, $init$, valid, copy, toString, !=, getClass, copy$default$1, getAppUI, ne, getConfig, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(LoadedAppUI, asInstanceOf, invalidate, lock, isInstanceOf, <init>, ui, ==, valid, toString, !=, getAppUI, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(LoadedAppUI, asInstanceOf, synchronized, invalidate, lock, ApplicationHistoryProvider, isInstanceOf, <init>, ui, ==, clone, valid, copy, toString, !=, getClass, getAppUI, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(LoadedAppUI, getListing, onUIDetached, getEmptyListingHtml, asInstanceOf, synchronized, ApplicationHistoryProvider, stop, isInstanceOf, getLastUpdatedTime, <init>, ui, getApplicationInfo, getEventLogsUnderProcess, writeEventLogs, toString, getAppUI, getConfig)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsConfig.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getInstance, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, subProperties, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, initialize, isInstanceOf, <init>, properties, ==, clone, perInstanceSubProperties, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, MetricsConfig, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(getInstance, asInstanceOf, subProperties, initialize, <init>, ==, logError, !=, logWarning, MetricsConfig, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	estimateSize, notify, wait, $asInstanceOf, size, equals, +=, resetSamples, asInstanceOf, synchronized, $isInstanceOf, afterUpdate, iterator, notifyAll, isInstanceOf, resize, array, <init>, apply, ==, clone, SizeTrackingVector, toArray, $init$, toString, length, !=, getClass, trim, ne, eq, ##, finalize, hashCode, capacity.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(estimateSize, size, +=, asInstanceOf, synchronized, iterator, isInstanceOf, <init>, apply, ==, SizeTrackingVector, toArray, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, SerializerManager, wait, $asInstanceOf, wrapForCompression, equals, canUseKryo, asInstanceOf, synchronized, $isInstanceOf, dataSerialize, dataDeserializeStream, notifyAll, wrapStream, dataSerializeStream, isInstanceOf, <init>, wrapForEncryption, setDefaultClassLoader, ==, clone, getSerializer, toString, !=, getClass, dataSerializeWithExplicitClassTag, ne, encryptionEnabled, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(SerializerManager, wrapStream, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(SerializerManager, asInstanceOf, isInstanceOf, <init>, ==, !=, ne, encryptionEnabled)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(SerializerManager, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(SerializerManager, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(SerializerManager, asInstanceOf, isInstanceOf, <init>, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(SerializerManager, canUseKryo, asInstanceOf, synchronized, dataDeserializeStream, dataSerializeStream, isInstanceOf, <init>, wrapForEncryption, ==, toString, !=, getClass, dataSerializeWithExplicitClassTag, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(SerializerManager, asInstanceOf, synchronized, wrapStream, <init>, ==, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(SerializerManager, asInstanceOf, synchronized, wrapStream, isInstanceOf, <init>, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(SerializerManager, wait, asInstanceOf, synchronized, isInstanceOf, <init>, wrapForEncryption, ==, toString, !=, getClass, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(SerializerManager, wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, setDefaultClassLoader, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(SerializerManager, asInstanceOf, wrapStream, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(SerializerManager, equals, asInstanceOf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(SerializerManager, asInstanceOf, <init>, getSerializer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(SerializerManager, wrapForCompression, asInstanceOf, synchronized, dataDeserializeStream, isInstanceOf, <init>, ==, getSerializer, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, application, ApplicationSource, wait, $asInstanceOf, equals, asInstanceOf, metricRegistry, synchronized, $isInstanceOf, notifyAll, sourceName, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala: Set(ApplicationSource, asInstanceOf, isInstanceOf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(application, ApplicationSource, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, prefix, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, ApplicationPage, renderJson, toString, !=, getClass, render, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(<init>, ==, ApplicationPage)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, accept, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, NotEqualsFileNameFilter, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, PipedRDD, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(creationSite, zipPartitions, parent, isLocallyCheckpointed, getOrCompute, partitioner, coalesce, name, count, isCheckpointedAndMaterialized, mapPartitions, union, map, subtract, intersection, scope, asInstanceOf, context, getPreferredLocations, doCheckpoint, synchronized, aggregate, compute, min, fold, getOutputDeterministicLevel, iterator, conf, isInstanceOf, filter, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, ++, flatMap, take, groupBy, ==, clone, distinct, retag, foreach, reduce, zipWithIndex, checkpoint, elementClassTag, sample, toString, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, clearDependencies, isEmpty, PipedRDD, ne, getDependencies, mapPartitionsWithIndex, keyBy, getCreationSite, computeOrReadCheckpoint, dependencies, isReliablyCheckpointed, withScope, log, treeAggregate, takeOrdered, firstParent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, notLeader, equals, masterInstance, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, WORKING_DIR, isLeader, ==, ZooKeeperLeaderElectionAgent, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(<init>, ZooKeeperLeaderElectionAgent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, unregisterShuffle, stop, notifyAll, isInstanceOf, ==, clone, shuffleBlockResolver, ShuffleManager, getWriter, registerShuffle, toString, !=, getClass, getReader, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, stop, ==, ShuffleManager, getWriter, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, ShuffleManager, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, stop, isInstanceOf, shuffleBlockResolver, ShuffleManager, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, ==, shuffleBlockResolver, ShuffleManager, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(asInstanceOf, ShuffleManager, registerShuffle)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, asInstanceOf, isInstanceOf, ==, ShuffleManager, toString, getReader, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, unregisterShuffle, isInstanceOf, ==, ShuffleManager, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf, ShuffleManager, getReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(asInstanceOf, isInstanceOf, ==, ShuffleManager, !=, getReader, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, onNetworkError, defaultParallelism, $asInstanceOf, LocalEndpoint, equals, localExecutorId, LocalSchedulerBackend, asInstanceOf, initializeLogIfNecessary, applicationId, synchronized, self, $isInstanceOf, receive, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, killTask, notifyAll, isReady, isInstanceOf, reviveOffers, <init>, onError, applicationAttemptId, statusUpdate, ==, localExecutorHostname, receiveAndReply, clone, $init$, onDisconnected, totalCores, toString, logError, !=, onConnected, getClass, logWarning, onStop, start, getDriverLogUrls, ne, onStart, rpcEnv, getUserClasspath, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(defaultParallelism, LocalSchedulerBackend, asInstanceOf, applicationId, synchronized, stop, isInstanceOf, <init>, applicationAttemptId, ==, clone, toString, logError, !=, getClass, logWarning, start, getDriverLogUrls, ne, rpcEnv, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onExecutorUnblacklisted, notify, showBytesDistribution, percentiles, percentilesHeader, wait, $asInstanceOf, onJobEnd, onApplicationEnd, equals, onTaskEnd, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, initializeLogIfNecessary, probabilities, seconds, synchronized, $isInstanceOf, millisToString, logTrace, onOtherEvent, isTraceEnabled, initializeLogIfNecessary$default$2, onTaskStart, logName, notifyAll, onExecutorRemoved, onExecutorAdded, isInstanceOf, minutes, hours, <init>, showDistribution, onBlockUpdated, extractLongDistribution, ==, onBlockManagerRemoved, clone, onExecutorMetricsUpdate, onEnvironmentUpdate, onBlockManagerAdded, $init$, onNodeBlacklisted, onApplicationStart, onSpeculativeTaskSubmitted, toString, StatsReportListener, logError, !=, getClass, logWarning, extractDoubleDistribution, onExecutorBlacklisted, onUnpersistRDD, ne, onStageSubmitted, showMillisDistribution, eq, onNodeUnblacklisted, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, newDefaultRNG, cloneComplement, wait, $asInstanceOf, BernoulliSampler, equals, BernoulliCellSampler, RandomSampler, asInstanceOf, GapSamplingReplacement, f, synchronized, rng, $isInstanceOf, defaultMaxGapSamplingFraction, setSeed, notifyAll, roundingEpsilon, isInstanceOf, PoissonSampler, GapSampling, <init>, q, ==, clone, $init$, sample, rngEpsilon, poissonGE1, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(BernoulliSampler, BernoulliCellSampler, RandomSampler, asInstanceOf, f, synchronized, setSeed, isInstanceOf, PoissonSampler, <init>, ==, clone, sample, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala: Set(RandomSampler, asInstanceOf, setSeed, <init>, clone, sample)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/config/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	UNROLL_MEMORY_CHECK_PERIOD, notify, DYN_ALLOCATION_INITIAL_EXECUTORS, FORCE_DOWNLOAD_SCHEMES, BLACKLIST_ENABLED, UI_STRICT_TRANSPORT_SECURITY, UI_X_CONTENT_TYPE_OPTIONS, package, PRINCIPAL, SHUFFLE_REGISTRATION_MAX_ATTEMPTS, MAX_TASK_ATTEMPTS_PER_NODE, DRIVER_CLASS_PATH, wait, PY_FILES, BLOCK_MANAGER_PORT, EXTRA_LISTENERS, IO_ENCRYPTION_KEYGEN_ALGORITHM, $asInstanceOf, METRICS_NAMESPACE, REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS, MEMORY_OFFHEAP_ENABLED, UNROLL_MEMORY_GROWTH_FACTOR, FILES_OPEN_COST_IN_BYTES, DRIVER_MEMORY_OVERHEAD, equals, MAX_FAILURES_PER_EXEC_STAGE, asInstanceOf, EXECUTOR_INSTANCES, MAX_FAILED_EXEC_PER_NODE, KEYTAB, synchronized, SHUFFLE_FILE_BUFFER_SIZE, MAX_FAILED_EXEC_PER_NODE_STAGE, EXECUTOR_USER_CLASS_PATH_FIRST, CHECKPOINT_COMPRESS, $isInstanceOf, LISTENER_BUS_METRICS_MAX_LISTENER_CLASSES_TIMED, EVENT_LOG_TESTING, DRIVER_MEMORY, MEMORY_OFFHEAP_SIZE, BLACKLIST_TIMEOUT_CONF, EVENT_LOG_OUTPUT_BUFFER_SIZE, PYSPARK_DRIVER_PYTHON, SHUFFLE_REGISTRATION_TIMEOUT, EXECUTOR_LIBRARY_PATH, EXECUTOR_MEMORY, UNREGISTER_OUTPUT_ON_HOST_ON_FETCH_FAILURE, UI_X_XSS_PROTECTION, notifyAll, MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM, isInstanceOf, HISTORY_UI_MAX_APPS, MAX_TASK_ATTEMPTS_PER_EXECUTOR, CPUS_PER_TASK, DRIVER_USER_CLASS_PATH_FIRST, DRIVER_BIND_ADDRESS, SHUFFLE_DISK_WRITE_BUFFER_SIZE, SHUFFLE_UNSAFE_FILE_OUTPUT_BUFFER_SIZE, STRING_REDACTION_PATTERN, IO_ENCRYPTION_KEY_SIZE_BITS, BUFFER_WRITE_CHUNK_SIZE, PYSPARK_PYTHON, EVENT_LOG_BLOCK_UPDATES, ==, IGNORE_CORRUPT_FILES, EXECUTOR_CLASS_PATH, clone, SECRET_REDACTION_PATTERN, LOCALITY_WAIT, LISTENER_BUS_EVENT_QUEUE_CAPACITY, BLACKLIST_LEGACY_TIMEOUT_CONF, SASL_ENCRYPTION_ENABLED, HADOOP_RDD_IGNORE_EMPTY_SPLITS, SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD, AUTH_SECRET_BIT_LENGTH, toString, UI_SHOW_CONSOLE_PROGRESS, !=, MAX_TASK_FAILURES, DYN_ALLOCATION_MIN_EXECUTORS, NETWORK_AUTH_ENABLED, getClass, EXECUTOR_MEMORY_OVERHEAD, DRIVER_JAVA_OPTIONS, DRIVER_HOST_ADDRESS, SHUFFLE_MAP_OUTPUT_PARALLEL_AGGREGATION_THRESHOLD, BLACKLIST_FETCH_FAILURE_ENABLED, DRIVER_LIBRARY_PATH, IO_CRYPTO_CIPHER_TRANSFORMATION, MAX_FAILURES_PER_EXEC, IO_ENCRYPTION_ENABLED, NETWORK_ENCRYPTION_ENABLED, ne, DYN_ALLOCATION_MAX_EXECUTORS, FILES_MAX_PARTITION_BYTES, EVENT_LOG_COMPRESS, DRIVER_BLOCK_MANAGER_PORT, SHUFFLE_ACCURATE_BLOCK_THRESHOLD, SHUFFLE_SERVICE_ENABLED, EXECUTOR_JAVA_OPTIONS, eq, BLACKLIST_KILL_ENABLED, ##, finalize, IS_PYTHON_APP, hashCode, EVENT_LOG_OVERWRITE, APP_CALLER_CONTEXT, TASK_METRICS_TRACK_UPDATED_BLOCK_STATUSES.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala: Set(package, PYSPARK_DRIVER_PYTHON, PYSPARK_PYTHON, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(package, asInstanceOf, synchronized, UNREGISTER_OUTPUT_ON_HOST_ON_FETCH_FAILURE, isInstanceOf, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(package, asInstanceOf, synchronized, isInstanceOf, ==, LOCALITY_WAIT, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(package, METRICS_NAMESPACE, asInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(package, EXTRA_LISTENERS, asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, UI_SHOW_CONSOLE_PROGRESS, !=, getClass, DRIVER_HOST_ADDRESS, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(package, asInstanceOf, CHECKPOINT_COMPRESS, isInstanceOf, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(DYN_ALLOCATION_INITIAL_EXECUTORS, package, equals, asInstanceOf, EXECUTOR_INSTANCES, synchronized, isInstanceOf, ==, SECRET_REDACTION_PATTERN, AUTH_SECRET_BIT_LENGTH, toString, !=, DYN_ALLOCATION_MIN_EXECUTORS, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala: Set(package, PRINCIPAL, asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(package, asInstanceOf, synchronized, isInstanceOf, HISTORY_UI_MAX_APPS, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(FORCE_DOWNLOAD_SCHEMES, package, PRINCIPAL, asInstanceOf, KEYTAB, isInstanceOf, DRIVER_USER_CLASS_PATH_FIRST, ==, toString, UI_SHOW_CONSOLE_PROGRESS, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(package, asInstanceOf, synchronized, isInstanceOf, ==, !=, DYN_ALLOCATION_MIN_EXECUTORS, ne, DYN_ALLOCATION_MAX_EXECUTORS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(BLACKLIST_ENABLED, package, MAX_TASK_ATTEMPTS_PER_NODE, MAX_FAILURES_PER_EXEC_STAGE, asInstanceOf, MAX_FAILED_EXEC_PER_NODE, MAX_FAILED_EXEC_PER_NODE_STAGE, BLACKLIST_TIMEOUT_CONF, isInstanceOf, MAX_TASK_ATTEMPTS_PER_EXECUTOR, ==, BLACKLIST_LEGACY_TIMEOUT_CONF, toString, MAX_TASK_FAILURES, BLACKLIST_FETCH_FAILURE_ENABLED, MAX_FAILURES_PER_EXEC, ne, SHUFFLE_SERVICE_ENABLED, eq, BLACKLIST_KILL_ENABLED)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala: Set(package, EXECUTOR_INSTANCES, DYN_ALLOCATION_MIN_EXECUTORS, DYN_ALLOCATION_MAX_EXECUTORS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala: Set(package, asInstanceOf, BUFFER_WRITE_CHUNK_SIZE, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(package, BLOCK_MANAGER_PORT, asInstanceOf, synchronized, isInstanceOf, DRIVER_BIND_ADDRESS, ==, toString, !=, getClass, DRIVER_HOST_ADDRESS, IO_ENCRYPTION_ENABLED, ne, DRIVER_BLOCK_MANAGER_PORT)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala: Set(package, MAX_TASK_ATTEMPTS_PER_NODE, MAX_FAILURES_PER_EXEC_STAGE, MAX_FAILED_EXEC_PER_NODE_STAGE, MAX_TASK_ATTEMPTS_PER_EXECUTOR)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SecurityManager.scala: Set(package, ==, SASL_ENCRYPTION_ENABLED, toString, !=, NETWORK_AUTH_ENABLED, NETWORK_ENCRYPTION_ENABLED, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HiveDelegationTokenProvider.scala: Set(package, KEYTAB, isInstanceOf, ==, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala: Set(package, FILES_OPEN_COST_IN_BYTES, asInstanceOf, toString, FILES_MAX_PARTITION_BYTES)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(package, SHUFFLE_REGISTRATION_MAX_ATTEMPTS, asInstanceOf, synchronized, SHUFFLE_REGISTRATION_TIMEOUT, MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM, isInstanceOf, ==, toString, !=, getClass, ne, hashCode, TASK_METRICS_TRACK_UPDATED_BLOCK_STATUSES)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(package, LISTENER_BUS_EVENT_QUEUE_CAPACITY, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala: Set(UI_STRICT_TRANSPORT_SECURITY, UI_X_CONTENT_TYPE_OPTIONS, package, asInstanceOf, UI_X_XSS_PROTECTION, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/WebUI.scala: Set(package, ==, DRIVER_HOST_ADDRESS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(package, equals, asInstanceOf, synchronized, isInstanceOf, ==, IGNORE_CORRUPT_FILES, HADOOP_RDD_IGNORE_EMPTY_SPLITS, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(package, asInstanceOf, synchronized, LISTENER_BUS_METRICS_MAX_LISTENER_CLASSES_TIMED, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(package, wait, asInstanceOf, synchronized, isInstanceOf, CPUS_PER_TASK, ==, toString, !=, MAX_TASK_FAILURES, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(package, asInstanceOf, EVENT_LOG_TESTING, EVENT_LOG_OUTPUT_BUFFER_SIZE, isInstanceOf, EVENT_LOG_BLOCK_UPDATES, ==, toString, !=, getClass, ne, EVENT_LOG_COMPRESS, EVENT_LOG_OVERWRITE)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(package, REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS, asInstanceOf, MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(package, synchronized, notifyAll, !=, APP_CALLER_CONTEXT)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(package, MEMORY_OFFHEAP_ENABLED, synchronized, MEMORY_OFFHEAP_SIZE, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala: Set(package, asInstanceOf, isInstanceOf, ==, !=, SHUFFLE_ACCURATE_BLOCK_THRESHOLD)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(package, wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, toString, !=, SHUFFLE_MAP_OUTPUT_PARALLEL_AGGREGATION_THRESHOLD, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala: Set(package, IO_ENCRYPTION_KEYGEN_ALGORITHM, IO_ENCRYPTION_KEY_SIZE_BITS, IO_CRYPTO_CIPHER_TRANSFORMATION)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(UNROLL_MEMORY_CHECK_PERIOD, package, UNROLL_MEMORY_GROWTH_FACTOR, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(package, MEMORY_OFFHEAP_ENABLED, DRIVER_MEMORY_OVERHEAD, asInstanceOf, MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM, isInstanceOf, ==, LISTENER_BUS_EVENT_QUEUE_CAPACITY, SASL_ENCRYPTION_ENABLED, toString, !=, NETWORK_AUTH_ENABLED, EXECUTOR_MEMORY_OVERHEAD, NETWORK_ENCRYPTION_ENABLED, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(package, equals, asInstanceOf, synchronized, isInstanceOf, ==, IGNORE_CORRUPT_FILES, HADOOP_RDD_IGNORE_EMPTY_SPLITS, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ReduceFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ReduceFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ReduceFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ReduceFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ReduceFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, ReduceFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/ReduceFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClientListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClientListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClientListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClientListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClientListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClientListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, executorRemoved, wait, $asInstanceOf, workerRemoved, equals, asInstanceOf, dead, synchronized, $isInstanceOf, executorAdded, notifyAll, isInstanceOf, ==, clone, disconnected, toString, !=, connected, getClass, ne, StandaloneAppClientListener, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(executorRemoved, workerRemoved, asInstanceOf, dead, executorAdded, isInstanceOf, ==, disconnected, !=, connected, ne, StandaloneAppClientListener)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, StandaloneAppClientListener)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onFlush, notify, read, count, wait, getMetadata, $asInstanceOf, equals, ElementTrackingStore, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, addTrigger, <init>, ==, clone, delete, toString, !=, getClass, close, doAsync, ne, eq, write, setMetadata, ##, finalize, hashCode, view.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(count, ElementTrackingStore, <init>, ==, toString, !=, eq, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala: Set(ElementTrackingStore)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(read, count, ElementTrackingStore, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, close, ne, write, view)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(onFlush, count, equals, ElementTrackingStore, asInstanceOf, isInstanceOf, addTrigger, <init>, ==, delete, toString, !=, getClass, close, doAsync, ne, write, view)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(read, count, ElementTrackingStore, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, delete, toString, !=, getClass, close, ne, eq, write, view)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, sameThread, newForkJoinPool, namedThreadFactory, wait, $asInstanceOf, equals, asInstanceOf, synchronized, newDaemonCachedThreadPool, $isInstanceOf, notifyAll, runInNewThread$default$2, isInstanceOf, ==, clone, newDaemonSingleThreadExecutor, awaitResult, runInNewThread, awaitReady, toString, !=, newDaemonSingleThreadScheduledExecutor, getClass, ThreadUtils, newDaemonFixedThreadPool, ne, newDaemonCachedThreadPool$default$3, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, awaitResult, awaitReady, toString, !=, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(sameThread, asInstanceOf, synchronized, isInstanceOf, ==, awaitResult, toString, !=, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(sameThread, asInstanceOf, isInstanceOf, ==, !=, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, awaitResult, toString, ThreadUtils, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala: Set(asInstanceOf, isInstanceOf, awaitResult, ThreadUtils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, !=, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, newDaemonCachedThreadPool, isInstanceOf, ==, newDaemonSingleThreadExecutor, toString, !=, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala: Set(==, awaitResult, toString, !=, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, !=, ThreadUtils, newDaemonFixedThreadPool, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, newDaemonCachedThreadPool, isInstanceOf, ==, toString, !=, ThreadUtils, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(sameThread, asInstanceOf, synchronized, newDaemonCachedThreadPool, isInstanceOf, ==, clone, toString, !=, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, newDaemonCachedThreadPool, isInstanceOf, ==, awaitReady, toString, !=, getClass, ThreadUtils, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, newDaemonSingleThreadExecutor, toString, getClass, ThreadUtils, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(asInstanceOf, isInstanceOf, ==, !=, newDaemonSingleThreadScheduledExecutor, getClass, ThreadUtils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(newDaemonCachedThreadPool, ==, ThreadUtils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala: Set(namedThreadFactory, isInstanceOf, !=, getClass, ThreadUtils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(sameThread, ==, awaitResult, !=, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, awaitResult, toString, !=, getClass, ThreadUtils, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, newDaemonCachedThreadPool, notifyAll, isInstanceOf, ==, !=, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, newDaemonSingleThreadScheduledExecutor, getClass, ThreadUtils, newDaemonFixedThreadPool, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, newDaemonSingleThreadScheduledExecutor, ThreadUtils, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala: Set(asInstanceOf, synchronized, isInstanceOf, awaitResult, awaitReady, ThreadUtils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(sameThread, asInstanceOf, newDaemonCachedThreadPool, isInstanceOf, ==, awaitResult, !=, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, newDaemonCachedThreadPool, isInstanceOf, ==, !=, ThreadUtils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, ==, !=, ThreadUtils, newDaemonFixedThreadPool, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, ==, awaitResult, toString, !=, ThreadUtils, newDaemonFixedThreadPool, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala: Set(asInstanceOf, isInstanceOf, awaitResult, ThreadUtils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, isInstanceOf, ==, newDaemonSingleThreadExecutor, toString, !=, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, isInstanceOf, ==, newDaemonSingleThreadScheduledExecutor, ThreadUtils, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, dependency, wait, $asInstanceOf, equals, asInstanceOf, shuffleId, initializeLogIfNecessary, synchronized, MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE, SerializedShuffleHandle, $isInstanceOf, unregisterShuffle, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, BypassMergeSortShuffleHandle, ==, clone, shuffleBlockResolver, getWriter, $init$, SortShuffleManager, numMaps, registerShuffle, toString, logError, !=, getClass, logWarning, getReader, ne, eq, log, canUseSerializedShuffle, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	killed, notify, duration, wait, TaskInfo, $asInstanceOf, equals, setAccumulables, running, asInstanceOf, host, finished, synchronized, failed, $isInstanceOf, speculative, markFinished, executorId, notifyAll, isInstanceOf, timeRunning, <init>, id, successful, ==, markGettingResult, clone, status, launchTime, finishTime, toString, attemptNumber, !=, getClass, taskLocality, gettingResultTime, accumulables, ne, gettingResult, eq, taskId, ##, finalize, index, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(duration, TaskInfo, setAccumulables, asInstanceOf, host, finished, synchronized, executorId, isInstanceOf, <init>, id, ==, clone, status, toString, attemptNumber, !=, accumulables, ne, taskId, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(killed, duration, TaskInfo, running, asInstanceOf, host, synchronized, failed, speculative, markFinished, executorId, isInstanceOf, timeRunning, <init>, id, successful, ==, markGettingResult, toString, attemptNumber, !=, taskLocality, ne, taskId, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(TaskInfo, asInstanceOf, host, synchronized, speculative, executorId, isInstanceOf, <init>, ==, !=, ne, taskId, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(duration, TaskInfo, host, finished, speculative, executorId, timeRunning, <init>, id, ==, status, launchTime, toString, attemptNumber, !=, taskLocality, gettingResultTime, accumulables, gettingResult, eq, taskId, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(duration, TaskInfo, equals, asInstanceOf, host, executorId, isInstanceOf, <init>, id, ==, status, launchTime, toString, attemptNumber, !=, getClass, taskLocality, ne, taskId, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(TaskInfo, asInstanceOf, host, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(TaskInfo, asInstanceOf, executorId, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, TaskInfo, asInstanceOf, host, synchronized, executorId, isInstanceOf, <init>, id, ==, toString, !=, ne, taskId, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(duration, TaskInfo, asInstanceOf, isInstanceOf, <init>, ==, toString, attemptNumber, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(killed, TaskInfo, setAccumulables, asInstanceOf, host, failed, speculative, executorId, isInstanceOf, <init>, id, ==, status, launchTime, finishTime, toString, attemptNumber, !=, taskLocality, gettingResultTime, accumulables, ne, taskId, index)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutablePair.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutablePair.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutablePair.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutablePair.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutablePair.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, canEqual, productPrefix, notifyAll, _2, isInstanceOf, <init>, ==, clone, MutablePair, _1, $init$, copy, toString, !=, getClass, copy$default$1, update, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	currentExecutorIdCounter, notify, killExecutor, wait, onNetworkError, localityAwareTasks, defaultParallelism, fetchHadoopDelegationTokens, $asInstanceOf, requestExecutors, equals, killExecutors, DriverEndpoint, disableExecutor, addressToExecutorId, asInstanceOf, initializeLogIfNecessary, applicationId, synchronized, executorsPendingLossReason, self, $isInstanceOf, receive, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, removeExecutor, stop, logName, killTask, notifyAll, conf, isReady, createDriverEndpoint, isInstanceOf, reviveOffers, <init>, onError, doRequestTotalExecutors, doKillExecutors, applicationAttemptId, ENDPOINT_NAME, createDriverEndpointRef, ==, stopExecutors, receiveAndReply, CoarseGrainedSchedulerBackend, clone, totalCoreCount, killExecutors$default$4, $init$, onDisconnected, reset, killExecutorsOnHost, toString, minRegisteredRatio, logError, driverEndpoint, !=, onConnected, hostToLocalTaskCount, getClass, logWarning, onStop, getExecutorIds, start, getDriverLogUrls, requestTotalExecutors, ne, onStart, totalRegisteredExecutors, rpcEnv, eq, log, removeWorker, ##, finalize, hashCode, sufficientResourcesRegistered, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(localityAwareTasks, defaultParallelism, requestExecutors, killExecutors, asInstanceOf, applicationId, synchronized, stop, conf, isInstanceOf, <init>, applicationAttemptId, ENDPOINT_NAME, ==, CoarseGrainedSchedulerBackend, clone, toString, logError, !=, hostToLocalTaskCount, getClass, logWarning, getExecutorIds, start, getDriverLogUrls, requestTotalExecutors, ne, rpcEnv, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(killExecutors, asInstanceOf, removeExecutor, stop, conf, isInstanceOf, <init>, ENDPOINT_NAME, ==, CoarseGrainedSchedulerBackend, totalCoreCount, toString, minRegisteredRatio, logError, !=, logWarning, start, requestTotalExecutors, rpcEnv, removeWorker, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(localityAwareTasks, defaultParallelism, requestExecutors, killExecutors, asInstanceOf, applicationId, synchronized, stop, conf, isInstanceOf, <init>, applicationAttemptId, ENDPOINT_NAME, ==, CoarseGrainedSchedulerBackend, clone, toString, logError, !=, hostToLocalTaskCount, getClass, logWarning, getExecutorIds, start, getDriverLogUrls, requestTotalExecutors, ne, rpcEnv, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, NettyBlockRpcServer, $isInstanceOf, receive, getStreamManager, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, channelActive, logName, notifyAll, isInstanceOf, <init>, channelInactive, ==, clone, $init$, toString, logError, !=, exceptionCaught, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(NettyBlockRpcServer, logTrace, <init>, logError, !=, getClass, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/TimeStampedHashMap.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, DefaultKeySet, find, span, flatten, toBuffer, count, reduceOption, sliceWithKnownDelta, wait, foldRight, copy$default$2, takeWhile, stringPrefix, $asInstanceOf, empty, splitAt, minBy, timestamp, size, union, intersect, inits, zip, compose, toSet, subsetOf, :\, map, takeRight, dropWhile, productArity, toMap, filterNot, equals, par, unzip3, repr, getTimeStampedValue, toList, +=, clear, isTraversableAgain, FilteredKeys, head, asInstanceOf, sameElements, initializeLogIfNecessary, filterKeys, unzip, result, reduceLeftOption, &, synchronized, sliding, withDefault, Self, partition, aggregate, $isInstanceOf, forall, newBuilder, valuesIterator, andThen, mkString, retain, min, scanRight, fold, |, keysIterator, logTrace, scan, nonEmpty, canEqual, getTimestamp, getOrElseUpdate, tail, isTraceEnabled, -=, initializeLogIfNecessary$default$2, lastOption, dropRight, productPrefix, iterator, last, orElse, logName, notifyAll, /:, toIterator, addString, clearOldValues, to, keySet, collectFirst, drop, -, isInstanceOf, getEntrySet, filter, isDefinedAt, ++:, sizeHint, putIfAbsent, <init>, toStream, withDefaultValue, companion, max, mapResult, tails, remove, updated, apply, ++, grouped, diff, flatMap, getOrElse, take, TimeStampedHashMap, parCombiner, reduceRight, groupBy, ==, maxBy, sliceWithKnownBound, clone, putAll, slice, default, foreach, &~, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, subsets, $init$, toSeq, zipWithIndex, sizeHintBounded, copy, put, DefaultValuesIterable, values, toString, genericBuilder, copyToArray, runWith, seq, +, logError, !=, MappedValues, get, transpose, collect, headOption, getClass, logWarning, --=, WithFilter, copy$default$1, update, hasDefiniteSize, mapValues, ++=, --, foldLeft, contains, toCollection, isEmpty, ne, transform, init, reversed, reduceLeft, value, lift, eq, productIterator, sum, log, thisCollection, TimeStampedValue, ##, scanLeft, finalize, productElement, keys, hashCode, zipAll, logDebug, product, view, logInfo, applyOrElse.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createClient, e, notify, name, wait, $asInstanceOf, RpcFailure, setupEndpointRef, productArity, equals, setupEndpointRefByURI, serializeStream, asInstanceOf, initializeLogIfNecessary, currentClient, defaultLookupTimeout, askSync, synchronized, $isInstanceOf, create, receive, receiver, getStreamManager, logTrace, canEqual, removeOutbox, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, channelActive, productPrefix, stop, logName, notifyAll, conf, transportConf, NettyRpcHandler, isInstanceOf, NettyRpcEnvFactory, RequestMessage, NettyRpcEndpointRef, shutdown, NettyRpcEnv, <init>, apply, channelInactive, ==, setupEndpoint, clone, client, fileServer, $init$, content, asyncSetupEndpointRefByURI, address, copy, toString, awaitTermination, timeoutScheduler, endpointRef, logError, !=, exceptionCaught, getClass, startServer, ask, logWarning, copy$default$1, senderAddress, ne, serialize, eq, productIterator, clientConnectionExecutor, log, ##, finalize, currentEnv, productElement, hashCode, logDebug, openChannel, logInfo, send.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala: Set(createClient, e, asInstanceOf, synchronized, removeOutbox, stop, isInstanceOf, NettyRpcEnv, <init>, apply, ==, client, content, address, toString, logError, !=, logWarning, ne, eq, clientConnectionExecutor, logDebug, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(name, setupEndpointRefByURI, asInstanceOf, defaultLookupTimeout, create, conf, isInstanceOf, NettyRpcEnvFactory, <init>, ==, asyncSetupEndpointRefByURI, address, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(e, name, asInstanceOf, synchronized, receiver, stop, conf, isInstanceOf, RequestMessage, NettyRpcEndpointRef, shutdown, NettyRpcEnv, <init>, ==, client, content, address, awaitTermination, endpointRef, logError, !=, logWarning, senderAddress, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(e, asInstanceOf, synchronized, receive, isInstanceOf, NettyRpcEndpointRef, <init>, ==, content, toString, endpointRef, logError, !=, logWarning, senderAddress, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala: Set(e, RpcFailure, NettyRpcEnv, <init>, client, senderAddress, serialize, send)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyStreamManager.scala: Set(transportConf, NettyRpcEnv, <init>, apply, ==, address, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, stageData, wait, $asInstanceOf, equals, withUI, httpRequest, asInstanceOf, synchronized, $isInstanceOf, taskList, oneAttemptData, notifyAll, isInstanceOf, <init>, StagesResource, attemptId, ==, clone, uiRoot, $init$, servletContext, toString, stageList, !=, getClass, appId, taskSummary, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala: Set(withUI, httpRequest, isInstanceOf, <init>, StagesResource, attemptId, ==, uiRoot, !=, appId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	isExecutorBlacklistedForTaskSet, notify, TaskSetBlacklist, wait, $asInstanceOf, isExecutorBlacklistedForTask, equals, stageId, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, execToFailures, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, conf, isInstanceOf, <init>, clock, ==, clone, getLatestFailureReason, $init$, isNodeBlacklistedForTaskSet, updateBlacklistForFailedTask, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo, isNodeBlacklistedForTask.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(isExecutorBlacklistedForTaskSet, TaskSetBlacklist, isExecutorBlacklistedForTask, stageId, asInstanceOf, synchronized, execToFailures, conf, isInstanceOf, <init>, clock, ==, getLatestFailureReason, isNodeBlacklistedForTaskSet, updateBlacklistForFailedTask, toString, logError, !=, logWarning, ne, logDebug, logInfo, isNodeBlacklistedForTask)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	canBuildFrom, rddToSequenceFileRDDFunctions, rddToAsyncRDDActions, rddToOrderedRDDFunctions, numericRDDToDoubleRDDFunctions, ordering, rddToPairRDDFunctions, mkOrderingOps, doubleRDDToDoubleRDDFunctions.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CartesianRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedPartitionsRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/util/PeriodicRDDCheckpointer.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Stage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIWorkloadGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionerAwareUnionRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionPruningRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PartitionwiseSampledRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, masterEndpointRef, serverInfo, webUrl, wait, $asInstanceOf, MasterWebUI, detachHandler, addStaticHandler, master, equals, getBasePath, asInstanceOf, detachTab, initializeLogIfNecessary, killEnabled, idToUiAddress, synchronized, bind, $isInstanceOf, attachPage, logTrace, attachHandler, publicHostName, isTraceEnabled, initializeLogIfNecessary$default$2, handlers, stop, tabs, logName, notifyAll, initialize, isInstanceOf, attachTab, getSecurityManager, <init>, getTabs, addProxy, pageToHandlers, ==, clone, securityManager, $init$, removeStaticHandler, toString, logError, !=, getHandlers, getClass, logWarning, detachPage, ne, eq, log, boundPort, ##, finalize, hashCode, logDebug, sslOptions, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(masterEndpointRef, MasterWebUI, master, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(masterEndpointRef, MasterWebUI, master, killEnabled, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(MasterWebUI, master, asInstanceOf, bind, attachHandler, stop, isInstanceOf, <init>, addProxy, ==, toString, logError, !=, logWarning, ne, log, boundPort, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, setSerializer, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, copy$default$2, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, productArity, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, canEqual, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, narrowDeps, zipWithUniqueId, productPrefix, iterator, coalesce$default$4, countApprox, logName, NarrowCoGroupSplitDep, notifyAll, countApproxDistinct$default$1, conf, CoGroupedRDD, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, rdd, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, splitIndex, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, split, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, copy$default$3, sample, copy, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, rdds, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, copy$default$1, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, CoGroupPartition, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, productIterator, withScope, log, ##, finalize, treeAggregate, index, productElement, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(partitioner, map, asInstanceOf, context, narrowDeps, iterator, NarrowCoGroupSplitDep, isInstanceOf, rdd, <init>, ==, split, foreach, zipWithIndex, !=, partitions, clearDependencies, ne, CoGroupPartition, dependencies, index, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(setSerializer, partitioner, mapPartitions, map, asInstanceOf, context, iterator, conf, CoGroupedRDD, isInstanceOf, filter, rdd, <init>, flatMap, ==, foreach, sparkContext, reduce, toString, !=, partitions, collect, logWarning, isEmpty, ne, countByValueApprox, mapPartitionsWithIndex, withScope, log, index)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/XORShiftRandom.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, nextBytes, asInstanceOf, nextBoolean, synchronized, nextFloat, $isInstanceOf, ints, setSeed, main, nextGaussian, notifyAll, isInstanceOf, <init>, internalNextLong, nextLong, longs, ==, clone, nextDouble, next, nextInt, hashSeed, toString, XORShiftRandom, !=, getClass, ne, internalNextDouble, eq, internalNextInt, ##, finalize, hashCode, benchmark, doubles.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/SamplingUtils.scala: Set(<init>, nextLong, nextDouble, next, XORShiftRandom)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/StratifiedSamplingUtils.scala: Set(setSeed, <init>, ==, nextDouble, XORShiftRandom, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/RandomSampler.scala: Set(asInstanceOf, setSeed, <init>, ==, nextDouble, XORShiftRandom)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, readExternal, wait, writeExternal, copy$default$2, $asInstanceOf, size, productArity, equals, IndirectTaskResult, asInstanceOf, synchronized, $isInstanceOf, DirectTaskResult, TaskResult, canEqual, valueBytes, value$default$1, productPrefix, notifyAll, isInstanceOf, <init>, ==, clone, $init$, accumUpdates, copy, toString, !=, getClass, copy$default$1, ne, blockId, value, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, size, asInstanceOf, synchronized, DirectTaskResult, isInstanceOf, <init>, ==, accumUpdates, toString, !=, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, size, IndirectTaskResult, asInstanceOf, synchronized, DirectTaskResult, valueBytes, notifyAll, isInstanceOf, <init>, ==, accumUpdates, !=, ne, blockId, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(size, asInstanceOf, synchronized, DirectTaskResult, isInstanceOf, <init>, ==, accumUpdates, toString, !=, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(size, IndirectTaskResult, asInstanceOf, DirectTaskResult, TaskResult, isInstanceOf, <init>, ==, accumUpdates, !=, ne, blockId, value)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getTimer, postToAll, wait, $asInstanceOf, listeners, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, removeListener, SparkListenerBus, ==, clone, $init$, findListenersByClass, toString, logError, !=, getClass, logWarning, doPostEvent, addListener, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo, removeListenerOnError.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala: Set(postToAll, asInstanceOf, isInstanceOf, SparkListenerBus, logError, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala: Set(logTrace, removeListener, SparkListenerBus, logError, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, logError, !=, getClass, logWarning, addListener, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala: Set(listeners, asInstanceOf, synchronized, isInstanceOf, removeListener, ==, findListenersByClass, logError, addListener)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, DoubleFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/DoubleFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(call, DoubleFunction)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, coresGranted, duration, desc, executorLimit, wait, startTime, $asInstanceOf, incrementRetryCount, submitDate, state, equals, addExecutor, isFinished, removedExecutors, asInstanceOf, resetRetryCount, synchronized, executors, $isInstanceOf, retryCount, markFinished, removeExecutor, notifyAll, isInstanceOf, endTime, <init>, id, driver, appSource, ==, clone, coresLeft, ApplicationInfo, toString, !=, addExecutor$default$3, getClass, getExecutorLimit, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(coresGranted, duration, desc, startTime, submitDate, state, isFinished, endTime, <init>, id, driver, ==, ApplicationInfo, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterMessages.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ApplicationInfo, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala: Set(coresGranted, duration, desc, startTime, submitDate, state, executors, <init>, id, ApplicationInfo, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(desc, state, <init>, id, ==, ApplicationInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(state, asInstanceOf, executors, isInstanceOf, <init>, id, driver, ==, ApplicationInfo, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala: Set(desc, state, executors, <init>, id, driver, ==, ApplicationInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala: Set(state, asInstanceOf, isInstanceOf, <init>, id, ==, ApplicationInfo, toString, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(desc, executorLimit, incrementRetryCount, submitDate, state, addExecutor, isFinished, asInstanceOf, resetRetryCount, executors, retryCount, markFinished, removeExecutor, isInstanceOf, <init>, id, driver, appSource, ==, coresLeft, ApplicationInfo, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala: Set(coresGranted, duration, desc, state, <init>, ApplicationInfo, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala: Set(<init>, id, driver, ApplicationInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(coresGranted, desc, executorLimit, submitDate, state, isFinished, removedExecutors, executors, <init>, id, ==, coresLeft, ApplicationInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala: Set(state, <init>, ==, ApplicationInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, BINARY_DESC, BINARY, wait, DOUBLE_DESC, DOUBLE_DESC_NULLS_FIRST, equals, PrefixComparators$SignedPrefixComparatorDescNullsFirst, computePrefix, sortSigned, nullsFirst, DOUBLE_NULLS_LAST, PrefixComparators$BinaryPrefixComparator, STRING_DESC, PrefixComparators$UnsignedPrefixComparatorDesc, LONG_DESC, STRING_DESC_NULLS_FIRST, BINARY_DESC_NULLS_FIRST, notifyAll, PrefixComparators$DoublePrefixComparator, <init>, STRING, PrefixComparators$SignedPrefixComparatorNullsLast, PrefixComparators$RadixSortSupport, DOUBLE, PrefixComparators$StringPrefixComparator, PrefixComparators$UnsignedPrefixComparatorDescNullsFirst, PrefixComparators$SignedPrefixComparator, PrefixComparators$SignedPrefixComparatorDesc, toString, PrefixComparators, LONG_DESC_NULLS_FIRST, getClass, sortDescending, LONG_NULLS_LAST, LONG, BINARY_NULLS_LAST, PrefixComparators$UnsignedPrefixComparator, compare, PrefixComparators$UnsignedPrefixComparatorNullsLast, hashCode, STRING_NULLS_LAST.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/PrefixComparators.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterArguments.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, host, synchronized, $isInstanceOf, webUiPort, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, port, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, MasterArguments, eq, log, ##, finalize, hashCode, logDebug, propertiesFile, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, host, webUiPort, isInstanceOf, <init>, port, ==, toString, logError, !=, logWarning, ne, MasterArguments, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, execId, ExecutorLost, dependency, wait, task, copy$default$2, $asInstanceOf, AllJobsCancelled, copy$default$5, listener, productArity, equals, taskSet, ExecutorAdded, stageId, asInstanceOf, JobSubmitted, host, result, CompletionEvent, synchronized, $isInstanceOf, exception, WorkerRemoved, BeginEvent, canEqual, copy$default$4, productPrefix, reason, JobCancelled, groupId, notifyAll, isInstanceOf, <init>, SpeculativeTaskSubmitted, properties, DAGSchedulerEvent, TaskSetFailed, ==, clone, GettingResultEvent, copy$default$7, jobId, $init$, accumUpdates, copy$default$3, func, copy, workerId, message, toString, JobGroupCancelled, MapStageSubmitted, !=, callSite, StageCancelled, partitions, getClass, copy$default$1, finalRDD, ResubmitFailedStages, copy$default$6, ne, eq, productIterator, taskInfo, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(execId, ExecutorLost, dependency, task, AllJobsCancelled, listener, taskSet, ExecutorAdded, stageId, asInstanceOf, JobSubmitted, host, result, CompletionEvent, synchronized, exception, WorkerRemoved, BeginEvent, reason, JobCancelled, groupId, isInstanceOf, <init>, SpeculativeTaskSubmitted, properties, DAGSchedulerEvent, TaskSetFailed, ==, clone, GettingResultEvent, jobId, accumUpdates, func, workerId, message, toString, JobGroupCancelled, MapStageSubmitted, !=, callSite, StageCancelled, partitions, finalRDD, ResubmitFailedStages, ne, taskInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, weight, parent, name, priority, wait, $asInstanceOf, runningTasks, equals, stageId, asInstanceOf, getSortedTaskSetQueue, synchronized, schedulableQueue, $isInstanceOf, checkSpeculatableTasks, Schedulable, notifyAll, isInstanceOf, addSchedulable, getSchedulableByName, minShare, ==, executorLost, clone, removeSchedulable, toString, !=, getClass, schedulingMode, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, executorLost, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala: Set(Schedulable)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala: Set(weight, parent, name, priority, runningTasks, stageId, getSortedTaskSetQueue, schedulableQueue, checkSpeculatableTasks, Schedulable, getSchedulableByName, minShare, ==, executorLost, !=, schedulingMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(parent, name, wait, runningTasks, stageId, asInstanceOf, getSortedTaskSetQueue, synchronized, checkSpeculatableTasks, Schedulable, isInstanceOf, ==, executorLost, removeSchedulable, toString, !=, schedulingMode, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(name, asInstanceOf, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(weight, parent, name, priority, runningTasks, stageId, asInstanceOf, synchronized, checkSpeculatableTasks, Schedulable, isInstanceOf, minShare, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(parent, name, stageId, asInstanceOf, synchronized, schedulableQueue, Schedulable, isInstanceOf, ==, clone, toString, !=, getClass, schedulingMode, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala: Set(weight, name, Schedulable, addSchedulable, getSchedulableByName, minShare, ==, !=, schedulingMode, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingAlgorithm.scala: Set(weight, name, priority, runningTasks, stageId, Schedulable, minShare, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(weight, parent, name, runningTasks, Schedulable, minShare, schedulingMode, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala: Set(weight, parent, name, priority, runningTasks, stageId, getSortedTaskSetQueue, schedulableQueue, checkSpeculatableTasks, Schedulable, getSchedulableByName, minShare, ==, executorLost, !=, schedulingMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(parent, name, wait, runningTasks, stageId, asInstanceOf, getSortedTaskSetQueue, synchronized, checkSpeculatableTasks, Schedulable, isInstanceOf, ==, executorLost, removeSchedulable, toString, !=, schedulingMode, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(parent, name, Schedulable, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(parent, Schedulable)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala: Set(Schedulable)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(weight, parent, name, priority, runningTasks, stageId, asInstanceOf, synchronized, checkSpeculatableTasks, Schedulable, isInstanceOf, minShare, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(parent, name, stageId, asInstanceOf, synchronized, schedulableQueue, Schedulable, isInstanceOf, ==, clone, toString, !=, getClass, schedulingMode, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulableBuilder.scala: Set(weight, name, Schedulable, addSchedulable, getSchedulableByName, minShare, ==, !=, schedulingMode, ne)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(parent, name, wait, runningTasks, stageId, asInstanceOf, getSortedTaskSetQueue, synchronized, checkSpeculatableTasks, Schedulable, isInstanceOf, ==, executorLost, removeSchedulable, toString, !=, schedulingMode, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordComparator.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordComparator.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordComparator.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordComparator.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordComparator.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, notifyAll, <init>, RecordComparator, toString, getClass, compare, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/RecordComparator.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/BitSet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/BitSet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/BitSet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/BitSet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/BitSet.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, andNot, wait, $asInstanceOf, equals, unset, clear, asInstanceOf, cardinality, set, &, synchronized, $isInstanceOf, |, setUntil, iterator, notifyAll, isInstanceOf, <init>, BitSet, clearUntil, ==, clone, nextSetBit, toString, !=, get, getClass, ne, ^, eq, ##, finalize, hashCode, capacity.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/OpenHashSet.scala: Set(asInstanceOf, set, &, |, iterator, <init>, BitSet, ==, nextSetBit, !=, get, ^, hashCode, capacity)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	estimateSize, notify, wait, $asInstanceOf, equals, resetSamples, asInstanceOf, synchronized, $isInstanceOf, afterUpdate, notifyAll, isInstanceOf, ==, clone, $init$, toString, !=, getClass, SizeTracker, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(estimateSize, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(estimateSize, asInstanceOf, synchronized, ==, !=, getClass, SizeTracker, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(estimateSize, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(estimateSize, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(estimateSize, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala: Set(resetSamples, afterUpdate, SizeTracker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(estimateSize, asInstanceOf, synchronized, ==, !=, getClass, SizeTracker, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala: Set(resetSamples, afterUpdate, SizeTracker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala: Set(resetSamples, asInstanceOf, afterUpdate, ==, SizeTracker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(estimateSize, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(estimateSize, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, loadTimer, attachSparkUI, operations, ApplicationCache, wait, copy$default$2, $asInstanceOf, size, detachSparkUI, productArity, equals, asInstanceOf, initializeLogIfNecessary, ApplicationCacheOperations, metricRegistry, evictionCount, synchronized, withSparkUI, invalidate, $isInstanceOf, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, sourceName, isInstanceOf, get$default$2, ApplicationCacheCheckFilter, loadCount, <init>, destroy, attemptId, CacheMetrics, doFilter, clock, completed, ==, clone, $init$, lookupCount, copy, toString, metrics, logError, !=, get, getClass, logWarning, copy$default$1, lookupFailureCount, appId, loadedUI, getAppUI, ne, CacheEntry, retainedApplications, init, CacheKey, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(size, asInstanceOf, synchronized, invalidate, isInstanceOf, <init>, attemptId, clock, completed, ==, clone, copy, toString, logError, !=, get, getClass, logWarning, appId, loadedUI, getAppUI, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(<init>, completed, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(ApplicationCache, asInstanceOf, ApplicationCacheOperations, synchronized, withSparkUI, isInstanceOf, <init>, attemptId, CacheMetrics, toString, metrics, get, appId, getAppUI, retainedApplications, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	appSparkVersion, notify, name, wait, EnvironmentTab, $asInstanceOf, equals, appName, prefix, asInstanceOf, synchronized, $isInstanceOf, pages, attachPage, basePath, notifyAll, isInstanceOf, <init>, EnvironmentPage, ==, clone, renderJson, toString, !=, getClass, render, headerTabs, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(appSparkVersion, EnvironmentTab, appName, prefix, basePath, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/PartialResult.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/PartialResult.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/PartialResult.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/PartialResult.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/PartialResult.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, onFail, wait, $asInstanceOf, map, equals, asInstanceOf, synchronized, isInitialValueFinal, $isInstanceOf, setFailure, notifyAll, isInstanceOf, initialValue, <init>, PartialResult, ==, clone, toString, !=, setFinalValue, getClass, getFinalValue, onComplete, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(map, asInstanceOf, synchronized, isInstanceOf, <init>, PartialResult, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(map, asInstanceOf, synchronized, isInstanceOf, initialValue, <init>, PartialResult, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(map, asInstanceOf, synchronized, isInstanceOf, <init>, PartialResult, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(map, <init>, PartialResult)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(map, asInstanceOf, <init>, PartialResult, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(map, asInstanceOf, isInstanceOf, <init>, PartialResult, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(map, asInstanceOf, <init>, PartialResult)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala: Set(wait, asInstanceOf, synchronized, notifyAll, <init>, PartialResult, ==, setFinalValue)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(map, asInstanceOf, isInstanceOf, <init>, PartialResult, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, METRIC_SHUFFLE_WRITE_TIME, $asInstanceOf, METRIC_SHUFFLE_BYTES_WRITTEN, METRIC_SHUFFLE_LOCAL_BLOCKS_FETCHED, equals, METRIC_SHUFFLE_REMOTE_BYTES_READ_TO_DISK, METRIC_MEMORY_BYTES_SPILLED, METRIC_SHUFFLE_FETCH_WAIT_TIME, METRIC_SHUFFLE_RECORDS_READ, asInstanceOf, metricRegistry, synchronized, $isInstanceOf, METRIC_INPUT_BYTES_READ, METRIC_SHUFFLE_REMOTE_BYTES_READ, METRIC_DESERIALIZE_CPU_TIME, METRIC_RESULT_SIZE, METRIC_DESERIALIZE_TIME, METRIC_JVM_GC_TIME, notifyAll, ExecutorSource, sourceName, isInstanceOf, <init>, METRIC_INPUT_RECORDS_READ, METRIC_RESULT_SERIALIZE_TIME, METRIC_SHUFFLE_TOTAL_BYTES_READ, ==, METRIC_SHUFFLE_RECORDS_WRITTEN, clone, METRIC_CPU_TIME, METRIC_SHUFFLE_REMOTE_BLOCKS_FETCHED, toString, METRIC_RUN_TIME, !=, getClass, METRIC_OUTPUT_RECORDS_WRITTEN, METRIC_OUTPUT_BYTES_WRITTEN, ne, eq, METRIC_DISK_BYTES_SPILLED, ##, finalize, METRIC_SHUFFLE_LOCAL_BYTES_READ, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, METRIC_SHUFFLE_WRITE_TIME, METRIC_SHUFFLE_BYTES_WRITTEN, METRIC_SHUFFLE_LOCAL_BLOCKS_FETCHED, METRIC_SHUFFLE_REMOTE_BYTES_READ_TO_DISK, METRIC_MEMORY_BYTES_SPILLED, METRIC_SHUFFLE_FETCH_WAIT_TIME, METRIC_SHUFFLE_RECORDS_READ, asInstanceOf, synchronized, METRIC_INPUT_BYTES_READ, METRIC_SHUFFLE_REMOTE_BYTES_READ, METRIC_DESERIALIZE_CPU_TIME, METRIC_RESULT_SIZE, METRIC_DESERIALIZE_TIME, METRIC_JVM_GC_TIME, notifyAll, ExecutorSource, isInstanceOf, <init>, METRIC_INPUT_RECORDS_READ, METRIC_RESULT_SERIALIZE_TIME, METRIC_SHUFFLE_TOTAL_BYTES_READ, ==, METRIC_SHUFFLE_RECORDS_WRITTEN, METRIC_CPU_TIME, METRIC_SHUFFLE_REMOTE_BLOCKS_FETCHED, METRIC_RUN_TIME, !=, METRIC_OUTPUT_RECORDS_WRITTEN, METRIC_OUTPUT_BYTES_WRITTEN, ne, eq, METRIC_DISK_BYTES_SPILLED, METRIC_SHUFFLE_LOCAL_BYTES_READ)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function2.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function2.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function2.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function2.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function2.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, Function2.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function2.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(call, Function2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala: Set(call, Function2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(call, Function2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(call, Function2)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/ConsoleSink.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, registry, CONSOLE_KEY_UNIT, wait, $asInstanceOf, equals, reporter, pollUnit, CONSOLE_DEFAULT_PERIOD, ConsoleSink, asInstanceOf, synchronized, $isInstanceOf, stop, notifyAll, isInstanceOf, <init>, pollPeriod, ==, clone, CONSOLE_DEFAULT_UNIT, CONSOLE_KEY_PERIOD, report, toString, property, !=, getClass, start, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ParentClassLoader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ParentClassLoader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ParentClassLoader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ParentClassLoader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ParentClassLoader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getPackages, addClass, notify, findResource, assertionLock, wait, $asInstanceOf, setDefaultAssertionStatus, findSystemClass, equals, desiredAssertionStatus, asInstanceOf, synchronized, $isInstanceOf, clearAssertionStatus, setSigners, getParent, classAssertionStatus, getResource, findLoadedClass, findLibrary, notifyAll, getResourceAsStream, isInstanceOf, loadClass, getPackage, <init>, defineClass, findClass, setClassAssertionStatus, ==, clone, findResources, resolveClass, isAncestor, toString, !=, setPackageAssertionStatus, getClass, getResources, getClassLoadingLock, ParentClassLoader, ne, eq, definePackage, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/MutableURLClassLoader.scala: Set(findResource, getResource, loadClass, <init>, findResources, !=, getResources, ParentClassLoader)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, offHeapStorageMemoryPool, releaseStorageMemory, wait, releaseExecutionMemory, $asInstanceOf, maxOnHeapStorageMemory, releaseAllStorageMemory, equals, acquireUnrollMemory, acquireExecutionMemory, offHeapExecutionMemoryPool, asInstanceOf, initializeLogIfNecessary, releaseAllExecutionMemoryForTask, synchronized, $isInstanceOf, pageSizeBytes, logTrace, getExecutionMemoryUsageForTask, storageMemoryUsed, isTraceEnabled, initializeLogIfNecessary$default$2, releaseUnrollMemory, logName, notifyAll, isInstanceOf, acquireStorageMemory, <init>, setMemoryStore, StaticMemoryManager, ==, maxOffHeapStorageMemory, clone, $init$, onHeapStorageMemoryPool, tungstenMemoryMode, offHeapStorageMemory, toString, onHeapExecutionMemoryPool, logError, !=, maxOffHeapMemory, getClass, logWarning, tungstenMemoryAllocator, ne, eq, log, executionMemoryUsed, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, StaticMemoryManager, ==, toString, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, JacksonMessageWriter, wait, $asInstanceOf, mapper, equals, asInstanceOf, synchronized, $isInstanceOf, writeTo, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, isWriteable, getClass, getSize, ne, makeISODateFormat, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorDescription.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorDescription.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorDescription.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorDescription.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorDescription.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, execId, wait, $asInstanceOf, state, equals, cores, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ExecutorDescription, <init>, ==, clone, toString, !=, getClass, appId, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(execId, state, cores, asInstanceOf, isInstanceOf, ExecutorDescription, <init>, ==, toString, appId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(execId, state, cores, asInstanceOf, synchronized, isInstanceOf, ExecutorDescription, <init>, ==, toString, !=, appId, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(execId, state, cores, asInstanceOf, isInstanceOf, ExecutorDescription, <init>, ==, toString, !=, appId, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala: Set(state, cores, asInstanceOf, isInstanceOf, ExecutorDescription, <init>, ==, toString, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	cancel, isCancelled, isDone, JavaFutureAction, get, jobIds.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala: Set(cancel, isCancelled, isDone, JavaFutureAction, get, jobIds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(JavaFutureAction)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaFutureAction.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala: Set(get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(JavaFutureAction)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(get)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBuffer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, toByteBuffer, wait, toInputStream$default$1, $asInstanceOf, size, equals, asInstanceOf, ChunkedByteBuffer, chunks, synchronized, $isInstanceOf, skip, ChunkedByteBufferInputStream, notifyAll, markSupported, isInstanceOf, toNetty, <init>, writeFully, getChunks, ==, dispose, clone, toArray, copy, chunkedByteBuffer, reset, available, toString, !=, getClass, toInputStream, mark, close, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(read, toByteBuffer, size, asInstanceOf, ChunkedByteBuffer, chunks, isInstanceOf, toNetty, <init>, writeFully, ==, toArray, toString, !=, toInputStream, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, ChunkedByteBuffer, chunks, synchronized, isInstanceOf, <init>, getChunks, ==, dispose, !=, toInputStream, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asInstanceOf, ChunkedByteBuffer, isInstanceOf, <init>, ==, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(toByteBuffer, size, asInstanceOf, ChunkedByteBuffer, chunks, synchronized, isInstanceOf, toNetty, <init>, ==, dispose, toArray, copy, toString, !=, getClass, toInputStream, close, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, size, asInstanceOf, ChunkedByteBuffer, synchronized, notifyAll, isInstanceOf, <init>, ==, toArray, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(read, size, asInstanceOf, ChunkedByteBuffer, synchronized, skip, markSupported, isInstanceOf, <init>, ==, toArray, reset, available, toString, !=, toInputStream, mark, close, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/io/ChunkedByteBufferOutputStream.scala: Set(size, ChunkedByteBuffer, <init>, ==, dispose, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(toByteBuffer, size, asInstanceOf, ChunkedByteBuffer, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(size, asInstanceOf, ChunkedByteBuffer, synchronized, isInstanceOf, <init>, ==, dispose, toArray, copy, toString, !=, toInputStream, close, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, currentResult, notifyAll, isInstanceOf, <init>, merge, ==, clone, SumEvaluator, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, merge, ==, SumEvaluator, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkExitCode.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkExitCode.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkExitCode.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkExitCode.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkExitCode.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, UNCAUGHT_EXCEPTION, $isInstanceOf, OOM, notifyAll, isInstanceOf, SparkExitCode, ==, clone, UNCAUGHT_EXCEPTION_TWICE, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, UNCAUGHT_EXCEPTION, isInstanceOf, SparkExitCode, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala: Set(UNCAUGHT_EXCEPTION, OOM, isInstanceOf, SparkExitCode, UNCAUGHT_EXCEPTION_TWICE)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ExecutorExitCode.scala: Set(UNCAUGHT_EXCEPTION, OOM, SparkExitCode, ==, UNCAUGHT_EXCEPTION_TWICE)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/CompactBuffer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, find, span, flatten, toBuffer, count, reduceOption, sliceWithKnownDelta, wait, +:, foldRight, indices, takeWhile, stringPrefix, $asInstanceOf, splitAt, minBy, size, union, intersect, inits, zip, compose, toSet, corresponds, :\, map, takeRight, dropWhile, toMap, filterNot, equals, par, unzip3, repr, startsWith, toList, +=, lastIndexOf, isTraversableAgain, head, asInstanceOf, sameElements, unzip, reduceLeftOption, sortBy, synchronized, sliding, Self, lengthCompare, partition, indexOfSlice, aggregate, $isInstanceOf, forall, newBuilder, andThen, CompactBuffer, mkString, min, scanRight, fold, scan, nonEmpty, canEqual, tail, lastOption, dropRight, iterator, last, orElse, lastIndexOfSlice, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, reverse, filter, isDefinedAt, sortWith, ++:, endsWith, <init>, containsSlice, toStream, companion, max, combinations, tails, updated, apply, ++, grouped, diff, flatMap, take, parCombiner, reduceRight, groupBy, ==, maxBy, :+, indexWhere, sliceWithKnownBound, sorted, clone, permutations, distinct, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, segmentLength, toArray, reduce, padTo, $init$, toSeq, zipWithIndex, toString, genericBuilder, copyToArray, runWith, length, seq, !=, transpose, collect, headOption, getClass, WithFilter, hasDefiniteSize, ++=, patch, foldLeft, contains, toCollection, isEmpty, ne, init, reversed, indexOf, reduceLeft, lastIndexWhere, lift, eq, prefixLength, reverseMap, sum, thisCollection, reverseIterator, ##, scanLeft, finalize, hashCode, zipAll, product, view, applyOrElse.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(withFilter, map, toMap, +=, asInstanceOf, lengthCompare, forall, CompactBuffer, iterator, isInstanceOf, filter, <init>, apply, flatMap, ==, foreach, reduce, toString, length, !=, collect, ++=, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(withFilter, map, startsWith, asInstanceOf, CompactBuffer, isInstanceOf, filter, <init>, max, apply, ==, foreach, !=, getClass, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(withFilter, map, equals, +=, head, asInstanceOf, CompactBuffer, iterator, isInstanceOf, <init>, apply, ==, foreach, toArray, zipWithIndex, toString, length, ++=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, FlatMapFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(call, FlatMapFunction)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/WorkerCommandBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/WorkerCommandBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/WorkerCommandBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/WorkerCommandBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/WorkerCommandBuilder.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, setPropertiesFile, jars, wait, $asInstanceOf, master, verbose, equals, deployMode, appName, addOptionString, asInstanceOf, buildJavaCommand, synchronized, buildCommand, $isInstanceOf, mainClass, files, notifyAll, conf, isInstanceOf, buildClassPath, <init>, WorkerCommandBuilder, pyFiles, appArgs, getenv, ==, clone, childEnv, getScalaVersion, toString, getEffectiveConfig, !=, getClass, getSparkHome, ne, javaHome, eq, appResource, ##, finalize, hashCode, propertiesFile.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala: Set(buildCommand, mainClass, <init>, WorkerCommandBuilder, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/LeaderElectionAgent.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, electedLeader, equals, masterInstance, asInstanceOf, synchronized, LeaderElectable, $isInstanceOf, MonarchyLeaderAgent, stop, notifyAll, isInstanceOf, <init>, ==, clone, revokedLeadership, LeaderElectionAgent, $init$, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(<init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(<init>, clone, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, stop, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperLeaderElectionAgent.scala: Set(electedLeader, masterInstance, synchronized, LeaderElectable, <init>, ==, revokedLeadership, LeaderElectionAgent)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(LeaderElectable, MonarchyLeaderAgent, <init>, LeaderElectionAgent)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, LeaderElectable, MonarchyLeaderAgent, stop, isInstanceOf, <init>, ==, LeaderElectionAgent, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(LeaderElectable, MonarchyLeaderAgent, <init>, LeaderElectionAgent)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	locations, GetExecutorEndpointRef, notify, readExternal, unapply, curried, execId, GetPeers, GetLocationsMultipleBlockIds, GetMatchingBlockIds, diskSize, wait, writeExternal, copy$default$2, $asInstanceOf, GetMemoryStatus, maxReplicas, RemoveBlock, copy$default$5, compose, GetStorageStatus, productArity, equals, RemoveRdd, BlockManagerMessages, asInstanceOf, shuffleId, BlockLocationsAndStatus, replicas, synchronized, $isInstanceOf, andThen, ToBlockManagerSlave, tupled, maxOnHeapMemSize, canEqual, RegisterBlockManager, copy$default$4, executorId, productPrefix, maxOffHeapMemSize, notifyAll, isInstanceOf, removeFromDriver, filter, broadcastId, HasCachedBlocks, ToBlockManagerMaster, blockManagerId, <init>, apply$default$2, apply, ==, rddId, GetLocations, clone, status, UpdateBlockInfo, RemoveBroadcast, ReplicateBlock, StopBlockManagerMaster, $init$, askSlaves, RemoveShuffle, copy$default$3, blockIds, copy, sender, toString, RemoveExecutor, !=, memSize, GetLocationsAndStatus, getClass, copy$default$1, GetBlockStatus, ne, blockId, TriggerThreadDump, <init>$default$2, eq, productIterator, storageLevel, ##, finalize, productElement, hashCode, BlockManagerHeartbeat.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(unapply, execId, BlockManagerMessages, asInstanceOf, shuffleId, synchronized, executorId, isInstanceOf, filter, blockManagerId, <init>, apply, ==, clone, status, blockIds, toString, !=, ne, BlockManagerHeartbeat)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(unapply, BlockManagerMessages, asInstanceOf, synchronized, executorId, isInstanceOf, filter, blockManagerId, <init>, apply, ==, rddId, clone, status, toString, !=, getClass, ne, TriggerThreadDump)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(locations, GetExecutorEndpointRef, execId, GetPeers, GetLocationsMultipleBlockIds, GetMatchingBlockIds, diskSize, GetMemoryStatus, maxReplicas, RemoveBlock, GetStorageStatus, RemoveRdd, BlockManagerMessages, asInstanceOf, shuffleId, BlockLocationsAndStatus, maxOnHeapMemSize, RegisterBlockManager, executorId, maxOffHeapMemSize, isInstanceOf, removeFromDriver, filter, broadcastId, HasCachedBlocks, blockManagerId, <init>, apply, ==, rddId, GetLocations, status, UpdateBlockInfo, RemoveBroadcast, ReplicateBlock, StopBlockManagerMaster, askSlaves, RemoveShuffle, blockIds, sender, toString, RemoveExecutor, !=, memSize, GetLocationsAndStatus, GetBlockStatus, ne, blockId, eq, storageLevel, hashCode, BlockManagerHeartbeat)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(locations, unapply, diskSize, maxReplicas, BlockManagerMessages, asInstanceOf, BlockLocationsAndStatus, synchronized, executorId, isInstanceOf, filter, broadcastId, blockManagerId, <init>, apply, ==, rddId, status, blockIds, copy, toString, !=, memSize, getClass, ne, blockId, storageLevel, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala: Set(diskSize, BlockManagerMessages, asInstanceOf, isInstanceOf, blockManagerId, <init>, ==, UpdateBlockInfo, toString, memSize, blockId, eq, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(GetExecutorEndpointRef, execId, GetPeers, GetLocationsMultipleBlockIds, GetMatchingBlockIds, diskSize, GetMemoryStatus, RemoveBlock, GetStorageStatus, RemoveRdd, BlockManagerMessages, shuffleId, BlockLocationsAndStatus, maxOnHeapMemSize, RegisterBlockManager, executorId, maxOffHeapMemSize, filter, broadcastId, HasCachedBlocks, blockManagerId, <init>, apply, ==, rddId, GetLocations, status, UpdateBlockInfo, RemoveBroadcast, StopBlockManagerMaster, askSlaves, RemoveShuffle, blockIds, RemoveExecutor, !=, memSize, GetLocationsAndStatus, GetBlockStatus, ne, blockId, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(GetMatchingBlockIds, maxReplicas, RemoveBlock, RemoveRdd, BlockManagerMessages, asInstanceOf, shuffleId, replicas, isInstanceOf, filter, broadcastId, <init>, apply, ==, rddId, RemoveBroadcast, ReplicateBlock, RemoveShuffle, !=, GetBlockStatus, blockId, TriggerThreadDump)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/EmptyRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, EmptyRDD, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(name, union, map, asInstanceOf, EmptyRDD, conf, <init>, ++, first)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, parent, partitioner, name, setName, union, map, asInstanceOf, EmptyRDD, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, <init>, id, max, toDebugString, ++, flatMap, ==, clone, foreach, zipWithIndex, first, toString, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, currentResult, notifyAll, isInstanceOf, <init>, merge, bound, ==, clone, toString, !=, getClass, ne, eq, CountEvaluator, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne, CountEvaluator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala: Set(<init>, bound, ==, ne, CountEvaluator)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, canEqual, notifyAll, isInstanceOf, executorHost, <init>, ==, clone, ExecutorInfo, totalCores, toString, !=, getClass, ne, eq, ##, finalize, logUrlMap, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, executorHost, <init>, ==, ExecutorInfo, totalCores, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ExecutorInfo, totalCores, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, executorHost, <init>, ==, ExecutorInfo, totalCores, toString, !=, getClass, ne, logUrlMap)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ExecutorInfo, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/ExecutorData.scala: Set(asInstanceOf, executorHost, <init>, ExecutorInfo, totalCores, logUrlMap)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, isInstanceOf, executorHost, <init>, ==, ExecutorInfo, totalCores, toString, !=, ne, logUrlMap)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, executorHost, <init>, ==, ExecutorInfo, totalCores, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	keytab, notify, defaultSparkProperties, switches, handleExtraArgs, PRINCIPAL, submissionToKill, SUPERVISE, PROXY_USER, name, primaryResource, jars, DRIVER_CLASS_PATH, wait, PY_FILES, supervise, $asInstanceOf, SparkSubmitArguments, executorMemory, REPOSITORIES, VERBOSE, driverExtraJavaOptions, master, verbose, equals, queue, NAME, deployMode, USAGE_ERROR, archives, asInstanceOf, proxyUser, KEYTAB, synchronized, $isInstanceOf, ivyRepoPath, DRIVER_MEMORY, STATUS, childArgs, EXECUTOR_MEMORY, mainClass, DRIVER_CORES, CONF, driverCores, files, notifyAll, totalExecutorCores, TOTAL_EXECUTOR_CORES, isInstanceOf, VERSION, EXECUTOR_CORES, FILES, isStandaloneCluster, <init>, pyFiles, packagesExclusions, DEPLOY_MODE, executorCores, useRest, parse, ==, HELP, PACKAGES_EXCLUDE, MASTER, clone, isPython, PROPERTIES_FILE, PACKAGES, KILL_SUBMISSION, repositories, principal, driverMemory, driverExtraClassPath, toString, NUM_EXECUTORS, QUEUE, submissionToRequestStatusFor, !=, getClass, DRIVER_JAVA_OPTIONS, JARS, DRIVER_LIBRARY_PATH, ARCHIVES, handleUnknown, ne, driverExtraLibraryPath, opts, CLASS, numExecutors, action, eq, packages, sparkProperties, ivySettingsPath, ##, finalize, isR, hashCode, propertiesFile, handle.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(keytab, PRINCIPAL, submissionToKill, name, primaryResource, jars, supervise, SparkSubmitArguments, executorMemory, driverExtraJavaOptions, master, verbose, queue, deployMode, archives, asInstanceOf, proxyUser, KEYTAB, ivyRepoPath, childArgs, mainClass, driverCores, files, totalExecutorCores, isInstanceOf, isStandaloneCluster, <init>, pyFiles, packagesExclusions, executorCores, useRest, ==, isPython, repositories, principal, driverMemory, driverExtraClassPath, toString, submissionToRequestStatusFor, !=, getClass, ne, driverExtraLibraryPath, numExecutors, action, eq, packages, sparkProperties, ivySettingsPath, isR)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, ExecutorTable, notifyAll, isInstanceOf, <init>, ==, toNodeSeq, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, ExecutorTable, isInstanceOf, <init>, ==, toNodeSeq, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ShuffleBlockResolver, notify, ShuffleId, wait, $asInstanceOf, getBlockData, equals, asInstanceOf, synchronized, $isInstanceOf, stop, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(stop, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, stop, isInstanceOf, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(ShuffleBlockResolver, getBlockData, asInstanceOf, synchronized, stop, isInstanceOf, ==, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(ShuffleBlockResolver, synchronized, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala: Set(ShuffleBlockResolver)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getServiceDelegationTokenProvider, wait, $asInstanceOf, isServiceEnabled, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, obtainDelegationTokens, getClass, logWarning, HadoopDelegationTokenManager, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, isConnected, onStopRequest, wait, $asInstanceOf, connect, setAppId, equals, setState, asInstanceOf, synchronized, $isInstanceOf, notifyAll, conf, isInstanceOf, LauncherBackend, <init>, ==, clone, onDisconnected, toString, !=, getClass, close, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(connect, setAppId, setState, asInstanceOf, conf, isInstanceOf, LauncherBackend, <init>, ==, toString, !=, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(connect, setAppId, setState, asInstanceOf, conf, isInstanceOf, LauncherBackend, <init>, ==, toString, close, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(setAppId, asInstanceOf, synchronized, conf, isInstanceOf, <init>, ==, clone, toString, !=, getClass, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(setAppId, asInstanceOf, synchronized, conf, isInstanceOf, <init>, ==, clone, toString, !=, getClass, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, basicSparkPage, commonHeaderNodes, wait, $asInstanceOf, makeHref, UIUtils, headerSparkPage$default$7, prependBaseUri$default$1, equals, vizHeaderNodes, prependBaseUri, asInstanceOf, initializeLogIfNecessary, listingTable, listingTable$default$6, synchronized, headerSparkPage$default$4, $isInstanceOf, logTrace, decodeURLParameter, isTraceEnabled, initializeLogIfNecessary$default$2, makeDescription$default$3, logName, notifyAll, listingTable$default$4, listingTable$default$5, isInstanceOf, formatNumber, listingTable$default$7, makeDescription, TABLE_CLASS_NOT_STRIPED, dataTablesHeaderNodes, makeProgressBar, basicSparkPage$default$3, headerSparkPage, ==, listingTable$default$8, getTimeZoneOffset, clone, formatDate, uiRoot, showDagVizForStage, headerSparkPage$default$5, $init$, TABLE_CLASS_STRIPED_SORTABLE, showDagVizForJob, toString, tooltip, logError, !=, getClass, logWarning, formatDuration, TABLE_CLASS_STRIPED, ne, prependBaseUri$default$2, stripXSS, eq, log, ##, finalize, headerSparkPage$default$6, hashCode, logDebug, logInfo, formatDurationVerbose.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(basicSparkPage, makeHref, UIUtils, listingTable, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala: Set(UIUtils, prependBaseUri, headerSparkPage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(basicSparkPage, makeHref, UIUtils, listingTable, ==, formatDate, toString, formatDuration, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(basicSparkPage, UIUtils, asInstanceOf, synchronized, isInstanceOf, toString, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(UIUtils, prependBaseUri, decodeURLParameter, isInstanceOf, makeDescription, makeProgressBar, headerSparkPage, ==, getTimeZoneOffset, formatDate, toString, !=, formatDuration, ne, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala: Set(UIUtils, decodeURLParameter, isInstanceOf, headerSparkPage, ==, formatDate, TABLE_CLASS_STRIPED, ne, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala: Set(UIUtils, prependBaseUri, asInstanceOf, decodeURLParameter, isInstanceOf, makeDescription, makeProgressBar, ==, formatDate, tooltip, !=, formatDuration, ne, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(UIUtils, prependBaseUri, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(UIUtils, logName, TABLE_CLASS_STRIPED_SORTABLE, toString, formatDuration, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(UIUtils, prependBaseUri, listingTable, headerSparkPage, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(UIUtils, prependBaseUri, asInstanceOf, listingTable, decodeURLParameter, logName, isInstanceOf, headerSparkPage, ==, getTimeZoneOffset, formatDate, showDagVizForStage, toString, tooltip, !=, formatDuration, ne, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala: Set(UIUtils, listingTable, headerSparkPage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(basicSparkPage, UIUtils, prependBaseUri, !=, ne, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(UIUtils, prependBaseUri, asInstanceOf, listingTable, isInstanceOf, headerSparkPage, ==, toString, !=, stripXSS, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(UIUtils, headerSparkPage, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(basicSparkPage, UIUtils, asInstanceOf, isInstanceOf, ==, toString, logError, ne, stripXSS, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(UIUtils, decodeURLParameter, headerSparkPage, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(UIUtils, asInstanceOf, isInstanceOf, headerSparkPage, ==, getTimeZoneOffset, formatDate, showDagVizForJob, toString, !=, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(UIUtils, ==, toString, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(UIUtils, ==, toString, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(basicSparkPage, makeHref, UIUtils, listingTable, ==, formatDate, stripXSS)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, PairFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(call, PairFunction)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(call, PairFunction)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, desc, wait, startTime, $asInstanceOf, submitDate, state, DriverInfo, equals, asInstanceOf, synchronized, $isInstanceOf, exception, notifyAll, isInstanceOf, <init>, id, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode, worker.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(desc, startTime, submitDate, state, DriverInfo, <init>, id, ==, toString, worker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala: Set(desc, startTime, submitDate, state, DriverInfo, <init>, id, toString, worker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(state, DriverInfo, asInstanceOf, exception, isInstanceOf, <init>, id, ==, toString, eq, worker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala: Set(desc, state, DriverInfo, <init>, id, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(desc, submitDate, state, DriverInfo, asInstanceOf, exception, isInstanceOf, <init>, id, ==, toString, !=, ne, worker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/PersistenceEngine.scala: Set(DriverInfo, <init>, id, worker)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/HadoopWriteConfigUtil.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, createCommitter, initOutputFormat, notifyAll, isInstanceOf, closeWriter, <init>, ==, clone, toString, assertConf, !=, getClass, createTaskAttemptContext, ne, HadoopWriteConfigUtil, createJobContext, eq, write, ##, finalize, hashCode, initWriter.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, createCommitter, initOutputFormat, isInstanceOf, closeWriter, <init>, ==, toString, assertConf, !=, getClass, createTaskAttemptContext, ne, HadoopWriteConfigUtil, createJobContext, write, initWriter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, HadoopWriteConfigUtil, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, HadoopWriteConfigUtil, write)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, formatPath$default$2, wait, $asInstanceOf, equals, asInstanceOf, formatPath, synchronized, $isInstanceOf, main, formatPaths, notifyAll, formatPaths$default$2, isInstanceOf, ==, clone, toString, !=, getClass, ne, PythonRunner, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, formatPaths, isInstanceOf, ==, toString, !=, getClass, ne, PythonRunner, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(formatPath, ==, toString, !=, ne, PythonRunner)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $assertionsDisabled, equals, packedRecordPointer, numRecords, insertRecord, notifyAll, <init>, ShuffleInMemorySorter$ShuffleSorterIterator, hasSpaceForAnotherRecord, reset, toString, loadNext, getClass, expandPointerArray, getMemoryUsage, hasNext, getSortedIterator, ShuffleInMemorySorter, free, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/SocketAuthHelper.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, secret, wait, $asInstanceOf, equals, asInstanceOf, SocketAuthHelper, synchronized, $isInstanceOf, readUtf8, authToServer, notifyAll, isInstanceOf, writeUtf8, <init>, ==, clone, authClient, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(secret, SocketAuthHelper, synchronized, authToServer, <init>, authClient, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(secret, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(secret, asInstanceOf, synchronized, isInstanceOf, <init>, ==, authClient, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(secret, wait, asInstanceOf, SocketAuthHelper, synchronized, isInstanceOf, <init>, ==, authClient, toString, !=, getClass, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(secret, asInstanceOf, <init>, ==, authClient, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala: Set(SocketAuthHelper, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(secret, asInstanceOf, synchronized, isInstanceOf, <init>, ==, authClient, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(secret, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(secret, asInstanceOf, <init>, ==, authClient, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, shuffleRecordsWritten, writeTime, _writeTime, wait, shuffleWriteTime, $asInstanceOf, incRecordsWritten, recordsWritten, equals, _bytesWritten, asInstanceOf, incBytesWritten, bytesWritten, synchronized, $isInstanceOf, _recordsWritten, notifyAll, decBytesWritten, isInstanceOf, <init>, ==, clone, incWriteTime, decRecordsWritten, toString, !=, getClass, ne, eq, ShuffleWriteMetrics, ##, finalize, hashCode, shuffleBytesWritten.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(incRecordsWritten, incBytesWritten, decBytesWritten, <init>, ==, incWriteTime, decRecordsWritten, !=, ShuffleWriteMetrics)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(shuffleRecordsWritten, writeTime, shuffleWriteTime, recordsWritten, bytesWritten, <init>, ==, toString, !=, eq, ShuffleWriteMetrics, shuffleBytesWritten)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>, incWriteTime, !=, ShuffleWriteMetrics)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, ne, ShuffleWriteMetrics, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, <init>, ==, !=, getClass, ne, ShuffleWriteMetrics, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, bytesWritten, isInstanceOf, <init>, ==, toString, !=, getClass, ne, ShuffleWriteMetrics)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(_writeTime, _bytesWritten, asInstanceOf, synchronized, _recordsWritten, <init>, ==, ne, ShuffleWriteMetrics)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, ne, eq, ShuffleWriteMetrics, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(writeTime, wait, recordsWritten, asInstanceOf, bytesWritten, synchronized, notifyAll, isInstanceOf, <init>, ==, !=, ne, eq, ShuffleWriteMetrics)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(asInstanceOf, bytesWritten, isInstanceOf, <init>, ==, toString, !=, ne, eq, ShuffleWriteMetrics)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(writeTime, incRecordsWritten, recordsWritten, asInstanceOf, incBytesWritten, bytesWritten, isInstanceOf, <init>, ==, incWriteTime, toString, !=, ne, ShuffleWriteMetrics)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorState.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorState.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ExecutorDescription.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ExecutorDesc.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	printStream, notify, CommandLineUtils, printWarning, wait, $asInstanceOf, equals, asInstanceOf, synchronized, exitFn, $isInstanceOf, parseSparkConfProperty, main, notifyAll, isInstanceOf, printErrorAndExit, ==, clone, $init$, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(printStream, CommandLineUtils, printWarning, asInstanceOf, exitFn, isInstanceOf, printErrorAndExit, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala: Set(printStream, printWarning, exitFn, parseSparkConfProperty, isInstanceOf, printErrorAndExit, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala: Set(printStream, printWarning, exitFn, parseSparkConfProperty, isInstanceOf, printErrorAndExit, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getBlacklistTimeout, notify, dropFailuresWithTimeoutBefore, BlacklistTracker, wait, $asInstanceOf, handleRemovedExecutor, isExecutorBlacklisted, equals, nodeToBlacklistedExecs, asInstanceOf, initializeLogIfNecessary, executorIdToBlacklistStatus, synchronized, $isInstanceOf, nextExpiryTime, <init>$default$4, addFailures, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, ExecutorFailureList, notifyAll, isInstanceOf, nodeIdToBlacklistExpiryTime, <init>, BLACKLIST_TIMEOUT_MILLIS, isBlacklistEnabled, ==, updateBlacklistForSuccessfulTaskSet, clone, $init$, numUniqueTaskFailures, toString, logError, !=, getClass, logWarning, applyBlacklistTimeout, isNodeBlacklisted, isEmpty, ne, updateBlacklistForFetchFailure, nodeBlacklist, validateBlacklistConfs, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(BlacklistTracker, wait, handleRemovedExecutor, isExecutorBlacklisted, asInstanceOf, synchronized, isInstanceOf, <init>, isBlacklistEnabled, ==, toString, logError, !=, logWarning, applyBlacklistTimeout, isNodeBlacklisted, isEmpty, ne, nodeBlacklist, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(BlacklistTracker, isExecutorBlacklisted, asInstanceOf, synchronized, isInstanceOf, <init>, ==, updateBlacklistForSuccessfulTaskSet, toString, logError, !=, logWarning, isNodeBlacklisted, isEmpty, ne, updateBlacklistForFetchFailure, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getStr, notify, wait, copy$default$2, $asInstanceOf, WriteInputFormatTestDataGenerator, productArity, equals, readFields, TestWritable, DoubleArrayToWritableConverter, getValueClass, asInstanceOf, getDouble, set, synchronized, $isInstanceOf, TestOutputKeyConverter, canEqual, TestInputValueConverter, main, productPrefix, notifyAll, setInt, isInstanceOf, setStr, double, DoubleArrayWritable, str, <init>, ==, TestInputKeyConverter, clone, int, getInt, toArray, $init$, copy$default$3, copy, toString, toStrings, !=, WritableToDoubleArrayConverter, get, getClass, copy$default$1, ne, TestOutputValueConverter, eq, convert, generateData, productIterator, write, ##, finalize, productElement, hashCode, setDouble.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcTimeout.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, duration, RpcTimeout, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, timeoutProp, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, apply, getSuppressed, ==, addMessageIfTimeout, getStackTraceDepth, clone, addSuppressed, awaitResult, toString, !=, getClass, RpcTimeoutException, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(duration, RpcTimeout, asInstanceOf, synchronized, isInstanceOf, getStackTrace, <init>, setStackTrace, apply, ==, clone, awaitResult, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(RpcTimeout, asInstanceOf, synchronized, isInstanceOf, <init>, getMessage, apply, ==, awaitResult, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(duration, RpcTimeout, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, awaitResult, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(RpcTimeout, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala: Set(RpcTimeout, asInstanceOf, isInstanceOf, <init>, ==, awaitResult, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(duration, RpcTimeout, asInstanceOf, synchronized, isInstanceOf, <init>, getMessage, apply, ==, addMessageIfTimeout, clone, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala: Set(RpcTimeout, <init>, awaitResult)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/RpcUtils.scala: Set(RpcTimeout, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(RpcTimeout, <init>, getMessage, apply, ==, awaitResult, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(RpcTimeout, wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, apply, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(RpcTimeout, asInstanceOf, isInstanceOf, <init>, apply, ==, awaitResult, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $assertionsDisabled, equals, getPartitionLengths, stop, notifyAll, <init>, toString, getClass, BypassMergeSortShuffleWriter, write, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(stop, <init>, getClass, BypassMergeSortShuffleWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	taskSucceeded, notify, jobFailed, wait, $asInstanceOf, equals, asInstanceOf, synchronized, JobListener, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala: Set(asInstanceOf, synchronized, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(taskSucceeded, jobFailed, asInstanceOf, synchronized, JobListener, isInstanceOf, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(taskSucceeded, jobFailed, asInstanceOf, synchronized, JobListener, isInstanceOf, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala: Set(asInstanceOf, JobListener, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(taskSucceeded, jobFailed, asInstanceOf, synchronized, JobListener, isInstanceOf, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, JobListener, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala: Set(wait, asInstanceOf, synchronized, JobListener, notifyAll, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala: Set(asInstanceOf, synchronized, JobListener, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TooLargePageException.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TooLargePageException.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TooLargePageException.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TooLargePageException.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TooLargePageException.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, equals, fillInStackTrace, initCause, getCause, notifyAll, getStackTrace, TooLargePageException, <init>, getMessage, setStackTrace, getSuppressed, addSuppressed, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TooLargePageException.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, setSerializer, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, prev, min, getCheckpointFile, ShuffledRDDPartition, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, setAggregator, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, setKeyOrdering, pipe$default$7, idx, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, setMapSideCombine, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, ShuffledRDD, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(creationSite, zipPartitions, parent, isLocallyCheckpointed, getOrCompute, partitioner, coalesce, name, count, isCheckpointedAndMaterialized, mapPartitions, union, map, subtract, intersection, scope, asInstanceOf, context, getPreferredLocations, doCheckpoint, synchronized, aggregate, compute, min, fold, getOutputDeterministicLevel, iterator, conf, isInstanceOf, filter, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, ++, flatMap, take, groupBy, ==, clone, distinct, retag, foreach, reduce, zipWithIndex, checkpoint, elementClassTag, sample, toString, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, ShuffledRDD, clearDependencies, isEmpty, ne, getDependencies, mapPartitionsWithIndex, keyBy, getCreationSite, computeOrReadCheckpoint, dependencies, isReliablyCheckpointed, withScope, log, treeAggregate, index, takeOrdered, firstParent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(setSerializer, partitioner, mapPartitions, map, asInstanceOf, context, iterator, conf, isInstanceOf, filter, <init>, setAggregator, flatMap, ==, foreach, sparkContext, reduce, toString, !=, partitions, collect, setMapSideCombine, logWarning, ShuffledRDD, isEmpty, ne, countByValueApprox, mapPartitionsWithIndex, withScope, log, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala: Set(partitioner, asInstanceOf, min, isInstanceOf, filter, <init>, max, setKeyOrdering, partitions, ShuffledRDD, ne, withScope)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	jobId, SparkJobInfo, status, stageIds.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(jobId, SparkJobInfo, status, stageIds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(jobId, SparkJobInfo, status, stageIds)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala: Set(jobId, SparkJobInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala: Set(jobId, SparkJobInfo, status, stageIds)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdReporter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, Set, getRateUnit, wait, convertRate, $asInstanceOf, StatsdMetricType, GAUGE, equals, asInstanceOf, convertDuration, initializeLogIfNecessary, synchronized, $isInstanceOf, TIMER, COUNTER, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, getDurationUnit, ==, clone, $init$, report, toString, logError, !=, StatsdReporter, getClass, logWarning, start, close, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala: Set(stop, <init>, report, StatsdReporter, start, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, find, span, flatten, toBuffer, count, reduceOption, sliceWithKnownDelta, wait, foldRight, takeWhile, stringPrefix, $asInstanceOf, splitAt, minBy, size, inits, zip, toSet, :\, map, takeRight, dropWhile, toMap, filterNot, equals, par, unzip3, repr, toList, isTraversableAgain, head, asInstanceOf, sameElements, unzip, reduceLeftOption, synchronized, sliding, atGrowThreshold, Self, partition, AppendOnlyMap, aggregate, $isInstanceOf, forall, newBuilder, mkString, min, scanRight, fold, scan, nonEmpty, canEqual, destructiveSortedIterator, tail, lastOption, dropRight, iterator, last, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, ++:, <init>, toStream, companion, max, tails, apply, ++, grouped, flatMap, take, parCombiner, reduceRight, groupBy, ==, maxBy, sliceWithKnownBound, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, changeValue, $init$, toSeq, zipWithIndex, growTable, toString, genericBuilder, copyToArray, seq, !=, transpose, collect, headOption, getClass, WithFilter, update, hasDefiniteSize, foldLeft, toCollection, isEmpty, ne, init, reversed, reduceLeft, eq, sum, thisCollection, ##, scanLeft, finalize, hashCode, zipAll, product, view.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala: Set(map, partition, destructiveSortedIterator, <init>, update)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(size, map, head, asInstanceOf, synchronized, mkString, nonEmpty, destructiveSortedIterator, iterator, last, <init>, apply, ++, ==, foreach, exists, changeValue, !=, getClass, update, isEmpty, ne, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(withFilter, flatten, size, zip, map, head, asInstanceOf, synchronized, mkString, iterator, isInstanceOf, filter, <init>, apply, ++, flatMap, ==, foreach, exists, toArray, changeValue, toString, !=, update, isEmpty, ne, eq, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedAppendOnlyMap.scala: Set(map, partition, destructiveSortedIterator, <init>, update)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(size, map, head, asInstanceOf, synchronized, mkString, nonEmpty, destructiveSortedIterator, iterator, last, <init>, apply, ++, ==, foreach, exists, changeValue, !=, getClass, update, isEmpty, ne, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingAppendOnlyMap.scala: Set(AppendOnlyMap, <init>, changeValue, growTable, update)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(withFilter, flatten, size, zip, map, head, asInstanceOf, synchronized, mkString, iterator, isInstanceOf, filter, <init>, apply, ++, flatMap, ==, foreach, exists, toArray, changeValue, toString, !=, update, isEmpty, ne, eq, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateEvaluator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, currentResult, notifyAll, isInstanceOf, ApproximateEvaluator, merge, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, ApproximateEvaluator, merge, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, ApproximateEvaluator, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala: Set(ApproximateEvaluator, merge, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ApproximateEvaluator, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala: Set(ApproximateEvaluator, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala: Set(ApproximateEvaluator, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala: Set(wait, asInstanceOf, synchronized, currentResult, notifyAll, ApproximateEvaluator, merge, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ApproximateEvaluator, merge, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala: Set(ApproximateEvaluator, merge, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ApproximateEvaluator, merge, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ApproximateEvaluator, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala: Set(ApproximateEvaluator, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, ApproximateEvaluator, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, ApproximateEvaluator, merge, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SizeEstimator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, SizeEstimator, estimatedSize, KnownSizeEstimation, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, estimate, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(SizeEstimator, asInstanceOf, synchronized, estimate, isInstanceOf, ==, toString, !=, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTracker.scala: Set(SizeEstimator, asInstanceOf, estimate, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/KVUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, mapper, equals, fillInStackTrace, initCause, deserializeLong, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, getCause, logTrace, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, open, KVIndexParam, logName, notifyAll, isInstanceOf, getStackTrace, KVStoreScalaSerializer, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, viewToSeq, getStackTraceDepth, clone, addSuppressed, $init$, MetadataMismatchException, toString, logError, !=, KVUtils, getClass, logWarning, ne, serialize, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, KVUtils, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(initCause, asInstanceOf, synchronized, getCause, open, isInstanceOf, <init>, ==, clone, toString, logError, !=, KVUtils, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, ==, viewToSeq, toString, !=, KVUtils, getClass, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, KVUtils, getClass, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ord.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/MedianHeap.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, securityMgr, wait, onNetworkError, Master, $asInstanceOf, electedLeader, equals, reverseProxy, asInstanceOf, initializeLogIfNecessary, workers, synchronized, self, $isInstanceOf, SYSTEM_NAME, receive, logTrace, isTraceEnabled, main, initializeLogIfNecessary$default$2, stop, logName, notifyAll, conf, idToApp, startRpcEnvAndEndpoint, isInstanceOf, removeApplication, <init>, onError, ENDPOINT_NAME, ==, receiveAndReply, clone, revokedLeadership, $init$, onDisconnected, toString, logError, !=, onConnected, getClass, logWarning, onStop, ne, onStart, rpcEnv, eq, log, ##, finalize, hashCode, apps, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(securityMgr, Master, reverseProxy, workers, idToApp, removeApplication, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(Master, conf, startRpcEnvAndEndpoint, <init>, clone, ne, rpcEnv, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(securityMgr, Master, workers, self, conf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(securityMgr, Master, reverseProxy, asInstanceOf, synchronized, self, SYSTEM_NAME, stop, conf, startRpcEnvAndEndpoint, isInstanceOf, <init>, ENDPOINT_NAME, ==, toString, logError, !=, logWarning, ne, rpcEnv, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(Master, asInstanceOf, self, stop, conf, isInstanceOf, <init>, ENDPOINT_NAME, ==, !=, logWarning, ne, rpcEnv, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(Master, asInstanceOf, self, conf, isInstanceOf, <init>, ENDPOINT_NAME, ==, logError, logWarning, ne, rpcEnv, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(Master, reverseProxy, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala: Set(Master, workers, <init>, ==, apps)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TestUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getKind, name, toUri, wait, getNestingKind, $asInstanceOf, isNameCompatible, createJarWithClasses, createJarWithClasses$default$2, equals, createCompiledClass$default$3, openInputStream, createJarWithFiles$default$2, asInstanceOf, uri, synchronized, createJarWithClasses$default$3, openWriter, $isInstanceOf, getCharContent, notifyAll, getName, createCompiledClass$default$4, code, waitUntilExecutorsUp, isInstanceOf, createJarWithClasses$default$4, httpResponseCode$default$2, <init>, createCompiledClass, openOutputStream, assertNotSpilled, ==, clone, assertSpilled, JavaSourceFromString, delete, createJar$default$3, getLastModified, TestUtils, toString, httpResponseCode$default$3, !=, createCompiledClass$default$5, httpResponseCode, getClass, kind, testCommandAvailable, createJar, createJarWithFiles, ne, openReader, getAccessLevel, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, stageAttemptId, _executorDeserializeTime, wait, collectAccumulatorUpdates$default$1, $asInstanceOf, epoch, ShuffleMapTask, equals, stageId, asInstanceOf, context, initializeLogIfNecessary, collectAccumulatorUpdates, run, synchronized, $isInstanceOf, logTrace, reasonIfKilled, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, partitionId, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, <init>, ==, appAttemptId, clone, setTaskMemoryManager, jobId, $init$, toString, preferredLocations, metrics, logError, !=, getClass, logWarning, appId, runTask, localProperties, kill, _executorDeserializeCpuTime, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(stageAttemptId, epoch, ShuffleMapTask, stageId, asInstanceOf, synchronized, logTrace, partitionId, isInstanceOf, <init>, ==, clone, jobId, toString, preferredLocations, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(stageAttemptId, epoch, ShuffleMapTask, stageId, asInstanceOf, synchronized, partitionId, isInstanceOf, <init>, ==, toString, preferredLocations, logError, !=, logWarning, localProperties, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, path, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, inputFormatClazz, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, computePreferredLocations, <init>, ==, clone, InputFormatInfo, $init$, configuration, toString, logError, !=, getClass, logWarning, mapredInputFormat, ne, mapreduceInputFormat, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, write$default$3, asInstanceOf, lastWriteTime, synchronized, $isInstanceOf, doUpdate, notifyAll, isInstanceOf, LiveEntity, <init>, ==, clone, toString, !=, getClass, ne, eq, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, lastWriteTime, isInstanceOf, LiveEntity, <init>, ==, toString, !=, getClass, ne, write)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, RUtils, isSparkRInstalled, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, rPackages, toString, isRInstalled, !=, getClass, ne, localSparkRPackagePath, eq, ##, sparkRPackagePath, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(RUtils, asInstanceOf, isInstanceOf, ==, rPackages, toString, !=, getClass, ne, localSparkRPackagePath, eq, sparkRPackagePath)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(RUtils, ==, toString, !=, ne, sparkRPackagePath)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala: Set(RUtils, ==, rPackages, toString, !=, sparkRPackagePath)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(RUtils, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, sparkRPackagePath)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(RUtils, asInstanceOf, ==, rPackages, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, fetchFailed, getLocalProperty, isCompleted, wait, addTaskFailureListener, taskAttemptId, $asInstanceOf, markTaskCompleted, equals, taskMemoryManager, taskMetrics, stageId, setFetchFailed, asInstanceOf, initializeLogIfNecessary, synchronized, addTaskCompletionListener, $isInstanceOf, killTaskIfInterrupted, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, stageAttemptNumber, partitionId, isInterrupted, isInstanceOf, <init>, getMetricsSources, ==, clone, $init$, registerAccumulator, getKillReason, markInterrupted, toString, attemptNumber, markTaskFailed, isRunningLocally, logError, !=, getClass, logWarning, ne, eq, log, TaskContextImpl, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(addTaskFailureListener, addTaskCompletionListener, partitionId, <init>, eq, TaskContextImpl)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(getLocalProperty, equals, stageId, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, ==, toString, markTaskFailed, logError, !=, getClass, logWarning, ne, eq, log, TaskContextImpl, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(fetchFailed, wait, taskMemoryManager, asInstanceOf, synchronized, killTaskIfInterrupted, notifyAll, isInstanceOf, <init>, ==, attemptNumber, logError, !=, logWarning, ne, eq, TaskContextImpl, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(taskAttemptId, markTaskCompleted, taskMemoryManager, taskMetrics, stageId, synchronized, notifyAll, partitionId, <init>, markInterrupted, attemptNumber, markTaskFailed, !=, TaskContextImpl)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockNotFoundException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockNotFoundException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockNotFoundException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockNotFoundException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockNotFoundException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, BlockNotFoundException, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, getMessage, ==, BlockNotFoundException, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Clock.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Clock.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Clock.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Clock.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Clock.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Clock.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, Clock, notifyAll, getTimeMillis, isInstanceOf, <init>, ==, clone, SystemClock, toString, minPollTime, !=, getClass, ne, waitTillTime, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, Clock, getTimeMillis, isInstanceOf, <init>, ==, clone, SystemClock, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, Clock, getTimeMillis, isInstanceOf, <init>, ==, SystemClock, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, Clock, isInstanceOf, <init>, SystemClock, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, Clock, getTimeMillis, isInstanceOf, <init>, ==, SystemClock, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala: Set(asInstanceOf, Clock, getTimeMillis, isInstanceOf, <init>, ==, SystemClock, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetBlacklist.scala: Set(Clock, getTimeMillis, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, Clock, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(asInstanceOf, Clock, getTimeMillis, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(asInstanceOf, synchronized, Clock, getTimeMillis, isInstanceOf, <init>, ==, clone, SystemClock, toString, !=, getClass, ne, waitTillTime, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala: Set(asInstanceOf, synchronized, Clock, getTimeMillis, isInstanceOf, <init>, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, Clock, getTimeMillis, isInstanceOf, <init>, ==, SystemClock, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(synchronized, Clock, getTimeMillis, <init>, ==, SystemClock, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala: Set(wait, synchronized, Clock, notifyAll, getTimeMillis, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	numAccums, notify, register, register$default$2, name, count, wait, _value, copy$default$2, $asInstanceOf, CollectionAccumulator, productArity, countFailedValues, equals, isZero, clear, asInstanceOf, SQL_ACCUM_IDENTIFIER, initializeLogIfNecessary, AccumulatorV2, register$default$3, synchronized, avg, $isInstanceOf, logTrace, copyAndReset, LegacyAccumulatorWrapper, canEqual, toInfo, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, isInstanceOf, setValue, <init>, merge, id, remove, DoubleAccumulator, newId, ==, clone, LongAccumulator, $init$, isAtDriverSide, writeReplace, copy$default$3, copy, reset, metadata, toString, logError, !=, isRegistered, get, getClass, logWarning, copy$default$1, AccumulatorMetadata, ne, add, value, eq, productIterator, sum, log, AccumulatorContext, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(name, isZero, clear, asInstanceOf, AccumulatorV2, synchronized, logTrace, toInfo, isInstanceOf, <init>, merge, id, remove, ==, clone, toString, logError, !=, get, logWarning, ne, add, value, AccumulatorContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(name, asInstanceOf, AccumulatorV2, synchronized, isInstanceOf, <init>, id, remove, ==, toString, logError, !=, get, logWarning, ne, add, value, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(register, name, CollectionAccumulator, asInstanceOf, AccumulatorV2, synchronized, LegacyAccumulatorWrapper, isInstanceOf, <init>, id, remove, DoubleAccumulator, ==, clone, LongAccumulator, toString, logError, !=, get, getClass, logWarning, ne, value, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, <init>, id, remove, ==, logError, !=, get, logWarning, ne, add, value, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala: Set(setValue, <init>, LongAccumulator, add, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala: Set(asInstanceOf, AccumulatorV2, isInstanceOf, <init>, ==, toString, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala: Set(setValue, <init>, LongAccumulator, add, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContextImpl.scala: Set(name, AccumulatorV2, synchronized, <init>, logError, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(name, count, SQL_ACCUM_IDENTIFIER, <init>, id, remove, ==, metadata, toString, !=, get, value, eq, AccumulatorContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala: Set(AccumulatorV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/OutputMetrics.scala: Set(setValue, <init>, LongAccumulator, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(register, name, CollectionAccumulator, isZero, asInstanceOf, AccumulatorV2, synchronized, setValue, <init>, merge, newId, ==, LongAccumulator, metadata, get, AccumulatorMetadata, ne, add, value, sum, AccumulatorContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(asInstanceOf, AccumulatorV2, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/InputMetrics.scala: Set(setValue, <init>, LongAccumulator, add, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, CollectionAccumulator, asInstanceOf, AccumulatorV2, synchronized, logTrace, isInstanceOf, <init>, merge, id, ==, toString, !=, get, getClass, logWarning, ne, value, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(name, wait, asInstanceOf, AccumulatorV2, synchronized, toInfo, isInstanceOf, <init>, id, remove, ==, toString, logError, !=, get, logWarning, ne, add, value, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(name, wait, asInstanceOf, AccumulatorV2, synchronized, toInfo, notifyAll, isInstanceOf, <init>, remove, ==, logError, !=, get, logWarning, ne, value, eq, sum, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskContext.scala: Set(AccumulatorV2, <init>, remove, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(countFailedValues, AccumulatorV2, synchronized, notifyAll, <init>, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, AccumulatorV2, synchronized, isInstanceOf, <init>, id, remove, ==, toString, logError, add, eq, AccumulatorContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(name, asInstanceOf, AccumulatorV2, isInstanceOf, setValue, <init>, ==, LongAccumulator, logError, !=, get, ne, value, sum, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala: Set(asInstanceOf, AccumulatorV2, isInstanceOf, <init>, ==, toString, getClass, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(register, name, _value, countFailedValues, clear, asInstanceOf, AccumulatorV2, LegacyAccumulatorWrapper, setValue, <init>, id, newId, ==, isAtDriverSide, copy, toString, AccumulatorMetadata, add, value, AccumulatorContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, AccumulatorV2, logTrace, isInstanceOf, <init>, remove, ==, toString, !=, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(name, countFailedValues, asInstanceOf, AccumulatorV2, toInfo, isInstanceOf, <init>, id, ==, metadata, toString, !=, get, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, logTrace, isInstanceOf, <init>, id, remove, ==, logError, !=, get, logWarning, ne, add, value, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(register, name, clear, asInstanceOf, isInstanceOf, <init>, ==, reset, logError, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(name, asInstanceOf, <init>, ==, toString, !=, get, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, getConf, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, mapPartitionsWithInputSplit$default$2, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, jobId, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, BinaryFileRDD, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, hashCode, takeOrdered, logDebug, mapPartitionsWithInputSplit, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, getConf, parent, partitioner, name, setName, union, map, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, <init>, id, max, toDebugString, ++, flatMap, ==, clone, foreach, jobId, zipWithIndex, first, toString, logError, !=, partitions, collect, getClass, BinaryFileRDD, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, taskCount, stageData, wait, $asInstanceOf, rddList, listener, equals, operationGraphForStage, asInstanceOf, stageAttempt, synchronized, $isInstanceOf, job, taskList, executorSummary, store, executorList, stageAttempt$default$3, notifyAll, localitySummary, isInstanceOf, rdd, lastStageAttempt, <init>, asOption, createLiveStore, operationGraphForJob, ==, jobsList, clone, rddList$default$1, appSummary, pool, AppStatusStore, toString, streamBlocksList, stageList, !=, getClass, CURRENT_VERSION, taskSummary, close, applicationInfo, activeStages, ne, environmentInfo, <init>$default$2, eq, ##, stageData$default$2, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(listener, asInstanceOf, synchronized, job, isInstanceOf, rdd, <init>, createLiveStore, ==, clone, pool, AppStatusStore, toString, !=, getClass, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala: Set(synchronized, <init>, ==, AppStatusStore, !=, activeStages)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(job, store, executorList, isInstanceOf, <init>, ==, jobsList, appSummary, AppStatusStore, toString, !=, applicationInfo, ne, environmentInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala: Set(stageData, rddList, asInstanceOf, store, isInstanceOf, rdd, <init>, ==, AppStatusStore, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala: Set(store, <init>, AppStatusStore)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala: Set(rddList, job, store, executorList, isInstanceOf, rdd, <init>, ==, jobsList, AppStatusStore, !=, close, environmentInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(store, <init>, ==, AppStatusStore, applicationInfo, environmentInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala: Set(stageData, stageAttempt, taskList, store, <init>, AppStatusStore, stageList, taskSummary)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(executorSummary, store, <init>, asOption, AppStatusStore, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(rddList, store, rdd, <init>, ==, AppStatusStore, toString, streamBlocksList, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(taskCount, stageData, operationGraphForStage, asInstanceOf, stageAttempt, job, taskList, executorSummary, store, localitySummary, isInstanceOf, <init>, asOption, ==, AppStatusStore, toString, !=, taskSummary, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala: Set(store, <init>, AppStatusStore, environmentInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(asInstanceOf, store, executorList, isInstanceOf, rdd, <init>, ==, AppStatusStore, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(store, <init>, asOption, ==, appSummary, pool, AppStatusStore, stageList, activeStages)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(listener, asInstanceOf, synchronized, store, isInstanceOf, <init>, ==, clone, pool, AppStatusStore, toString, !=, getClass, CURRENT_VERSION, close, applicationInfo, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(store, lastStageAttempt, <init>, asOption, pool, AppStatusStore, activeStages)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(job, store, executorList, lastStageAttempt, <init>, asOption, ==, jobsList, AppStatusStore, stageList, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(asInstanceOf, job, store, executorList, isInstanceOf, lastStageAttempt, <init>, asOption, operationGraphForJob, ==, AppStatusStore, toString, !=, applicationInfo, activeStages)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(job, store, <init>, asOption, ==, AppStatusStore, toString, environmentInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(store, lastStageAttempt, <init>, asOption, ==, AppStatusStore, toString, environmentInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	removeRdd, notify, wait, $asInstanceOf, removeBroadcast, equals, getLocationsAndStatus, getMatchingBlockIds, registerBlockManager, asInstanceOf, initializeLogIfNecessary, removeExecutorAsync, synchronized, getMemoryStatus, $isInstanceOf, removeBlock, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, removeExecutor, stop, logName, notifyAll, isInstanceOf, <init>, DRIVER_ENDPOINT_NAME, updateBlockInfo, getLocations, hasCachedBlocks, getPeers, ==, clone, $init$, getExecutorEndpointRef, toString, logError, driverEndpoint, !=, getBlockStatus, getClass, logWarning, getBlockStatus$default$2, contains, ne, BlockManagerMaster, timeout, getStorageStatus, eq, log, removeShuffle, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, logTrace, removeExecutor, stop, isInstanceOf, <init>, getLocations, ==, clone, toString, logError, driverEndpoint, !=, logWarning, contains, ne, BlockManagerMaster, timeout, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, removeExecutorAsync, synchronized, removeExecutor, stop, isInstanceOf, <init>, ==, toString, logError, driverEndpoint, !=, logWarning, contains, ne, BlockManagerMaster, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(removeRdd, asInstanceOf, synchronized, getMemoryStatus, stop, isInstanceOf, <init>, ==, clone, getExecutorEndpointRef, toString, logError, !=, getClass, logWarning, contains, ne, BlockManagerMaster, timeout, getStorageStatus, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSource.scala: Set(<init>, BlockManagerMaster, getStorageStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, hasCachedBlocks, ==, !=, logWarning, contains, ne, BlockManagerMaster, timeout, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, <init>, DRIVER_ENDPOINT_NAME, ==, toString, !=, getClass, logWarning, contains, ne, BlockManagerMaster, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(removeBroadcast, asInstanceOf, synchronized, isInstanceOf, <init>, ==, !=, ne, BlockManagerMaster, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(getLocationsAndStatus, registerBlockManager, asInstanceOf, synchronized, removeBlock, logTrace, stop, isInstanceOf, <init>, updateBlockInfo, getLocations, getPeers, ==, toString, logError, !=, getClass, logWarning, contains, ne, BlockManagerMaster, timeout, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(<init>, contains, BlockManagerMaster)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, removeBlock, isInstanceOf, <init>, ==, toString, BlockManagerMaster)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, logError, BlockManagerMaster, eq, removeShuffle, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, removeBlock, isInstanceOf, <init>, ==, logError, !=, ne, BlockManagerMaster, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ManualClock.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, setTime, wait, $asInstanceOf, ManualClock, equals, asInstanceOf, synchronized, $isInstanceOf, advance, notifyAll, getTimeMillis, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne, waitTillTime, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, prefix, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, renderLog, $init$, renderJson, toString, logError, !=, LogPage, getClass, logWarning, render, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(<init>, renderLog, LogPage)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, productArity, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, exitCode, $isInstanceOf, getCause, canEqual, productPrefix, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, SparkException, getMessage, setStackTrace, getSuppressed, SparkDriverExecutionException, ==, getStackTraceDepth, clone, addSuppressed, $init$, copy, toString, SparkUserAppException, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/PythonRunner.scala: Set(exitCode, <init>, ==, copy, SparkUserAppException, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, getStackTrace, <init>, SparkException, setStackTrace, SparkDriverExecutionException, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, getMessage, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, clone, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala: Set(synchronized, getStackTrace, <init>, SparkException, setStackTrace, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, getMessage, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, getMessage, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/package.scala: Set(<init>, SparkException, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(printStackTrace, equals, initCause, asInstanceOf, synchronized, exitCode, getCause, isInstanceOf, getStackTrace, <init>, SparkException, getMessage, setStackTrace, ==, addSuppressed, copy, toString, !=, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(printStackTrace, asInstanceOf, exitCode, getCause, isInstanceOf, getStackTrace, <init>, SparkException, getMessage, ==, toString, SparkUserAppException, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(<init>, SparkException, ==, toString, SparkUserAppException, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, getMessage, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, SparkException, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala: Set(<init>, SparkException)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CommandLineUtils.scala: Set(exitCode, <init>, SparkException, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, getMessage, ==, copy, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala: Set(asInstanceOf, isInstanceOf, getStackTrace, <init>, SparkException, setStackTrace, ==, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointNotFoundException.scala: Set(<init>, SparkException)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RUtils.scala: Set(<init>, SparkException, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(<init>, SparkException)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointRef.scala: Set(<init>, SparkException)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala: Set(asInstanceOf, <init>, SparkException, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockId.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpointAddress.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(<init>, SparkException, getMessage, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala: Set(<init>, SparkException, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, toString, !=, getClass, ne, eq, finalize)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, SparkException, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(initCause, asInstanceOf, synchronized, getCause, isInstanceOf, <init>, SparkException, ==, clone, copy, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala: Set(equals, asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, getMessage, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala: Set(synchronized, <init>, SparkException, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/FutureAction.scala: Set(initCause, asInstanceOf, synchronized, isInstanceOf, <init>, SparkException)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, SparkException, getMessage, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, clone, copy, toString, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, getMessage, ==, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, getMessage, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, getStackTrace, <init>, SparkException, setStackTrace, SparkDriverExecutionException, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(<init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMessages.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockException.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala: Set(asInstanceOf, isInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskStore.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/BlockDataManager.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala: Set(synchronized, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asInstanceOf, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockInfoManager.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, SparkException, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/ShuffleBlockResolver.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, getMessage, ==, copy, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(asInstanceOf, synchronized, <init>, ==, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/UnifiedMemoryManager.scala: Set(synchronized, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalCheckpointRDD.scala: Set(<init>, SparkException)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/TaskMetrics.scala: Set(asInstanceOf, synchronized, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(<init>, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockUpdatedInfo.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMaster.scala: Set(<init>, SparkException, getMessage, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockReplicationPolicy.scala: Set(<init>, ==, !=, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StaticMemoryManager.scala: Set(synchronized, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/StorageMemoryPool.scala: Set(synchronized, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, SparkException, ==, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/BlockRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkException, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SparkException, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala: Set(synchronized, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, SparkException, getMessage, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, copy, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala: Set(asInstanceOf, isInstanceOf, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala: Set(synchronized, <init>, clone, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(asInstanceOf, isInstanceOf, getStackTrace, <init>, getMessage, setStackTrace, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, getMessage, ==, clone, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, GetExecutorLossReason, unapply, curried, Shutdown, CoarseGrainedClusterMessages, executorIds, RemoveWorker, StopDriver, wait, StopExecutor, localityAwareTasks, copy$default$2, $asInstanceOf, hostname, am, copy$default$5, compose, SetupDriver, CoarseGrainedClusterMessage, state, executorRef, productArity, equals, SparkAppConfig, RegisterClusterManager, cores, asInstanceOf, UpdateDelegationTokens, ReviveOffers, host, filterName, data, synchronized, RetrieveLastAllocatedExecutorId, $isInstanceOf, andThen, logUrls, tupled, interruptThread, canEqual, copy$default$4, hadoopDelegationCreds, executorId, RegisterExecutor, productPrefix, reason, RequestExecutors, notifyAll, isInstanceOf, StatusUpdate, RegisteredExecutor, RetrieveSparkAppConfig, <init>, filterParams, driver, AddWebUIFilter, apply, requestedTotal, ==, clone, $init$, copy$default$3, proxyBase, copy, workerId, message, tokens, toString, RemoveExecutor, StopExecutors, !=, hostToLocalTaskCount, getClass, copy$default$1, ioEncryptionKey, KillExecutorsOnHost, KillExecutors, executor, ne, nodeBlacklist, eq, productIterator, taskId, RegisterExecutorFailed, sparkProperties, ##, finalize, productElement, hashCode, LaunchTask, RegisterExecutorResponse, KillTask.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(CoarseGrainedClusterMessages, executorIds, RemoveWorker, StopDriver, StopExecutor, localityAwareTasks, hostname, state, executorRef, SparkAppConfig, cores, asInstanceOf, UpdateDelegationTokens, ReviveOffers, host, data, synchronized, logUrls, interruptThread, executorId, RegisterExecutor, reason, isInstanceOf, StatusUpdate, RegisteredExecutor, RetrieveSparkAppConfig, <init>, apply, ==, workerId, message, tokens, toString, RemoveExecutor, StopExecutors, !=, hostToLocalTaskCount, KillExecutorsOnHost, executor, ne, nodeBlacklist, taskId, RegisterExecutorFailed, sparkProperties, LaunchTask, KillTask)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(unapply, Shutdown, CoarseGrainedClusterMessages, StopExecutor, hostname, state, SparkAppConfig, cores, asInstanceOf, UpdateDelegationTokens, data, interruptThread, hadoopDelegationCreds, executorId, RegisterExecutor, reason, isInstanceOf, StatusUpdate, RegisteredExecutor, RetrieveSparkAppConfig, <init>, driver, apply, ==, message, tokens, RemoveExecutor, !=, ioEncryptionKey, executor, ne, taskId, RegisterExecutorFailed, sparkProperties, LaunchTask, KillTask)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	mapToConf, JavaToWritableConverter, notify, getInstance, wait, Converter, $asInstanceOf, equals, convertRDD, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, WritableToJavaConverter, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, mergeConfs, eq, convert, log, ##, PythonHadoopUtil, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(Converter, asInstanceOf, isInstanceOf, <init>, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(mapToConf, JavaToWritableConverter, getInstance, wait, Converter, convertRDD, asInstanceOf, synchronized, logTrace, WritableToJavaConverter, isInstanceOf, <init>, ==, toString, !=, getClass, logWarning, ne, mergeConfs, eq, convert, PythonHadoopUtil, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, getRecordLength, $assertionsDisabled, equals, UnsafeInMemorySorter, numRecords, insertRecord, notifyAll, <init>, hasSpaceForAnotherRecord, access$100, this$0, clone, getKeyPrefix, access$000, getCurrentPageNumber, reset, toString, loadNext, getBaseOffset, getBaseObject, UnsafeInMemorySorter$SortedIterator, getClass, expandPointerArray, getMemoryUsage, getNumRecords, hasNext, getSortedIterator, getSortTimeNanos, free, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcCallContext.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, reply, notifyAll, isInstanceOf, ==, RpcCallContext, clone, sendFailure, toString, !=, getClass, senderAddress, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/RpcEndpointVerifier.scala: Set(asInstanceOf, reply, isInstanceOf, ==, RpcCallContext, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, reply, isInstanceOf, ==, RpcCallContext, toString, !=, senderAddress, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, reply, isInstanceOf, ==, RpcCallContext, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(asInstanceOf, synchronized, reply, isInstanceOf, ==, RpcCallContext, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, reply, isInstanceOf, ==, RpcCallContext, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala: Set(asInstanceOf, reply, isInstanceOf, ==, RpcCallContext, toString, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/RpcEndpoint.scala: Set(asInstanceOf, RpcCallContext, sendFailure, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, RpcCallContext, sendFailure, toString, !=, senderAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala: Set(reply, RpcCallContext, senderAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(asInstanceOf, reply, isInstanceOf, ==, RpcCallContext, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(asInstanceOf, reply, isInstanceOf, ==, RpcCallContext, sendFailure, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, reply, isInstanceOf, ==, RpcCallContext, sendFailure, !=, senderAddress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, synchronized, reply, notifyAll, isInstanceOf, ==, RpcCallContext, toString, !=, senderAddress, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(asInstanceOf, reply, isInstanceOf, ==, RpcCallContext, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, RpcCallContext, sendFailure, toString, !=, senderAddress, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, !=, senderAddress, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/StatCounter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/StatCounter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/StatCounter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/StatCounter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/StatCounter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, count, wait, $asInstanceOf, variance, equals, asInstanceOf, stdev, synchronized, $isInstanceOf, mean, min, popVariance, popStdev, notifyAll, isInstanceOf, <init>, merge, max, apply, ==, clone, copy, toString, !=, getClass, sampleVariance, ne, StatCounter, eq, sum, ##, finalize, sampleStdev, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/MeanEvaluator.scala: Set(count, stdev, mean, <init>, merge, ==, sampleVariance, StatCounter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Distribution.scala: Set(min, <init>, apply, StatCounter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(variance, stdev, mean, min, popVariance, popStdev, <init>, max, apply, sampleVariance, StatCounter, sum, sampleStdev)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/StatsReportListener.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne, StatCounter, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala: Set(count, variance, asInstanceOf, stdev, mean, min, popVariance, popStdev, isInstanceOf, <init>, merge, max, apply, ==, !=, sampleVariance, ne, StatCounter, sampleStdev)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/SumEvaluator.scala: Set(count, mean, <init>, merge, ==, sampleVariance, StatCounter, sum)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/EnumUtil.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/EnumUtil.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/EnumUtil.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/EnumUtil.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/EnumUtil.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, parseIgnoreCase, wait, equals, EnumUtil, notifyAll, <init>, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/ApplicationStatus.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/EnumUtil.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/status/api/v1/TaskSorting.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, UnsafeSorterSpillWriter, notifyAll, <init>, getFile, toString, getClass, recordsSpilled, getReader, close, write, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, handleAppKillRequest, equals, prefix, asInstanceOf, synchronized, $isInstanceOf, MasterPage, notifyAll, isInstanceOf, <init>, ==, clone, getMasterState, handleDriverKillRequest, renderJson, toString, !=, getClass, render, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(handleAppKillRequest, MasterPage, <init>, ==, handleDriverKillRequest)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	classTag.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, rdd2, mapPartitions$default$2, min, getCheckpointFile, fold, rdd1, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, SubtractedRDD, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(partitioner, mapPartitions, map, asInstanceOf, context, iterator, conf, isInstanceOf, filter, <init>, flatMap, ==, foreach, sparkContext, reduce, SubtractedRDD, toString, !=, partitions, collect, logWarning, isEmpty, ne, countByValueApprox, mapPartitionsWithIndex, withScope, log)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, SparkUncaughtExceptionHandler, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, uncaughtException, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, exitOnUncaughtException, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(SparkUncaughtExceptionHandler, equals, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, ==, uncaughtException, toString, logError, !=, getClass, logWarning, ne, eq, log, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(SparkUncaughtExceptionHandler, wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, ==, uncaughtException, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(SparkUncaughtExceptionHandler, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(SparkUncaughtExceptionHandler, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	blockUpdatedInfoToJson, taskEndReasonToJson, notify, exceptionFromJson, UUIDFromJson, blockManagerAddedToJson, jobResultToJson, stackTraceToJson, wait, applicationStartToJson, logStartFromJson, $asInstanceOf, jobEndToJson, jobStartFromJson, applicationStartFromJson, executorRemovedFromJson, stageInfoToJson, equals, asInstanceOf, executorRemovedToJson, blockManagerIdToJson, stageSubmittedToJson, blockManagerAddedFromJson, synchronized, propertiesToJson, applicationEndFromJson, $isInstanceOf, applicationEndToJson, taskMetricsToJson, taskInfoToJson, blockStatusToJson, blockManagerRemovedFromJson, accumulablesToJson, logStartToJson, stageCompletedFromJson, storageLevelToJson, taskInfoFromJson, propertiesFromJson, notifyAll, executorMetricsUpdateFromJson, stageInfoFromJson, environmentUpdateToJson, isInstanceOf, stackTraceFromJson, unpersistRDDToJson, taskGettingResultToJson, executorMetricsUpdateToJson, stageCompletedToJson, exceptionToJson, jobStartToJson, taskEndReasonFromJson, ==, jobResultFromJson, sparkEventToJson, clone, storageLevelFromJson, jobEndFromJson, JsonProtocol, sparkEventFromJson, blockManagerIdFromJson, UUIDToJson, blockStatusFromJson, taskEndToJson, stageSubmittedFromJson, mapToJson, executorAddedFromJson, rddInfoFromJson, blockUpdateFromJson, toString, unpersistRDDFromJson, taskGettingResultFromJson, !=, getClass, accumValueFromJson, rddInfoToJson, taskEndFromJson, mapFromJson, accumulableInfoToJson, blockUpdateToJson, blockManagerRemovedToJson, accumValueToJson, executorInfoFromJson, ne, accumulableInfoFromJson, blockUpdatedInfoFromJson, taskMetricsFromJson, eq, taskStartFromJson, environmentUpdateFromJson, taskStartToJson, ##, finalize, hashCode, executorAddedToJson, executorInfoToJson.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala: Set(asInstanceOf, isInstanceOf, JsonProtocol, sparkEventFromJson, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(asInstanceOf, logStartToJson, isInstanceOf, ==, sparkEventToJson, JsonProtocol, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/Broadcast.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unpersist, wait, $asInstanceOf, doDestroy, isValid, equals, asInstanceOf, initializeLogIfNecessary, Broadcast, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, doUnpersist, assertValid, <init>, id, destroy, ==, clone, $init$, toString, logError, !=, getClass, getValue, logWarning, ne, value, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala: Set(unpersist, Broadcast, <init>, id)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, Broadcast, synchronized, logTrace, isInstanceOf, <init>, id, ==, clone, toString, logError, !=, logWarning, ne, value, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(asInstanceOf, Broadcast, <init>, ==, !=, ne, value, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, Broadcast, synchronized, isInstanceOf, <init>, id, ==, clone, toString, logError, !=, getClass, logWarning, ne, value, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(asInstanceOf, Broadcast, isInstanceOf, <init>, id, toString, !=, logWarning, ne, value, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala: Set(asInstanceOf, Broadcast, logTrace, isInstanceOf, <init>, id, ==, logError, !=, logWarning, ne, value, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(unpersist, asInstanceOf, Broadcast, synchronized, isInstanceOf, assertValid, <init>, id, ==, !=, getValue, ne, value, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(Broadcast, synchronized, <init>, id)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(asInstanceOf, Broadcast, synchronized, isInstanceOf, <init>, id, ==, toString, logError, !=, value, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala: Set(unpersist, Broadcast, <init>, id)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, Broadcast, <init>, ==, toString, !=, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(equals, asInstanceOf, Broadcast, synchronized, isInstanceOf, <init>, id, ==, toString, !=, logWarning, value, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(asInstanceOf, Broadcast, <init>, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, asInstanceOf, Broadcast, synchronized, logTrace, isInstanceOf, <init>, id, ==, toString, !=, getClass, logWarning, ne, value, eq, finalize, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(asInstanceOf, Broadcast, <init>, ==, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala: Set(asInstanceOf, Broadcast, synchronized, isInstanceOf, <init>, id, ==, toString, logError, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala: Set(Broadcast)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/MapOutputTracker.scala: Set(wait, asInstanceOf, Broadcast, synchronized, notifyAll, isInstanceOf, <init>, id, destroy, ==, toString, logError, !=, ne, value, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(equals, asInstanceOf, Broadcast, synchronized, isInstanceOf, <init>, id, ==, toString, !=, logWarning, value, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala: Set(asInstanceOf, Broadcast, isInstanceOf, <init>, ==, clone, toString, logError, getClass, getValue, ne, value, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/TaskDetailsClassNames.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/TaskDetailsClassNames.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/TaskDetailsClassNames.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/TaskDetailsClassNames.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/TaskDetailsClassNames.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, PEAK_EXECUTION_MEMORY, synchronized, $isInstanceOf, SHUFFLE_READ_REMOTE_SIZE, notifyAll, SHUFFLE_READ_BLOCKED_TIME, isInstanceOf, TASK_DESERIALIZATION_TIME, ==, clone, RESULT_SERIALIZATION_TIME, toString, !=, SCHEDULER_DELAY, GETTING_RESULT_TIME, getClass, ne, eq, ##, TaskDetailsClassNames, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(asInstanceOf, PEAK_EXECUTION_MEMORY, SHUFFLE_READ_REMOTE_SIZE, SHUFFLE_READ_BLOCKED_TIME, isInstanceOf, TASK_DESERIALIZATION_TIME, ==, RESULT_SERIALIZATION_TIME, toString, !=, SCHEDULER_DELAY, GETTING_RESULT_TIME, ne, TaskDetailsClassNames)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, getGroups, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, ShellBasedGroupsMappingProvider, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/coalesce-public.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, prefLoc, coalesce, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, PartitionGroup, <init>, ==, clone, toString, !=, partitions, getClass, PartitionCoalescer, ne, numPartitions, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(coalesce, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, partitions, getClass, PartitionCoalescer, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala: Set(coalesce, asInstanceOf, <init>, toString, PartitionCoalescer, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(coalesce, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, partitions, getClass, PartitionCoalescer, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(coalesce, <init>, PartitionCoalescer, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(coalesce, asInstanceOf, <init>, PartitionCoalescer, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoalescedRDD.scala: Set(prefLoc, coalesce, asInstanceOf, isInstanceOf, PartitionGroup, <init>, ==, toString, partitions, PartitionCoalescer, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SequenceFileRDDFunctions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, SequenceFileRDDFunctions, wait, $asInstanceOf, equals, saveAsSequenceFile$default$2, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, saveAsSequenceFile, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(SequenceFileRDDFunctions, asInstanceOf, isInstanceOf, <init>, ==, toString, getClass, ne, saveAsSequenceFile, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(SequenceFileRDDFunctions, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, logWarning, ne, saveAsSequenceFile, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	finishedExecutors, coresUsed, notify, drivers, securityMgr, wait, onNetworkError, isUseLocalNodeSSLConfig, $asInstanceOf, memoryUsed, startRpcEnvAndEndpoint$default$8, equals, finishedDrivers, memoryFree, reverseProxy, asInstanceOf, initializeLogIfNecessary, synchronized, maybeUpdateSSLSettings, executors, self, <init>$default$7, $isInstanceOf, SYSTEM_NAME, receive, activeMasterWebUiUrl, logTrace, handleExecutorStateChanged, workDir, startRpcEnvAndEndpoint$default$9, isTraceEnabled, main, initializeLogIfNecessary$default$2, stop, logName, notifyAll, conf, startRpcEnvAndEndpoint, isInstanceOf, <init>, onError, handleDriverStateChanged, coresFree, ENDPOINT_NAME, ==, receiveAndReply, clone, $init$, onDisconnected, toString, logError, !=, retainedExecutors, onConnected, getClass, Worker, logWarning, onStop, retainedDrivers, ne, onStart, rpcEnv, finishedApps, eq, log, appDirectories, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(finishedExecutors, coresUsed, drivers, memoryUsed, finishedDrivers, reverseProxy, executors, self, <init>, ==, toString, Worker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/LocalSparkCluster.scala: Set(conf, startRpcEnvAndEndpoint, <init>, clone, Worker, ne, rpcEnv, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(securityMgr, workDir, conf, <init>, Worker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerSource.scala: Set(coresUsed, memoryUsed, memoryFree, executors, <init>, coresFree, Worker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, activeMasterWebUiUrl, workDir, conf, isInstanceOf, <init>, ==, toString, logError, Worker, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	BaseAppResource, setUiRoot, notify, application, s, ApiRootResource, printStackTrace, getLocalizedMessage, ForbiddenException, wait, $asInstanceOf, getUiRoot, productArity, equals, withUI, UIRoot, httpRequest, UIRootFromServletContext, fillInStackTrace, initCause, asInstanceOf, getApplicationInfoList, getServletHandler, synchronized, withSparkUI, $isInstanceOf, getCause, ErrorWrapper, canEqual, NotFoundException, getResponse, productPrefix, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, version, <init>, BadParameterException, getMessage, attemptId, setStackTrace, getApplicationInfo, getSuppressed, ==, getStackTraceDepth, clone, ApiRequestContext, writeEventLogs, addSuppressed, uiRoot, securityManager, $init$, servletContext, copy, toString, !=, applicationList, getClass, copy$default$1, appId, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(s, ApiRootResource, UIRoot, getApplicationInfoList, getServletHandler, <init>, ==, securityManager, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(s, initCause, asInstanceOf, synchronized, getCause, isInstanceOf, version, <init>, attemptId, ==, clone, copy, toString, !=, getClass, appId, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(<init>, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(BaseAppResource, setUiRoot, s, ApiRootResource, ForbiddenException, getUiRoot, UIRoot, httpRequest, UIRootFromServletContext, asInstanceOf, withSparkUI, ErrorWrapper, NotFoundException, isInstanceOf, <init>, BadParameterException, attemptId, ==, ApiRequestContext, uiRoot, securityManager, servletContext, toString, getClass, appId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(s, ApiRootResource, UIRoot, getApplicationInfoList, getServletHandler, <init>, ==, securityManager, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(BaseAppResource, setUiRoot, s, ApiRootResource, ForbiddenException, getUiRoot, UIRoot, httpRequest, UIRootFromServletContext, asInstanceOf, withSparkUI, ErrorWrapper, NotFoundException, isInstanceOf, <init>, BadParameterException, attemptId, ==, ApiRequestContext, uiRoot, securityManager, servletContext, toString, getClass, appId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorsTab.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(s, asInstanceOf, synchronized, isInstanceOf, <init>, getMessage, ==, clone, securityManager, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(s, ApiRootResource, UIRoot, asInstanceOf, getServletHandler, synchronized, withSparkUI, isInstanceOf, <init>, attemptId, getApplicationInfo, writeEventLogs, securityManager, toString, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(<init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(s, asInstanceOf, isInstanceOf, <init>, ==, uiRoot, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(s, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala: Set(s, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StorageTab.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala: Set(BaseAppResource, s, withUI, UIRoot, httpRequest, NotFoundException, isInstanceOf, <init>, attemptId, getApplicationInfo, ==, writeEventLogs, uiRoot, !=, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(s, <init>, attemptId, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala: Set(BaseAppResource, s, withUI, NotFoundException, <init>, BadParameterException, attemptId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(BaseAppResource, setUiRoot, s, ApiRootResource, ForbiddenException, getUiRoot, UIRoot, httpRequest, UIRootFromServletContext, asInstanceOf, withSparkUI, ErrorWrapper, NotFoundException, isInstanceOf, <init>, BadParameterException, attemptId, ==, ApiRequestContext, uiRoot, securityManager, servletContext, toString, getClass, appId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppHistoryServerPlugin.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(s, equals, asInstanceOf, isInstanceOf, version, <init>, attemptId, ==, toString, !=, getClass, appId, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(s, <init>, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, appId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(s, asInstanceOf, isInstanceOf, <init>, attemptId, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(s, asInstanceOf, isInstanceOf, <init>, getMessage, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(s, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(s, httpRequest, asInstanceOf, getApplicationInfoList, getCause, isInstanceOf, <init>, attemptId, ==, toString, !=, appId, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(s, initCause, asInstanceOf, synchronized, getCause, isInstanceOf, version, <init>, attemptId, ==, clone, copy, toString, !=, getClass, appId, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(s, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(s, asInstanceOf, isInstanceOf, <init>, attemptId, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(<init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(<init>, ==, securityManager, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala: Set(BaseAppResource, s, withUI, UIRoot, httpRequest, NotFoundException, isInstanceOf, <init>, attemptId, getApplicationInfo, ==, writeEventLogs, uiRoot, !=, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(s, ApiRootResource, UIRoot, asInstanceOf, getServletHandler, synchronized, withSparkUI, isInstanceOf, <init>, attemptId, getApplicationInfo, writeEventLogs, securityManager, toString, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala: Set(UIRoot, getApplicationInfoList, <init>, ApiRequestContext, uiRoot)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala: Set(BaseAppResource, s, withUI, UIRoot, httpRequest, NotFoundException, isInstanceOf, <init>, attemptId, getApplicationInfo, ==, writeEventLogs, uiRoot, !=, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(s, ApiRootResource, UIRoot, getApplicationInfoList, getServletHandler, <init>, ==, securityManager, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala: Set(BaseAppResource, s, withUI, NotFoundException, <init>, BadParameterException, attemptId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala: Set(UIRoot, httpRequest, <init>, ApiRequestContext, uiRoot, securityManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala: Set(s, asInstanceOf, ErrorWrapper, isInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(s, ApiRootResource, UIRoot, getApplicationInfoList, getServletHandler, <init>, ==, securityManager, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(s, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(s, asInstanceOf, isInstanceOf, <init>, attemptId, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(s, ApiRootResource, UIRoot, getApplicationInfoList, getServletHandler, <init>, ==, securityManager, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolTable.scala: Set(s, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(s, ApiRootResource, UIRoot, getApplicationInfoList, getServletHandler, <init>, ==, securityManager, appId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(s, asInstanceOf, isInstanceOf, <init>, attemptId, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(s, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(s, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/FileSegment.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/FileSegment.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/FileSegment.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/FileSegment.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/FileSegment.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, FileSegment, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, offset, ==, clone, toString, file, length, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(FileSegment, <init>, ==, file, length, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(FileSegment, asInstanceOf, synchronized, <init>, ==, file, length, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(FileSegment, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, file, length, !=, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/sink/MetricsServlet.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, registry, wait, $asInstanceOf, servletPath, getMetricsSnapshot, mapper, equals, servletShowSample, SERVLET_KEY_PATH, asInstanceOf, synchronized, SERVLET_DEFAULT_SAMPLE, $isInstanceOf, stop, notifyAll, MetricsServlet, isInstanceOf, <init>, SERVLET_KEY_SAMPLE, ==, clone, report, toString, property, !=, getHandlers, getClass, start, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala: Set(registry, asInstanceOf, stop, MetricsServlet, <init>, ==, report, !=, getHandlers, start)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/SchedulerBackendUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, getInitialTargetExecutorNumber, synchronized, $isInstanceOf, notifyAll, isInstanceOf, DEFAULT_NUMBER_EXECUTORS, ==, clone, SchedulerBackendUtils, toString, !=, getClass, ne, getInitialTargetExecutorNumber$default$2, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	VersionInfo, rddIds, shuffleRead, ApplicationAttemptInfo, appSparkVersion, notify, totalBlocksFetched, coresGranted, duration, maxCores, ShuffleReadMetrics, readRecords, javaVersion, onHeapMemoryUsed, numActiveStages, taskTime, name, submissionTime, resultSerializationTime, writeTime, maxTasks, wait, attempts, RDDDataDistribution, startTime, copy$default$2, $asInstanceOf, TaskMetrics, killedTasks, memoryUsed, copy$default$5, schedulingPool, systemProperties, errorMessage, failureReason, diskUsed, productArity, memoryMetrics, recordsWritten, equals, lastUpdated, totalGCTime, taskMetrics, hostPort, description, stageId, outputBytes, peakExecutionMemory, inputMetrics, ShuffleWriteMetricDistributions, asInstanceOf, host, bytesWritten, shuffleWriteMetrics, outputRecords, synchronized, diskBytesSpilled, totalShuffleRead, executors, writeBytes, writeRecords, $isInstanceOf, onHeapMemoryRemaining, recordsRead, numCompleteTasks, copy$default$8, executorSummary, totalTasks, OutputMetrics, localBytesRead, speculative, canEqual, activeTasks, copy$default$4, fetchWaitTime, RDDStorageInfo, executorId, rddBlocks, offHeapMemoryRemaining, productPrefix, remoteBytesRead, scalaVersion, InputMetricDistributions, shuffleWriteBytes, notifyAll, shuffleReadBytes, removeReason, quantiles, schedulerDelay, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, StageData, numFailedStages, bytesRead, endTime, completionTime, <init>, totalOffHeapStorageMemory, OutputMetricDistributions, id, numCompletedIndices, attemptId, RDDPartitionInfo, numCachedPartitions, numSkippedStages, tasks, AccumulableInfo, numActiveTasks, JobData, spark, executorLogs, totalDuration, completed, failedTasks, RuntimeInfo, ==, outputMetrics, getEndTimeEpoch, removeTime, clone, shuffleWrite, numTasks, completedTasks, status, totalOnHeapStorageMemory, readBytes, inputBytes, memoryBytesSpilled, stageIds, launchTime, ExecutorStageSummary, copy$default$7, jobId, ApplicationEnvironmentInfo, $init$, isActive, inputRecords, localBlocksFetched, copy$default$3, offHeapMemoryUsed, address, details, ApplicationInfo, copy, succeededTasks, totalCores, classpathEntries, blockName, toString, jvmGcTime, usedOnHeapStorageMemory, !=, usedOffHeapStorageMemory, coresPerExecutor, executorCpuTime, MemoryMetrics, partitions, getClass, copy$default$1, update, numCompletedStages, attempt, memoryRemaining, accumulatorUpdates, taskLocality, sparkUser, totalInputBytes, remoteBlocksFetched, addTime, ExecutorSummary, numFailedTasks, gettingResultTime, getLastUpdatedEpoch, totalShuffleWrite, copy$default$6, TaskMetricDistributions, InputMetrics, ne, jobGroup, executorRunTime, shuffleReadMetrics, firstTaskLaunchedTime, value, numSkippedTasks, javaHome, ShuffleReadMetricDistributions, maxMemory, dataDistribution, numKilledTasks, getStartTimeEpoch, TaskData, shuffleWriteRecords, numPartitions, eq, killedTasksSummary, productIterator, storageLevel, taskId, sparkProperties, isBlacklisted, resultFetchStart, numCompletedTasks, ShuffleWriteMetrics, runtime, memoryPerExecutorMB, ##, finalize, resultSize, index, productElement, hashCode, remoteBytesReadToDisk, shuffleReadRecords.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/ConsoleProgressBar.scala: Set(submissionTime, stageId, synchronized, numCompleteTasks, StageData, <init>, numActiveTasks, spark, ==, numTasks, status, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, id, attemptId, spark, status, ApplicationInfo, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusUtils.scala: Set(duration, resultSerializationTime, TaskMetrics, taskMetrics, schedulerDelay, executorDeserializeTime, <init>, spark, status, launchTime, gettingResultTime, executorRunTime, TaskData, resultFetchStart)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApplicationListResource.scala: Set(ApplicationAttemptInfo, attempts, startTime, endTime, <init>, spark, completed, status, ApplicationInfo, attempt)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(ApplicationAttemptInfo, duration, submissionTime, attempts, startTime, executors, removeReason, isInstanceOf, numFailedStages, endTime, completionTime, <init>, id, numCompletedIndices, numSkippedStages, numActiveTasks, JobData, spark, ==, removeTime, numTasks, status, stageIds, jobId, ApplicationEnvironmentInfo, ApplicationInfo, toString, !=, numCompletedStages, addTime, ExecutorSummary, numFailedTasks, ne, jobGroup, numSkippedTasks, killedTasksSummary, sparkProperties)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala: Set(rddIds, shuffleRead, duration, name, submissionTime, schedulingPool, failureReason, description, stageId, outputBytes, asInstanceOf, numCompleteTasks, RDDStorageInfo, shuffleWriteBytes, shuffleReadBytes, isInstanceOf, StageData, completionTime, <init>, id, attemptId, numActiveTasks, spark, ==, shuffleWrite, numTasks, status, inputBytes, details, !=, numFailedTasks, ne, firstTaskLaunchedTime, killedTasksSummary)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala: Set(ApplicationAttemptInfo, attempts, RDDStorageInfo, isInstanceOf, <init>, attemptId, JobData, spark, ==, status, jobId, ApplicationEnvironmentInfo, ApplicationInfo, !=, ExecutorSummary)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(duration, ShuffleReadMetrics, taskTime, name, submissionTime, resultSerializationTime, writeTime, maxTasks, RDDDataDistribution, TaskMetrics, killedTasks, memoryUsed, schedulingPool, errorMessage, failureReason, diskUsed, memoryMetrics, recordsWritten, hostPort, description, stageId, peakExecutionMemory, inputMetrics, host, bytesWritten, shuffleWriteMetrics, diskBytesSpilled, totalShuffleRead, executors, recordsRead, totalTasks, OutputMetrics, localBytesRead, speculative, activeTasks, fetchWaitTime, RDDStorageInfo, executorId, rddBlocks, remoteBytesRead, removeReason, executorDeserializeTime, executorDeserializeCpuTime, StageData, bytesRead, completionTime, <init>, id, attemptId, RDDPartitionInfo, AccumulableInfo, JobData, spark, executorLogs, totalDuration, failedTasks, ==, outputMetrics, removeTime, numTasks, completedTasks, status, memoryBytesSpilled, stageIds, launchTime, ExecutorStageSummary, jobId, isActive, localBlocksFetched, details, succeededTasks, totalCores, blockName, toString, jvmGcTime, !=, executorCpuTime, MemoryMetrics, partitions, update, taskLocality, totalInputBytes, remoteBlocksFetched, addTime, ExecutorSummary, gettingResultTime, totalShuffleWrite, InputMetrics, jobGroup, executorRunTime, shuffleReadMetrics, value, maxMemory, TaskData, numPartitions, eq, storageLevel, taskId, isBlacklisted, ShuffleWriteMetrics, resultSize, index, remoteBytesReadToDisk)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(ApplicationAttemptInfo, appSparkVersion, attempts, startTime, systemProperties, <init>, id, spark, ==, status, ApplicationEnvironmentInfo, ApplicationInfo, sparkUser)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala: Set(stageId, quantiles, StageData, <init>, attemptId, spark, status, details, TaskMetricDistributions, TaskData)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(VersionInfo, asInstanceOf, isInstanceOf, <init>, attemptId, spark, ==, status, ApplicationInfo, toString, getClass, eq, runtime)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala: Set(shuffleRead, taskTime, killedTasks, hostPort, stageId, outputBytes, outputRecords, diskBytesSpilled, executorSummary, StageData, <init>, attemptId, spark, executorLogs, failedTasks, shuffleWrite, status, inputBytes, memoryBytesSpilled, ExecutorStageSummary, inputRecords, succeededTasks, toString, ExecutorSummary, ne, shuffleWriteRecords, isBlacklisted, shuffleReadRecords)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(rddIds, totalBlocksFetched, readRecords, name, submissionTime, resultSerializationTime, writeTime, maxTasks, schedulingPool, failureReason, recordsWritten, description, stageId, outputBytes, peakExecutionMemory, inputMetrics, ShuffleWriteMetricDistributions, asInstanceOf, bytesWritten, shuffleWriteMetrics, outputRecords, diskBytesSpilled, writeBytes, writeRecords, recordsRead, numCompleteTasks, executorSummary, fetchWaitTime, RDDStorageInfo, executorId, remoteBytesRead, InputMetricDistributions, shuffleWriteBytes, shuffleReadBytes, quantiles, schedulerDelay, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, StageData, bytesRead, completionTime, <init>, OutputMetricDistributions, id, numCompletedIndices, attemptId, numCachedPartitions, tasks, AccumulableInfo, numActiveTasks, JobData, spark, ==, outputMetrics, numTasks, status, readBytes, inputBytes, memoryBytesSpilled, stageIds, ExecutorStageSummary, jobId, ApplicationEnvironmentInfo, inputRecords, localBlocksFetched, details, ApplicationInfo, toString, jvmGcTime, !=, executorCpuTime, accumulatorUpdates, remoteBlocksFetched, ExecutorSummary, numFailedTasks, gettingResultTime, TaskMetricDistributions, ne, executorRunTime, shuffleReadMetrics, firstTaskLaunchedTime, ShuffleReadMetricDistributions, numKilledTasks, TaskData, shuffleWriteRecords, killedTasksSummary, taskId, resultSize, index, remoteBytesReadToDisk, shuffleReadRecords)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(ApplicationAttemptInfo, appSparkVersion, duration, ShuffleReadMetrics, taskTime, name, submissionTime, attempts, RDDDataDistribution, startTime, TaskMetrics, killedTasks, memoryUsed, schedulingPool, errorMessage, failureReason, diskUsed, equals, taskMetrics, hostPort, description, stageId, inputMetrics, asInstanceOf, host, bytesWritten, shuffleWriteMetrics, totalShuffleRead, executors, executorSummary, totalTasks, localBytesRead, activeTasks, executorId, rddBlocks, remoteBytesRead, isInstanceOf, StageData, bytesRead, completionTime, <init>, id, attemptId, tasks, AccumulableInfo, JobData, spark, totalDuration, failedTasks, RuntimeInfo, ==, numTasks, completedTasks, status, stageIds, launchTime, jobId, ApplicationEnvironmentInfo, details, ApplicationInfo, succeededTasks, totalCores, toString, !=, getClass, update, numCompletedStages, attempt, taskLocality, sparkUser, totalInputBytes, addTime, ExecutorSummary, totalShuffleWrite, InputMetrics, ne, jobGroup, shuffleReadMetrics, killedTasksSummary, storageLevel, taskId, sparkProperties, ShuffleWriteMetrics, runtime, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala: Set(name, memoryUsed, diskUsed, hostPort, RDDStorageInfo, executorId, <init>, id, numCachedPartitions, spark, ==, status, toString, ne, numPartitions, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(duration, ShuffleReadMetrics, readRecords, name, resultSerializationTime, writeTime, TaskMetrics, errorMessage, recordsWritten, taskMetrics, description, stageId, outputBytes, peakExecutionMemory, inputMetrics, ShuffleWriteMetricDistributions, asInstanceOf, host, bytesWritten, shuffleWriteMetrics, outputRecords, diskBytesSpilled, writeBytes, writeRecords, recordsRead, numCompleteTasks, executorSummary, totalTasks, OutputMetrics, localBytesRead, speculative, fetchWaitTime, executorId, remoteBytesRead, InputMetricDistributions, shuffleWriteBytes, shuffleReadBytes, schedulerDelay, executorDeserializeTime, isInstanceOf, StageData, bytesRead, <init>, OutputMetricDistributions, id, attemptId, tasks, AccumulableInfo, numActiveTasks, JobData, spark, executorLogs, ==, outputMetrics, status, readBytes, inputBytes, memoryBytesSpilled, stageIds, launchTime, inputRecords, details, toString, jvmGcTime, !=, update, attempt, accumulatorUpdates, taskLocality, ExecutorSummary, numFailedTasks, gettingResultTime, TaskMetricDistributions, InputMetrics, ne, executorRunTime, shuffleReadMetrics, value, ShuffleReadMetricDistributions, numKilledTasks, TaskData, shuffleWriteRecords, taskId, ShuffleWriteMetrics, index, shuffleReadRecords)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/env/EnvironmentPage.scala: Set(javaVersion, systemProperties, scalaVersion, <init>, spark, RuntimeInfo, status, ApplicationEnvironmentInfo, classpathEntries, javaHome, sparkProperties, runtime)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala: Set(ApplicationAttemptInfo, attempts, <init>, spark, completed, status, ApplicationInfo, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala: Set(onHeapMemoryUsed, name, RDDDataDistribution, memoryUsed, diskUsed, hostPort, asInstanceOf, executors, onHeapMemoryRemaining, RDDStorageInfo, offHeapMemoryRemaining, isInstanceOf, <init>, id, RDDPartitionInfo, numCachedPartitions, spark, ==, status, offHeapMemoryUsed, address, blockName, toString, !=, partitions, ExecutorSummary, dataDistribution, numPartitions, eq, storageLevel, runtime)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllStagesPage.scala: Set(name, StageData, numFailedStages, <init>, spark, ==, status, numCompletedStages)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationCache.scala: Set(ApplicationAttemptInfo, name, attempts, asInstanceOf, isInstanceOf, <init>, id, attemptId, spark, completed, ==, ApplicationInfo, toString, !=, ne, eq, runtime)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala: Set(ApplicationAttemptInfo, appSparkVersion, coresGranted, duration, maxCores, name, attempts, startTime, lastUpdated, asInstanceOf, synchronized, isInstanceOf, endTime, <init>, id, attemptId, tasks, spark, completed, ==, clone, status, ApplicationInfo, copy, toString, !=, coresPerExecutor, getClass, update, attempt, sparkUser, ne, eq, isBlacklisted, runtime, memoryPerExecutorMB, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/history/ApplicationHistoryProvider.scala: Set(asInstanceOf, isInstanceOf, <init>, spark, ==, status, ApplicationInfo, toString, eq, runtime)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/PoolPage.scala: Set(StageData, <init>, spark, status, stageIds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(name, submissionTime, memoryMetrics, hostPort, stageId, host, numCompleteTasks, activeTasks, StageData, <init>, attemptId, numActiveTasks, JobData, spark, ==, numTasks, status, stageIds, jobId, usedOnHeapStorageMemory, !=, usedOffHeapStorageMemory, MemoryMetrics, ExecutorSummary, numFailedTasks, ne, jobGroup)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala: Set(duration, ShuffleReadMetrics, name, resultSerializationTime, TaskMetrics, errorMessage, recordsWritten, hostPort, stageId, peakExecutionMemory, asInstanceOf, host, bytesWritten, diskBytesSpilled, recordsRead, OutputMetrics, speculative, RDDStorageInfo, executorId, shuffleWriteBytes, shuffleReadBytes, schedulerDelay, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, StageData, bytesRead, completionTime, <init>, id, attemptId, numCachedPartitions, AccumulableInfo, JobData, spark, ==, status, memoryBytesSpilled, stageIds, launchTime, ExecutorStageSummary, jobId, ApplicationEnvironmentInfo, isActive, ApplicationInfo, toString, jvmGcTime, executorCpuTime, numCompletedStages, attempt, accumulatorUpdates, taskLocality, ExecutorSummary, gettingResultTime, InputMetrics, executorRunTime, value, TaskData, shuffleWriteRecords, eq, storageLevel, taskId, resultFetchStart, ShuffleWriteMetrics, runtime, resultSize, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(ApplicationAttemptInfo, name, submissionTime, attempts, startTime, stageId, asInstanceOf, executors, removeReason, isInstanceOf, StageData, completionTime, <init>, id, attemptId, AccumulableInfo, JobData, spark, ==, removeTime, status, stageIds, ExecutorStageSummary, jobId, ApplicationInfo, toString, !=, addTime, ExecutorSummary, jobGroup, TaskData)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(<init>, id, JobData, spark, ==, status, jobId, ApplicationEnvironmentInfo, toString, sparkProperties)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagesTab.scala: Set(stageId, StageData, <init>, id, spark, ==, status, ApplicationEnvironmentInfo, toString, sparkProperties)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RPackageUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, checkAndBuildRPackage$default$3, checkAndBuildRPackage, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, checkAndBuildRPackage$default$2, $isInstanceOf, checkManifestForR, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, zipRLibraries, ==, clone, RPackageUtils, $init$, toString, logError, !=, getClass, logWarning, ne, RJarDoc, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(checkAndBuildRPackage, asInstanceOf, initializeLogIfNecessary, isInstanceOf, zipRLibraries, ==, RPackageUtils, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/BoundedPriorityQueue.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/BoundedPriorityQueue.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/BoundedPriorityQueue.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/BoundedPriorityQueue.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/BoundedPriorityQueue.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, find, span, flatten, toBuffer, count, poll, reduceOption, sliceWithKnownDelta, wait, foldRight, takeWhile, stringPrefix, $asInstanceOf, splitAt, minBy, size, inits, zip, toSet, :\, map, takeRight, dropWhile, toMap, filterNot, equals, par, unzip3, repr, toList, +=, clear, isTraversableAgain, head, asInstanceOf, sameElements, unzip, reduceLeftOption, synchronized, sliding, Self, partition, aggregate, $isInstanceOf, forall, newBuilder, mkString, min, scanRight, fold, scan, nonEmpty, canEqual, tail, lastOption, dropRight, iterator, last, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, ++:, <init>, toStream, companion, max, tails, ++, grouped, flatMap, take, parCombiner, reduceRight, groupBy, ==, maxBy, BoundedPriorityQueue, sliceWithKnownBound, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, $init$, toSeq, zipWithIndex, toString, genericBuilder, copyToArray, seq, !=, transpose, collect, headOption, getClass, WithFilter, hasDefiniteSize, ++=, foldLeft, toCollection, isEmpty, ne, init, reversed, reduceLeft, eq, sum, thisCollection, ##, scanLeft, finalize, hashCode, zipAll, product, view.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, head, asInstanceOf, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, iterator, last, isInstanceOf, filter, <init>, max, ++, grouped, flatMap, take, groupBy, ==, maxBy, BoundedPriorityQueue, clone, foreach, exists, toArray, reduce, toSeq, zipWithIndex, toString, !=, collect, getClass, ++=, isEmpty, ne, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(withFilter, map, clear, asInstanceOf, isInstanceOf, filter, <init>, max, ==, BoundedPriorityQueue, foreach, !=, getClass, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, sortByKey$default$2, sortByKey, synchronized, repartitionAndSortWithinPartitions, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, OrderedRDDFunctions, clone, $init$, sortByKey$default$1, toString, logError, !=, getClass, logWarning, filterByRange, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, sortByKey, synchronized, isInstanceOf, <init>, ==, OrderedRDDFunctions, clone, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(asInstanceOf, sortByKey, repartitionAndSortWithinPartitions, <init>, OrderedRDDFunctions, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Outbox.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, copy$default$2, $asInstanceOf, onTimeout, productArity, equals, asInstanceOf, initializeLogIfNecessary, synchronized, Outbox, $isInstanceOf, _onFailure, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, stop, logName, notifyAll, _onSuccess, isInstanceOf, onSuccess, <init>, ==, clone, RpcOutboxMessage, $init$, content, copy$default$3, address, copy, toString, logError, !=, getClass, logWarning, copy$default$1, onFailure, ne, sendWith, OneWayOutboxMessage, OutboxMessage, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo, send.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(onTimeout, asInstanceOf, synchronized, Outbox, stop, isInstanceOf, onSuccess, <init>, ==, clone, RpcOutboxMessage, content, address, toString, logError, !=, logWarning, onFailure, ne, sendWith, OneWayOutboxMessage, OutboxMessage, eq, logDebug, send)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, serializeStream, INSTANCE, DummySerializerInstance, deserialize, deserializeStream, notifyAll, toString, getClass, serialize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, writeWorkerState, equals, writeApplicationInfo, asInstanceOf, writeWorkerInfo, synchronized, writeDriverInfo, $isInstanceOf, notifyAll, isInstanceOf, writeApplicationDescription, ==, clone, JsonProtocol, writeExecutorRunner, toString, !=, getClass, ne, eq, ##, finalize, hashCode, writeMasterState.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(==, JsonProtocol, toString, writeMasterState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(writeWorkerState, ==, JsonProtocol, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, getRecordLength, equals, UnsafeSorterIterator, notifyAll, <init>, getKeyPrefix, toString, loadNext, getBaseOffset, getBaseObject, getClass, getNumRecords, hasNext, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillMerger.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, CoarseGrainedExecutorBackend, exitExecutor$default$4, wait, onNetworkError, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, exitExecutor$default$3, synchronized, self, $isInstanceOf, receive, logTrace, isTraceEnabled, main, initializeLogIfNecessary$default$2, stop, logName, notifyAll, extractLogUrls, isInstanceOf, <init>, onError, driver, exitExecutor, statusUpdate, ==, receiveAndReply, clone, $init$, onDisconnected, toString, logError, !=, onConnected, getClass, logWarning, onStop, executor, ne, onStart, rpcEnv, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, stop, notifyAll, initialize, isInstanceOf, BroadcastFactory, ==, clone, toString, !=, getClass, unbroadcast, newBroadcast, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(synchronized, stop, initialize, BroadcastFactory, unbroadcast, newBroadcast)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/BroadcastManager.scala: Set(synchronized, stop, initialize, BroadcastFactory, unbroadcast, newBroadcast)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcastFactory.scala: Set(BroadcastFactory)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/random/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/SerializableJobConf.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, SerializableJobConf, ne, value, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, SerializableJobConf, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, SerializableJobConf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, getClass, SerializableJobConf, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function3.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function3.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function3.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function3.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function3.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, Function3.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function3.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ApplicationDescription.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ApplicationDescription.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ApplicationDescription.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ApplicationDescription.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/ApplicationDescription.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, maxCores, name, wait, copy$default$2, $asInstanceOf, copy$default$5, productArity, equals, copy$default$9, ApplicationDescription, asInstanceOf, synchronized, $isInstanceOf, appUiUrl, copy$default$8, canEqual, copy$default$4, command, productPrefix, notifyAll, isInstanceOf, eventLogDir, <init>, ==, clone, initialExecutorLimit, copy$default$7, copy$default$10, $init$, copy$default$3, copy, toString, eventLogCodec, !=, coresPerExecutor, getClass, copy$default$1, copy$default$6, ne, user, eq, productIterator, memoryPerExecutorMB, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(name, ApplicationDescription, appUiUrl, command, <init>, ==, toString, user)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(maxCores, ApplicationDescription, asInstanceOf, command, isInstanceOf, eventLogDir, <init>, ==, initialExecutorLimit, toString, eventLogCodec, !=, coresPerExecutor)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(name, ApplicationDescription, appUiUrl, command, <init>, ==, toString, user, memoryPerExecutorMB)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala: Set(maxCores, name, ApplicationDescription, command, <init>, toString, user, memoryPerExecutorMB)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterWebUI.scala: Set(ApplicationDescription, appUiUrl, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(name, ApplicationDescription, asInstanceOf, synchronized, command, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(ApplicationDescription, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(ApplicationDescription, command, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(name, ApplicationDescription, asInstanceOf, command, isInstanceOf, <init>, ==, toString, !=, coresPerExecutor, ne, memoryPerExecutorMB)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/client/StandaloneAppClient.scala: Set(ApplicationDescription, asInstanceOf, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationSource.scala: Set(name, ApplicationDescription, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ApplicationInfo.scala: Set(maxCores, ApplicationDescription, asInstanceOf, isInstanceOf, <init>, ==, initialExecutorLimit, !=, memoryPerExecutorMB)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/ApplicationPage.scala: Set(maxCores, name, ApplicationDescription, appUiUrl, <init>, ==, user, memoryPerExecutorMB)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	classTag, toRDD, fromRDD.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/SerDeUtil.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/WorkerWatcher.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, onNetworkError, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, self, $isInstanceOf, receive, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, WorkerWatcher, <init>, onError, ==, receiveAndReply, clone, $init$, onDisconnected, isShutDown, toString, logError, !=, onConnected, getClass, logWarning, onStop, ne, onStart, rpcEnv, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, self, stop, isInstanceOf, WorkerWatcher, <init>, ==, logError, !=, logWarning, ne, rpcEnv, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(asInstanceOf, isInstanceOf, WorkerWatcher, <init>, ==, !=, rpcEnv, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkStatusTracker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, JavaSparkStatusTracker, equals, asInstanceOf, getActiveStageIds, synchronized, $isInstanceOf, notifyAll, isInstanceOf, getActiveJobIds, <init>, ==, clone, getStageInfo, toString, !=, getClass, getJobInfo, ne, eq, getJobIdsForGroup, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(JavaSparkStatusTracker, asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, enqueueFailedTask, equals, asInstanceOf, initializeLogIfNecessary, TaskResultGetter, synchronized, getTaskResultExecutor, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, serializer, ==, taskResultSerializer, clone, $init$, toString, logError, !=, getClass, enqueueSuccessfulTask, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, enqueueFailedTask, asInstanceOf, TaskResultGetter, synchronized, stop, isInstanceOf, <init>, ==, toString, logError, !=, enqueueSuccessfulTask, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, SortShuffleWriter, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, shouldBypassMergeSort, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, write, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(SortShuffleWriter, asInstanceOf, stop, isInstanceOf, shouldBypassMergeSort, <init>, getClass, logWarning, log)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onExecutorUnblacklisted, notify, wait, onNetworkError, copy$default$2, $asInstanceOf, onJobEnd, onApplicationEnd, productArity, equals, onTaskEnd, addExecutor, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, initializeLogIfNecessary, synchronized, self, reregisterBlockManager, $isInstanceOf, receive, ExpireDeadHosts, logTrace, onOtherEvent, canEqual, isTraceEnabled, executorId, initializeLogIfNecessary$default$2, removeExecutor, onTaskStart, productPrefix, HeartbeatReceiver, stop, logName, notifyAll, onExecutorRemoved, onExecutorAdded, isInstanceOf, blockManagerId, <init>, onError, onBlockUpdated, scheduler, ENDPOINT_NAME, ==, onBlockManagerRemoved, receiveAndReply, HeartbeatResponse, clone, onExecutorMetricsUpdate, onEnvironmentUpdate, onBlockManagerAdded, $init$, TaskSchedulerIsSet, onNodeBlacklisted, onDisconnected, onApplicationStart, accumUpdates, copy$default$3, copy, onSpeculativeTaskSubmitted, toString, logError, !=, onConnected, getClass, logWarning, copy$default$1, onStop, onExecutorBlacklisted, onUnpersistRDD, ne, onStageSubmitted, onStart, rpcEnv, eq, onNodeUnblacklisted, productIterator, Heartbeat, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, reregisterBlockManager, executorId, HeartbeatReceiver, stop, notifyAll, isInstanceOf, blockManagerId, <init>, scheduler, ENDPOINT_NAME, ==, HeartbeatResponse, accumUpdates, logError, !=, logWarning, ne, rpcEnv, eq, Heartbeat, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, executorId, HeartbeatReceiver, stop, isInstanceOf, blockManagerId, <init>, scheduler, ENDPOINT_NAME, ==, clone, TaskSchedulerIsSet, toString, logError, !=, getClass, logWarning, ne, rpcEnv, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ClientApp, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, main, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, start, Client, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, messages, post, wait, OnStop, copy$default$2, $asInstanceOf, productArity, equals, Inbox, process, asInstanceOf, RemoteProcessDisconnected, context, initializeLogIfNecessary, synchronized, onDrop, $isInstanceOf, logTrace, canEqual, OneWayMessage, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, stop, logName, notifyAll, isInstanceOf, <init>, ==, clone, cause, $init$, content, RemoteProcessConnectionError, endpoint, copy$default$3, copy, RemoteProcessConnected, toString, RpcMessage, endpointRef, logError, !=, InboxMessage, getClass, logWarning, copy$default$1, senderAddress, isEmpty, ne, OnStart, eq, productIterator, log, remoteAddress, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(asInstanceOf, RemoteProcessDisconnected, synchronized, stop, isInstanceOf, <init>, ==, clone, cause, content, RemoteProcessConnectionError, endpoint, RemoteProcessConnected, toString, endpointRef, logError, !=, InboxMessage, logWarning, senderAddress, isEmpty, ne, eq, remoteAddress, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(post, Inbox, process, asInstanceOf, synchronized, OneWayMessage, stop, isInstanceOf, <init>, ==, content, endpoint, RpcMessage, endpointRef, logError, !=, InboxMessage, logWarning, senderAddress, isEmpty, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/AsyncRDDActions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, AsyncRDDActions, equals, asInstanceOf, initializeLogIfNecessary, foreachAsync, takeAsync, synchronized, $isInstanceOf, countAsync, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, foreachPartitionAsync, <init>, ==, clone, collectAsync, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(AsyncRDDActions, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, logWarning, ne, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(AsyncRDDActions, asInstanceOf, foreachAsync, takeAsync, countAsync, foreachPartitionAsync, <init>, collectAsync)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PrimitiveVector.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, size, equals, +=, asInstanceOf, synchronized, $isInstanceOf, iterator, notifyAll, isInstanceOf, resize, array, <init>, apply, ==, clone, toArray, toString, length, !=, getClass, trim, ne, eq, ##, finalize, hashCode, PrimitiveVector, capacity.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(size, +=, asInstanceOf, synchronized, iterator, isInstanceOf, <init>, apply, ==, toArray, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/SizeTrackingVector.scala: Set(+=, resize, <init>, PrimitiveVector)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(size, +=, asInstanceOf, synchronized, iterator, isInstanceOf, <init>, apply, ==, toArray, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/EventLoop.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, post, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, onReceive, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, EventLoop, logName, notifyAll, isInstanceOf, <init>, onError, ==, clone, $init$, isActive, toString, logError, !=, getClass, logWarning, onStop, start, ne, onStart, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(post, asInstanceOf, synchronized, logTrace, stop, EventLoop, isInstanceOf, <init>, ==, clone, toString, logError, !=, logWarning, start, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(post, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, start, ne, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, start, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala: Set(asInstanceOf, synchronized, <init>, ==, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/TaskEndReason.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, FetchFailed, execId, toErrorString, wait, copy$default$2, $asInstanceOf, copy$default$5, productArity, equals, TaskKilled, description, accums$1, asInstanceOf, shuffleId, initializeLogIfNecessary, bmAddress, synchronized, accums, $isInstanceOf, exception, logTrace, canEqual, stackTrace, copy$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, reason, partitionID, logName, ExecutorLostFailure, notifyAll, TaskEndReason, isInstanceOf, fullStackTrace, mapId, TaskResultLost, <init>, exitCausedByApp, ==, jobID, clone, className, copy$default$7, ThrowableSerializationWrapper, ExceptionFailure, $init$, TaskCommitDenied, accumUpdates, copy$default$3, copy, UnknownReason, exceptionWrapper$1, message, toString, attemptNumber, logError, !=, getClass, logWarning, copy$default$1, Success, reduceId, copy$default$6, ne, withAccums, TaskFailedReason, Resubmitted, eq, productIterator, log, countTowardsTaskFailures, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(FetchFailed, execId, asInstanceOf, shuffleId, bmAddress, synchronized, exception, logTrace, reason, TaskEndReason, isInstanceOf, mapId, TaskResultLost, <init>, ==, clone, accumUpdates, UnknownReason, message, toString, attemptNumber, logError, !=, logWarning, Success, ne, Resubmitted, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/FetchFailedException.scala: Set(FetchFailed, shuffleId, bmAddress, mapId, <init>, message, reduceId, TaskFailedReason)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(FetchFailed, execId, toErrorString, description, asInstanceOf, bmAddress, synchronized, accums, exception, reason, ExecutorLostFailure, TaskEndReason, isInstanceOf, <init>, exitCausedByApp, ==, className, ExceptionFailure, accumUpdates, message, toString, attemptNumber, logError, !=, logWarning, Success, ne, TaskFailedReason, Resubmitted, countTowardsTaskFailures, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala: Set(asInstanceOf, synchronized, reason, TaskEndReason, isInstanceOf, <init>, ==, toString, attemptNumber, logError, Success, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CommitDeniedException.scala: Set(<init>, jobID, TaskCommitDenied, attemptNumber)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala: Set(asInstanceOf, synchronized, reason, TaskEndReason, isInstanceOf, <init>, ==, !=, logWarning, Success, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(execId, toErrorString, equals, TaskKilled, description, asInstanceOf, reason, TaskEndReason, isInstanceOf, <init>, ==, ExceptionFailure, TaskCommitDenied, accumUpdates, toString, attemptNumber, !=, getClass, Success, ne, TaskFailedReason, Resubmitted, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(execId, asInstanceOf, exception, reason, TaskEndReason, isInstanceOf, <init>, ==, accumUpdates, message, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala: Set(execId, asInstanceOf, reason, TaskEndReason, isInstanceOf, <init>, ==, accumUpdates, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(execId, wait, asInstanceOf, synchronized, reason, isInstanceOf, <init>, ==, accumUpdates, message, toString, logError, !=, logWarning, ne, TaskFailedReason, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, TaskKilled, asInstanceOf, synchronized, accums, stackTrace, reason, notifyAll, isInstanceOf, <init>, ==, ExceptionFailure, TaskCommitDenied, accumUpdates, message, attemptNumber, logError, !=, logWarning, ne, withAccums, TaskFailedReason, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(asInstanceOf, reason, isInstanceOf, TaskResultLost, <init>, ==, accumUpdates, UnknownReason, logError, !=, ne, TaskFailedReason, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(FetchFailed, execId, TaskKilled, description, asInstanceOf, shuffleId, bmAddress, exception, stackTrace, reason, partitionID, ExecutorLostFailure, TaskEndReason, isInstanceOf, fullStackTrace, mapId, TaskResultLost, <init>, exitCausedByApp, ==, jobID, className, ThrowableSerializationWrapper, ExceptionFailure, TaskCommitDenied, accumUpdates, UnknownReason, message, toString, attemptNumber, !=, Success, reduceId, ne, Resubmitted)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, readShort, readLong, writeChars, in, newKryoOutput, KryoSerializerInstance, wait, $asInstanceOf, asIterator, readUTF, readBoolean, equals, serializeStream, registerClasses, asInstanceOf, initializeLogIfNecessary, writeInt, getAutoReset, writeChar, writeShort, synchronized, borrowKryo, writeByte, writeBytes, $isInstanceOf, asKeyValueIterator, readObject, writeLong, readFloat, releaseKryo, KryoOutputObjectOutputBridge, skip, logTrace, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, KryoSerializer, skipBytes, readKey, deserializeStream, readValue, logName, notifyAll, writeDouble, out, KryoInputObjectInputBridge, markSupported, isInstanceOf, writeUTF, readByte, readLine, defaultClassLoader, writeObject, <init>, supportsRelocationOfSerializedObjects, setDefaultClassLoader, KryoRegistrator, writeValue, ==, newKryo, clone, maxBufferSizeMb, $init$, KryoSerializationStream, newInstance, reset, readDouble, available, flush, readFully, toString, logError, !=, writeKey, getClass, logWarning, mark, close, readUnsignedShort, ne, serialize, readUnsignedByte, readChar, writeBoolean, eq, write, readInt, log, KryoDeserializationStream, writeFloat, ##, finalize, writeAll, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(asIterator, serializeStream, asInstanceOf, KryoSerializer, deserializeStream, isInstanceOf, <init>, setDefaultClassLoader, ==, newInstance, close, writeAll)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(asInstanceOf, KryoSerializer, isInstanceOf, <init>, ==, toString, !=, logWarning, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerState.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerState.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/MasterSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, SPARK_VERSION, synchronized, $isInstanceOf, notifyAll, SPARK_BUILD_USER, isInstanceOf, SPARK_BRANCH, SPARK_REPO_URL, SPARK_REVISION, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, SPARK_BUILD_DATE, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(package, asInstanceOf, SPARK_VERSION, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(package, asInstanceOf, SPARK_VERSION, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(package, asInstanceOf, SPARK_VERSION, synchronized, isInstanceOf, ==, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(package, asInstanceOf, SPARK_VERSION, SPARK_BUILD_USER, isInstanceOf, SPARK_BRANCH, SPARK_REPO_URL, SPARK_REVISION, ==, toString, !=, getClass, ne, eq, SPARK_BUILD_DATE)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(package, asInstanceOf, SPARK_VERSION, isInstanceOf, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(package, asInstanceOf, SPARK_VERSION, synchronized, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(package, SPARK_VERSION, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/ApiRootResource.scala: Set(package, asInstanceOf, SPARK_VERSION, isInstanceOf, ==, toString, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(package, equals, asInstanceOf, SPARK_VERSION, isInstanceOf, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/launcher/LauncherBackend.scala: Set(package, SPARK_VERSION, isInstanceOf, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(package, asInstanceOf, SPARK_VERSION, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala: Set(package, asInstanceOf, SPARK_VERSION, isInstanceOf, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(package, asInstanceOf, SPARK_VERSION, isInstanceOf, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala: Set(package, asInstanceOf, SPARK_VERSION, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, TaskRunner, wait, task, <init>$default$5, $asInstanceOf, <init>$default$6, equals, isFinished, asInstanceOf, initializeLogIfNecessary, run, synchronized, $isInstanceOf, getThreadId, <init>$default$4, logTrace, Executor, isTraceEnabled, initializeLogIfNecessary$default$2, numRunningTasks, stop, logName, killTask, notifyAll, isInstanceOf, launchTask, <init>, ==, clone, killAllTasks, $init$, toString, logError, !=, threadName, getClass, logWarning, taskDeserializationProps, kill, ne, startGCTime, eq, taskId, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(asInstanceOf, run, Executor, stop, killTask, isInstanceOf, launchTask, <init>, ==, logError, !=, logWarning, ne, taskId, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(task, isFinished, asInstanceOf, Executor, stop, killTask, isInstanceOf, launchTask, <init>, ==, toString, eq, taskId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SerializableWritable.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, SerializableWritable, wait, $asInstanceOf, equals, t, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne, value, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(SerializableWritable, equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(SerializableWritable, t, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(SerializableWritable, equals, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, value)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, defaultParallelism, $asInstanceOf, equals, asInstanceOf, applicationId, synchronized, $isInstanceOf, stop, killTask, notifyAll, isReady, isInstanceOf, reviveOffers, applicationAttemptId, ==, clone, $init$, toString, !=, getClass, start, getDriverLogUrls, ne, SchedulerBackend, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(defaultParallelism, asInstanceOf, applicationId, synchronized, stop, isInstanceOf, applicationAttemptId, ==, clone, toString, !=, getClass, start, getDriverLogUrls, ne, SchedulerBackend)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(asInstanceOf, synchronized, stop, isReady, isInstanceOf, ==, toString, !=, ne, SchedulerBackend)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(asInstanceOf, synchronized, killTask, isInstanceOf, ==, toString, !=, ne, SchedulerBackend)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(defaultParallelism, asInstanceOf, applicationId, synchronized, stop, isInstanceOf, applicationAttemptId, ==, clone, toString, !=, getClass, start, getDriverLogUrls, ne, SchedulerBackend)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/local/LocalSchedulerBackend.scala: Set(asInstanceOf, stop, killTask, isInstanceOf, reviveOffers, ==, toString, SchedulerBackend, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala: Set(SchedulerBackend)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(wait, defaultParallelism, asInstanceOf, applicationId, synchronized, stop, killTask, isReady, isInstanceOf, reviveOffers, applicationAttemptId, ==, toString, !=, start, ne, SchedulerBackend)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(asInstanceOf, stop, isInstanceOf, ==, toString, !=, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(defaultParallelism, asInstanceOf, applicationId, synchronized, stop, isInstanceOf, applicationAttemptId, ==, clone, toString, !=, getClass, start, getDriverLogUrls, ne, SchedulerBackend)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(defaultParallelism, asInstanceOf, applicationId, synchronized, stop, isInstanceOf, applicationAttemptId, ==, clone, toString, !=, getClass, start, getDriverLogUrls, ne, SchedulerBackend)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SerDe.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SerDe.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SerDe.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SerDe.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/SerDe.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ROW, readObjectType, notify, readString, SerDe, readStringBytes, writeString, writeTime, wait, $asInstanceOf, readStringArr, readBoolean, writeStringArr, readTime, equals, readArray, readTypedObject, asInstanceOf, writeInt, SQLReadObject, synchronized, writeBytes, $isInstanceOf, readObject, readIntArr, notifyAll, writeDouble, readDoubleArr, isInstanceOf, writeDate, readBytesArr, writeBooleanArr, setSQLReadObject, writeObject, STRING, ==, clone, readBytes, readMap, setSQLWriteObject, SQLWriteObject, readBooleanArr, writeType, BYTE, writeDoubleArr, readDouble, readDate, toString, !=, writeIntArr, getClass, readList, ne, writeBoolean, eq, readInt, SerializationFormats, ##, finalize, hashCode, writeJObj.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala: Set(SerDe, writeString, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRunner.scala: Set(ROW, SerDe, readStringBytes, writeString, asInstanceOf, writeInt, synchronized, isInstanceOf, writeObject, STRING, ==, BYTE, readDouble, toString, !=, readInt, SerializationFormats)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(asInstanceOf, STRING, ==, BYTE, toString, !=, ne, SerializationFormats)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(readObjectType, readString, SerDe, writeString, readBoolean, asInstanceOf, writeInt, readObject, isInstanceOf, writeObject, ==, writeType, !=, getClass, readInt)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala: Set(SerDe, writeString, asInstanceOf, writeInt, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RAuthHelper.scala: Set(readString, SerDe, writeString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, addJarsToClassPath, equals, asInstanceOf, synchronized, $isInstanceOf, DependencyUtils, notifyAll, resolveGlobPaths, isInstanceOf, resolveAndDownloadJars, ==, clone, resolveMavenDependencies, downloadFile, toString, !=, getClass, ne, eq, downloadFileList, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, DependencyUtils, resolveGlobPaths, isInstanceOf, ==, resolveMavenDependencies, downloadFile, toString, !=, getClass, ne, eq, downloadFileList)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala: Set(addJarsToClassPath, asInstanceOf, DependencyUtils, isInstanceOf, resolveAndDownloadJars, ==, resolveMavenDependencies, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadStackTrace.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadStackTrace.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadStackTrace.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadStackTrace.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ThreadStackTrace.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	blockedByThreadId, notify, wait, copy$default$2, $asInstanceOf, copy$default$5, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, threadState, canEqual, stackTrace, copy$default$4, productPrefix, notifyAll, isInstanceOf, holdingLocks, <init>, ==, clone, copy$default$7, $init$, copy$default$3, copy, blockedByLock, toString, !=, threadName, getClass, copy$default$1, threadId, copy$default$6, ne, eq, ThreadStackTrace, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne, ThreadStackTrace)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(equals, asInstanceOf, synchronized, stackTrace, isInstanceOf, <init>, ==, copy, toString, !=, threadName, getClass, threadId, ne, eq, ThreadStackTrace, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/exec/ExecutorThreadDumpPage.scala: Set(blockedByThreadId, threadState, stackTrace, isInstanceOf, holdingLocks, <init>, ==, blockedByLock, threadName, threadId, ne, ThreadStackTrace)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, asInstanceOf, synchronized, stackTrace, notifyAll, isInstanceOf, <init>, ==, !=, threadName, threadId, ne, eq, ThreadStackTrace)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, ThreadStackTrace)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkApplication.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, SparkApplication, toString, !=, getClass, start, ne, eq, JavaMainApplication, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, SparkApplication, toString, !=, getClass, start, ne, eq, JavaMainApplication)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, start, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, SparkApplication, start, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, SparkApplication, toString, !=, getClass, start, ne, eq, JavaMainApplication)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, SparkApplication, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/DoubleRDDFunctions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, histogram$default$2, wait, stats, $asInstanceOf, variance, equals, sumApprox$default$2, asInstanceOf, initializeLogIfNecessary, stdev, synchronized, $isInstanceOf, mean, meanApprox$default$2, logTrace, popVariance, isTraceEnabled, popStdev, initializeLogIfNecessary$default$2, logName, DoubleRDDFunctions, notifyAll, histogram, isInstanceOf, <init>, ==, meanApprox, clone, $init$, toString, logError, !=, getClass, logWarning, sumApprox, sampleVariance, ne, eq, sum, log, ##, finalize, sampleStdev, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, DoubleRDDFunctions, isInstanceOf, <init>, ==, clone, toString, !=, getClass, logWarning, ne, sum, log, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaDoubleRDD.scala: Set(stats, variance, stdev, mean, popVariance, popStdev, DoubleRDDFunctions, histogram, <init>, meanApprox, sumApprox, sampleVariance, sum, sampleStdev)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendAuthHandler.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, PairFlatMapFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/PairFlatMapFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(call, PairFlatMapFunction)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/NextIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, getNext, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, finished, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, fold, closeIfNeeded, nonEmpty, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, GroupedIterator, <init>, toStream, NextIterator, max, buffered, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, toString, copyToArray, length, seq, !=, collect, getClass, hasDefiniteSize, patch, close, foldLeft, contains, isEmpty, ne, withPartial, reversed, hasNext, indexOf, reduceLeft, eq, sum, ##, scanLeft, finalize, hashCode, zipAll, product.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/Serializer.scala: Set(<init>, NextIterator, next, close, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala: Set(size, map, equals, asInstanceOf, finished, synchronized, partition, closeIfNeeded, isInstanceOf, filter, <init>, NextIterator, flatMap, ==, foreach, next, toString, !=, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(map, asInstanceOf, closeIfNeeded, <init>, NextIterator, toArray, next, length, !=, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(withFilter, map, asInstanceOf, drop, isInstanceOf, <init>, take, ==, slice, foreach, toArray, toSeq, zipWithIndex, length, seq, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(map, filterNot, asInstanceOf, synchronized, isInstanceOf, filter, <init>, ++, ==, foreach, toSeq, toString, !=, collect, getClass, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(find, asInstanceOf, synchronized, nonEmpty, isInstanceOf, <init>, flatMap, ==, clone, foreach, next, toString, length, !=, close, isEmpty, ne, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(toList, <init>, foreach, toSeq, !=, getClass, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(size, map, asInstanceOf, sameElements, partition, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, ==, maxBy, foreach, exists, toArray, length, collect, contains, isEmpty, ne, sum, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(withFilter, find, count, size, map, filterNot, toList, asInstanceOf, partition, mkString, min, nonEmpty, to, isInstanceOf, filter, <init>, max, ++, flatMap, take, ==, foreach, exists, toArray, toSeq, toString, length, !=, close, contains, isEmpty, ne, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(asInstanceOf, <init>, ==, exists, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(withFilter, find, size, map, toMap, toList, asInstanceOf, finished, synchronized, partition, mkString, nonEmpty, drop, isInstanceOf, filter, <init>, ++, flatMap, take, ==, clone, foreach, exists, toArray, toSeq, toString, length, !=, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ZooKeeperPersistenceEngine.scala: Set(filter, <init>, flatMap, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala: Set(<init>, ==, length, !=, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(withFilter, size, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, nonEmpty, isInstanceOf, <init>, max, flatMap, ==, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, !=, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(toSet, asInstanceOf, partition, <init>, ==, toSeq, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala: Set(withFilter, size, zip, toList, asInstanceOf, isInstanceOf, <init>, ==, clone, foreach, toString, length, !=, getClass, contains, isEmpty, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(withFilter, map, toList, asInstanceOf, mkString, nonEmpty, isInstanceOf, <init>, ++, flatMap, ==, foreach, exists, length, !=, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ParallelCollectionRDD.scala: Set(withFilter, map, asInstanceOf, drop, isInstanceOf, <init>, take, ==, slice, foreach, toArray, toSeq, zipWithIndex, length, seq, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala: Set(map, toList, asInstanceOf, nonEmpty, isInstanceOf, filter, <init>, flatMap, foreach, exists, zipWithIndex, toString, length, !=, close, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(withFilter, find, count, size, zip, toSet, map, toMap, filterNot, equals, asInstanceOf, synchronized, forall, mkString, min, nonEmpty, to, isInstanceOf, filter, <init>, max, ++, flatMap, take, ==, foreach, exists, toArray, toSeq, next, toString, length, seq, !=, getClass, close, contains, isEmpty, ne, hasNext, indexOf, eq, sum, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/FileSystemPersistenceEngine.scala: Set(map, filter, <init>, !=, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ==, foreach, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResult.scala: Set(size, asInstanceOf, isInstanceOf, <init>, ==, foreach, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(map, filterNot, asInstanceOf, synchronized, isInstanceOf, filter, <init>, ++, ==, foreach, toSeq, toString, !=, collect, getClass, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala: Set(zip, duplicate, map, asInstanceOf, synchronized, nonEmpty, isInstanceOf, <init>, ==, foreach, next, zipWithIndex, length, !=, close, ne, hasNext, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala: Set(asInstanceOf, isInstanceOf, <init>, foreach, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(count, size, map, filterNot, asInstanceOf, synchronized, sliding, partition, aggregate, forall, mkString, min, fold, nonEmpty, isInstanceOf, filter, GroupedIterator, <init>, max, ++, grouped, flatMap, take, ==, maxBy, clone, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, getClass, contains, isEmpty, ne, hasNext, reduceLeft, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>, exists, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ++, ==, close, contains)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala: Set(find, asInstanceOf, synchronized, nonEmpty, isInstanceOf, <init>, flatMap, ==, clone, foreach, next, toString, length, !=, close, isEmpty, ne, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(withFilter, size, map, toMap, filterNot, asInstanceOf, synchronized, partition, mkString, toIterator, to, isInstanceOf, filter, <init>, max, ++, flatMap, ==, foreach, exists, toArray, toSeq, next, toString, length, !=, collect, getClass, close, contains, isEmpty, ne, hasNext, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(size, map, asInstanceOf, synchronized, mkString, nonEmpty, <init>, buffered, ++, ==, foreach, exists, next, length, !=, getClass, close, isEmpty, ne, hasNext, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(map, toMap, asInstanceOf, filter, <init>, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala: Set(toList, <init>, foreach, toSeq, !=, getClass, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala: Set(withFilter, map, toMap, asInstanceOf, forall, isInstanceOf, filter, <init>, flatMap, ==, foreach, reduce, toString, length, !=, collect, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/RecoveryModeFactory.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(withFilter, size, zip, map, asInstanceOf, finished, synchronized, mkString, isInstanceOf, filter, <init>, buffered, ++, flatMap, ==, foreach, exists, toArray, next, toString, length, !=, close, isEmpty, ne, hasNext, eq, scanLeft, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Dependency.scala: Set(map, asInstanceOf, <init>, foreach, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(size, map, asInstanceOf, sameElements, partition, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, ==, maxBy, foreach, exists, toArray, length, collect, contains, isEmpty, ne, sum, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(withFilter, find, count, size, map, filterNot, toList, asInstanceOf, partition, mkString, min, nonEmpty, to, isInstanceOf, filter, <init>, max, ++, flatMap, take, ==, foreach, exists, toArray, toSeq, toString, length, !=, close, contains, isEmpty, ne, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(withFilter, wait, size, duplicate, map, asInstanceOf, finished, synchronized, mkString, min, nonEmpty, notifyAll, isInstanceOf, <init>, ++, ==, foreach, toArray, !=, contains, isEmpty, ne, eq, sum)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/serializer/DummySerializerInstance.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(map, asInstanceOf, isInstanceOf, <init>, flatMap, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala: Set(synchronized, notifyAll, filter, <init>, ++, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala: Set(asInstanceOf, isInstanceOf, <init>, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(toSet, asInstanceOf, partition, <init>, ==, toSeq, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(withFilter, map, equals, asInstanceOf, isInstanceOf, <init>, ==, foreach, toArray, zipWithIndex, toString, length, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala: Set(size, map, asInstanceOf, isInstanceOf, <init>, ==, !=, isEmpty, ne, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala: Set(asInstanceOf, partition, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala: Set(size, toSet, map, asInstanceOf, synchronized, min, toIterator, isInstanceOf, <init>, ++, flatMap, ==, foreach, toArray, next, toString, !=, close, contains, isEmpty, hasNext, eq, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala: Set(map, asInstanceOf, partition, isInstanceOf, <init>, ==, foreach, toArray, zipWithIndex, length, seq, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Accumulable.scala: Set(asInstanceOf, <init>, ==, exists, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/network/netty/NettyBlockRpcServer.scala: Set(map, asInstanceOf, isInstanceOf, <init>, length, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(withFilter, map, asInstanceOf, isInstanceOf, filter, <init>, max, ==, foreach, !=, getClass, close, contains, isEmpty, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, next, toString, !=, getClass, close, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(withFilter, size, toSet, map, toMap, asInstanceOf, synchronized, partition, forall, mkString, min, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, clone, foreach, exists, toArray, toSeq, zipWithIndex, toString, length, seq, !=, collect, getClass, close, contains, isEmpty, ne, indexOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/PipedRDD.scala: Set(equals, asInstanceOf, mkString, isInstanceOf, <init>, foreach, next, toString, !=, close, ne, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala: Set(size, equals, asInstanceOf, finished, synchronized, partition, isInstanceOf, filter, <init>, ==, foreach, toString, !=, close, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(map, asInstanceOf, <init>, ++, toArray, toSeq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ++, ==, close, contains)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkConf.scala: Set(withFilter, toSet, map, toMap, asInstanceOf, mkString, nonEmpty, collectFirst, isInstanceOf, filter, <init>, ++, flatMap, ==, foreach, exists, toArray, toSeq, toString, length, !=, contains, isEmpty, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	vClassTag, kClassTag.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerSlaveEndpoint.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, onNetworkError, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, self, $isInstanceOf, receive, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, onError, BlockManagerSlaveEndpoint, ==, receiveAndReply, clone, $init$, onDisconnected, toString, logError, !=, onConnected, getClass, logWarning, onStop, ne, onStart, rpcEnv, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(asInstanceOf, synchronized, logTrace, stop, isInstanceOf, <init>, BlockManagerSlaveEndpoint, ==, toString, logError, !=, getClass, logWarning, ne, rpcEnv, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManagerManagedBuffer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, size, equals, asInstanceOf, synchronized, createInputStream, $isInstanceOf, retain, convertToNetty, notifyAll, isInstanceOf, <init>, nioByteBuffer, ==, clone, toString, !=, release, getClass, ne, eq, ##, finalize, hashCode, BlockManagerManagedBuffer.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(size, asInstanceOf, synchronized, isInstanceOf, <init>, nioByteBuffer, ==, toString, !=, getClass, ne, hashCode, BlockManagerManagedBuffer)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	shuffleIdToMapStage, handleSpeculativeTaskSubmitted, handleJobGroupCancelled, notify, submitJob, post, DAGScheduler, jobIdToStageIds, wait, cancelJob, handleWorkerRemoved, createShuffleMapStage, $asInstanceOf, workerRemoved, jobIdToActiveJob, DEFAULT_MAX_CONSECUTIVE_STAGE_ATTEMPTS, numTotalJobs, equals, speculativeTaskSubmitted, asInstanceOf, initializeLogIfNecessary, markMapStageJobAsFinished, synchronized, sc, taskEnded, handleJobCancellation, <init>$default$7, $isInstanceOf, stageIdToStage, waitingStages, onReceive, eventProcessLoop, handleTaskSetFailed, handleStageCancellation, submitMapStage, abortStage, logTrace, taskStarted, executorAdded, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, getPreferredLocs, isInstanceOf, getCacheLocs, resubmitFailedStages, <init>, onError, handleMapStageSubmitted, getShuffleDependencies, cleanUpAfterSchedulerStop, activeJobs, handleGetTaskResult, runApproximateJob, ==, executorLost, clone, handleExecutorAdded, cancelAllJobs, $init$, runJob, isActive, cancelJobGroup, failedStages, toString, markMapStageJobsAsFinished, taskScheduler, logError, runningStages, !=, DAGSchedulerEventProcessLoop, getClass, RESUBMIT_TIMEOUT, logWarning, onStop, nextJobId, handleExecutorLost, killTaskAttempt, start, handleJobSubmitted, maxConsecutiveStageAttempts, ne, onStart, doCancelAllJobs, handleTaskCompletion, executorHeartbeatReceived, taskSetFailed, eq, log, taskGettingResult, cancelStage, unRegisterOutputOnHostOnFetchFailure, outputCommitCoordinator, ##, finalize, hashCode, logDebug, logInfo, metricsSource, handleBeginEvent.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(DAGScheduler, speculativeTaskSubmitted, asInstanceOf, synchronized, sc, taskEnded, taskStarted, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, taskSetFailed, taskGettingResult, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(submitJob, post, DAGScheduler, cancelJob, asInstanceOf, synchronized, sc, submitMapStage, stop, getPreferredLocs, isInstanceOf, <init>, runApproximateJob, ==, clone, cancelAllJobs, runJob, cancelJobGroup, toString, taskScheduler, logError, !=, getClass, logWarning, killTaskAttempt, start, ne, log, cancelStage, logDebug, logInfo, metricsSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerSource.scala: Set(DAGScheduler, numTotalJobs, waitingStages, <init>, activeJobs, failedStages, runningStages)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala: Set(DAGScheduler)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(DAGScheduler, wait, workerRemoved, asInstanceOf, synchronized, sc, executorAdded, stop, isInstanceOf, <init>, ==, executorLost, toString, logError, !=, logWarning, start, ne, executorHeartbeatReceived, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/JobWaiter.scala: Set(DAGScheduler, cancelJob, asInstanceOf, synchronized, <init>, ==, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SplitInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SplitInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SplitInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SplitInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SplitInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, toSplitInfo, wait, $asInstanceOf, path, equals, asInstanceOf, synchronized, $isInstanceOf, inputFormatClazz, notifyAll, isInstanceOf, <init>, ==, hostLocation, clone, toString, length, SplitInfo, !=, getClass, ne, eq, ##, finalize, hashCode, underlyingSplit.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala: Set(toSplitInfo, path, asInstanceOf, inputFormatClazz, isInstanceOf, <init>, ==, hostLocation, toString, SplitInfo, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/io/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, main, initializeLogIfNecessary$default$2, logName, notifyAll, DriverWrapper, isInstanceOf, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/GroupedCountEvaluator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, currentResult, notifyAll, isInstanceOf, <init>, merge, ==, GroupedCountEvaluator, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, GroupedCountEvaluator, clone, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/UnionRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, parentRddIndex, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, partitionEvalTaskSupport, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, UnionPartition, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, UnionRDD, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, isPartitionListingParallel, toString, mapPartitionsInternal, preferredLocations, rdds, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, parentPartition, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(creationSite, parent, partitioner, name, setName, union, map, asInstanceOf, doCheckpoint, synchronized, min, conf, isInstanceOf, filter, <init>, id, max, toDebugString, ++, flatMap, UnionRDD, ==, clone, foreach, zipWithIndex, first, toString, rdds, logError, !=, partitions, collect, getClass, logWarning, isEmpty, ne, mapPartitionsWithIndex, withScope, log, index, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, onNetworkError, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, self, $isInstanceOf, receive, OutputCommitCoordinator, logTrace, handleAskPermissionToCommit, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, <init>, onError, coordinatorRef, OutputCommitCoordinatorEndpoint, canCommit, ==, receiveAndReply, clone, stageStart, $init$, onDisconnected, toString, logError, !=, onConnected, getClass, logWarning, onStop, taskCompleted, isEmpty, ne, onStart, rpcEnv, eq, log, ##, finalize, hashCode, logDebug, stageEnd, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/mapred/SparkHadoopMapRedUtil.scala: Set(OutputCommitCoordinator, <init>, canCommit, logError, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(asInstanceOf, synchronized, OutputCommitCoordinator, logTrace, stop, isInstanceOf, <init>, ==, clone, stageStart, toString, logError, !=, logWarning, taskCompleted, isEmpty, ne, logDebug, stageEnd, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, OutputCommitCoordinator, stop, isInstanceOf, <init>, ==, clone, toString, logError, !=, getClass, logWarning, isEmpty, ne, rpcEnv, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkEnv.scala: Set(asInstanceOf, synchronized, OutputCommitCoordinator, stop, isInstanceOf, <init>, OutputCommitCoordinatorEndpoint, ==, toString, !=, getClass, logWarning, isEmpty, ne, rpcEnv, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExternalClusterManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, canCreate, ExternalClusterManager, $isInstanceOf, notifyAll, initialize, isInstanceOf, ==, clone, createTaskScheduler, toString, !=, getClass, createSchedulerBackend, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkContext.scala: Set(asInstanceOf, synchronized, canCreate, ExternalClusterManager, initialize, isInstanceOf, ==, clone, createTaskScheduler, toString, !=, getClass, createSchedulerBackend, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CollectionsUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CollectionsUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CollectionsUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CollectionsUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/CollectionsUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, CollectionsUtils, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, makeBinarySearch, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Partitioner.scala: Set(CollectionsUtils, asInstanceOf, makeBinarySearch, isInstanceOf, ==, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolRequest.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	assert, notify, clientSparkVersion, wait, $asInstanceOf, equals, asInstanceOf, validate, synchronized, environmentVariables, $isInstanceOf, SubmitRestProtocolRequest, mainClass, notifyAll, isInstanceOf, messageType, <init>, appArgs, ==, clone, toJson, message, toString, !=, CreateSubmissionRequest, getClass, doValidate, ne, action, eq, sparkProperties, appResource, ##, finalize, hashCode, assertFieldIsSet.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, environmentVariables, mainClass, isInstanceOf, messageType, <init>, appArgs, message, toString, CreateSubmissionRequest, sparkProperties, appResource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala: Set(asInstanceOf, validate, environmentVariables, mainClass, isInstanceOf, messageType, <init>, appArgs, ==, toJson, message, !=, CreateSubmissionRequest, ne, sparkProperties, appResource)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingAlgorithm.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingAlgorithm.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingAlgorithm.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingAlgorithm.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/SchedulingAlgorithm.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, comparator, getClass, FIFOSchedulingAlgorithm, SchedulingAlgorithm, ne, eq, FairSchedulingAlgorithm, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala: Set(<init>, ==, !=, comparator, FIFOSchedulingAlgorithm, SchedulingAlgorithm, FairSchedulingAlgorithm)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Task.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, stageAttemptId, _executorDeserializeTime, wait, collectAccumulatorUpdates$default$1, $asInstanceOf, epoch, equals, stageId, asInstanceOf, context, collectAccumulatorUpdates, run, synchronized, $isInstanceOf, reasonIfKilled, notifyAll, partitionId, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, <init>, ==, appAttemptId, clone, setTaskMemoryManager, jobId, toString, preferredLocations, metrics, !=, getClass, appId, runTask, localProperties, kill, _executorDeserializeCpuTime, ne, eq, Task, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(stageAttemptId, epoch, stageId, asInstanceOf, synchronized, partitionId, isInstanceOf, <init>, ==, clone, jobId, toString, preferredLocations, !=, ne, Task)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(stageAttemptId, epoch, stageId, asInstanceOf, synchronized, partitionId, isInstanceOf, <init>, ==, clone, jobId, toString, preferredLocations, !=, ne, Task)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(stageAttemptId, epoch, stageId, asInstanceOf, synchronized, partitionId, isInstanceOf, <init>, ==, toString, preferredLocations, !=, localProperties, ne, Task)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala: Set(stageAttemptId, stageId, asInstanceOf, context, partitionId, <init>, ==, appAttemptId, jobId, !=, appId, localProperties, ne, Task)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(stageId, asInstanceOf, isInstanceOf, <init>, ==, jobId, toString, eq, Task)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(stageAttemptId, wait, stageId, asInstanceOf, synchronized, partitionId, isInstanceOf, <init>, ==, toString, !=, ne, Task)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/Executor.scala: Set(wait, epoch, asInstanceOf, context, collectAccumulatorUpdates, run, synchronized, reasonIfKilled, notifyAll, executorDeserializeTime, isInstanceOf, executorDeserializeCpuTime, <init>, ==, setTaskMemoryManager, metrics, !=, kill, ne, eq, Task)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala: Set(stageAttemptId, stageId, asInstanceOf, context, partitionId, <init>, ==, appAttemptId, jobId, appId, localProperties, ne, Task)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSet.scala: Set(stageAttemptId, stageId, <init>, Task)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(stageAttemptId, epoch, stageId, asInstanceOf, synchronized, partitionId, isInstanceOf, <init>, ==, clone, jobId, toString, preferredLocations, !=, ne, Task)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(stageAttemptId, epoch, stageId, asInstanceOf, synchronized, partitionId, isInstanceOf, <init>, ==, toString, preferredLocations, !=, localProperties, ne, Task)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, Sorter, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, sort, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/AppendOnlyMap.scala: Set(equals, asInstanceOf, Sorter, <init>, ==, !=, sort, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala: Set(asInstanceOf, Sorter, <init>, ==, sort)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerWebUI.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, serverInfo, webUrl, wait, WorkerWebUI, $asInstanceOf, detachHandler, addStaticHandler, equals, getBasePath, asInstanceOf, detachTab, initializeLogIfNecessary, synchronized, bind, $isInstanceOf, attachPage, logTrace, attachHandler, publicHostName, workDir, isTraceEnabled, initializeLogIfNecessary$default$2, handlers, stop, tabs, logName, notifyAll, initialize, STATIC_RESOURCE_BASE, isInstanceOf, attachTab, getSecurityManager, <init>, getTabs, pageToHandlers, ==, clone, DEFAULT_RETAINED_EXECUTORS, securityManager, $init$, removeStaticHandler, toString, logError, !=, getHandlers, getClass, logWarning, detachPage, ne, timeout, DEFAULT_RETAINED_DRIVERS, eq, log, boundPort, ##, finalize, hashCode, logDebug, sslOptions, logInfo, worker.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(WorkerWebUI, asInstanceOf, workDir, isInstanceOf, <init>, ==, toString, logError, ne, logDebug, worker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(WorkerWebUI, asInstanceOf, synchronized, bind, attachHandler, workDir, stop, isInstanceOf, <init>, ==, DEFAULT_RETAINED_EXECUTORS, toString, logError, !=, logWarning, ne, DEFAULT_RETAINED_DRIVERS, log, boundPort, logDebug, logInfo, worker)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(webUrl, WorkerWebUI, <init>, ==, toString, worker)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DriverDescription.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, supervise, copy$default$2, $asInstanceOf, copy$default$5, productArity, equals, cores, asInstanceOf, synchronized, jarUrl, $isInstanceOf, canEqual, copy$default$4, command, productPrefix, notifyAll, isInstanceOf, <init>, DriverDescription, ==, clone, mem, $init$, copy$default$3, copy, toString, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala: Set(cores, command, <init>, DriverDescription, ==, mem, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala: Set(cores, command, <init>, DriverDescription, ==, mem, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala: Set(cores, command, <init>, DriverDescription, mem, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala: Set(cores, asInstanceOf, synchronized, command, isInstanceOf, <init>, DriverDescription, ==, mem, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala: Set(cores, asInstanceOf, isInstanceOf, <init>, DriverDescription, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/WorkerInfo.scala: Set(cores, <init>, DriverDescription, ==, mem)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala: Set(supervise, cores, asInstanceOf, command, isInstanceOf, <init>, DriverDescription, ==, mem, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala: Set(asInstanceOf, command, isInstanceOf, <init>, DriverDescription, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala: Set(supervise, synchronized, jarUrl, command, <init>, DriverDescription, ==, mem, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala: Set(supervise, cores, asInstanceOf, jarUrl, command, isInstanceOf, <init>, DriverDescription, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala: Set(<init>, DriverDescription)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $assertionsDisabled, equals, access$500, notifyAll, <init>, toString, getClass, sort, hashCode, TimSort.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/TimSort.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Sorter.scala: Set(<init>, sort, TimSort)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, union, equals, JavaSparkContextVarargsWorkaround, notifyAll, <init>, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala: Set(union, JavaSparkContextVarargsWorkaround, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDD.scala: Set(union, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RRDD.scala: Set(<init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala: Set(union, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala: Set(wait, <init>, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaNewHadoopRDD.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaHadoopRDD.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/python/WriteInputFormatTestDataGenerator.scala: Set(<init>, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcCallContext.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, reply, logName, notifyAll, isInstanceOf, <init>, ==, RemoteNettyRpcCallContext, clone, $init$, sendFailure, toString, logError, !=, getClass, logWarning, senderAddress, NettyRpcCallContext, ne, eq, log, LocalNettyRpcCallContext, ##, finalize, hashCode, logDebug, logInfo, send.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Inbox.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, sendFailure, toString, logError, !=, logWarning, senderAddress, NettyRpcCallContext, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rpc/netty/Dispatcher.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, RemoteNettyRpcCallContext, logError, !=, logWarning, senderAddress, NettyRpcCallContext, ne, LocalNettyRpcCallContext, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/partial/ApproximateActionListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	taskSucceeded, notify, jobFailed, failure, wait, startTime, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, totalTasks, notifyAll, resultObject, isInstanceOf, <init>, ==, clone, finishedTasks, awaitResult, toString, !=, getClass, ApproximateActionListener, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(taskSucceeded, jobFailed, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, awaitResult, toString, !=, ApproximateActionListener, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/PartitionedPairBuffer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	estimateSize, notify, MAXIMUM_CAPACITY, wait, $asInstanceOf, <init>$default$1, insert, equals, resetSamples, asInstanceOf, PartitionedPairBuffer, synchronized, $isInstanceOf, afterUpdate, notifyAll, isInstanceOf, <init>, ==, clone, partitionedDestructiveSortedIterator, $init$, toString, !=, getClass, ne, destructiveSortedWritablePartitionedIterator, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(estimateSize, insert, asInstanceOf, PartitionedPairBuffer, synchronized, isInstanceOf, <init>, ==, partitionedDestructiveSortedIterator, toString, !=, ne, destructiveSortedWritablePartitionedIterator, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function4.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function4.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function4.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function4.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function4.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, Function4.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/Function4.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ActiveJob.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, ActiveJob, $asInstanceOf, listener, equals, finalStage, asInstanceOf, finished, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, properties, ==, clone, jobId, numFinished, toString, !=, callSite, getClass, ne, numPartitions, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ResultStage.scala: Set(ActiveJob, asInstanceOf, finished, <init>, callSite, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(ActiveJob, listener, finalStage, asInstanceOf, finished, synchronized, isInstanceOf, <init>, properties, ==, clone, jobId, numFinished, toString, !=, callSite, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapStage.scala: Set(ActiveJob, asInstanceOf, <init>, ==, !=, callSite, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/GroupMappingServiceProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/GroupMappingServiceProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/GroupMappingServiceProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/GroupMappingServiceProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/GroupMappingServiceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/GroupMappingServiceProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, getGroups, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, GroupMappingServiceProvider, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/Utils.scala: Set(getGroups, equals, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, getClass, GroupMappingServiceProvider, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala: Set(GroupMappingServiceProvider)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/SecurityFilter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, httpRequest, SecurityFilter, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, filter, <init>, ==, clone, uiRoot, $init$, servletContext, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, FlatMapGroupsFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, FileAppender, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, appendStreamToFile, $isInstanceOf, closeFile, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, appendToFile, isInstanceOf, <init>$default$3, <init>, apply, ==, clone, openFile, $init$, toString, awaitTermination, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala: Set(<init>, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/LogPage.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, logError, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(FileAppender, appendStreamToFile, closeFile, appendToFile, isInstanceOf, <init>, ==, openFile, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala: Set(FileAppender, stop, <init>, apply, ==, toString, logError, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala: Set(FileAppender, closeFile, stop, appendToFile, <init>, openFile, toString, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, setAcceptsNull, getAcceptsNull, wait, $asInstanceOf, equals, deserializeDatum, setImmutable, asInstanceOf, synchronized, $isInstanceOf, setGenerics, notifyAll, isInstanceOf, <init>, ==, clone, compress, copy, toString, !=, isImmutable, serializeDatum, getClass, decompress, ne, eq, write, GenericAvroSerializer, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala: Set(read, asInstanceOf, isInstanceOf, <init>, ==, !=, getClass, ne, write, GenericAvroSerializer)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackend.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, run, synchronized, $isInstanceOf, RBackend, logTrace, isTraceEnabled, main, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, jvmObjectTracker, ==, clone, $init$, toString, logError, !=, getClass, logWarning, close, ne, init, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/api/r/RBackendHandler.scala: Set(asInstanceOf, RBackend, isInstanceOf, <init>, jvmObjectTracker, ==, logError, !=, getClass, logWarning, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/RRunner.scala: Set(run, RBackend, <init>, ==, toString, !=, close, ne, init)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IdGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IdGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IdGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IdGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/IdGenerator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, IdGenerator, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, next, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/storage/BlockManager.scala: Set(IdGenerator, asInstanceOf, synchronized, isInstanceOf, <init>, ==, next, toString, !=, getClass, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	used, notify, MemoryConsumer, wait, equals, taskMemoryManager, allocateArray, notifyAll, allocatePage, freePage, <init>, acquireMemory, freeArray, toString, freeMemory, spill, getClass, getMode, hashCode, getUsed.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(taskMemoryManager, <init>, spill, getClass, hashCode, getUsed)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(taskMemoryManager, <init>, toString, spill, hashCode, getUsed)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala: Set(MemoryConsumer, taskMemoryManager, <init>, acquireMemory, freeMemory, spill)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(<init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/Aggregator.scala: Set(<init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalAppendOnlyMap.scala: Set(taskMemoryManager, <init>, spill, getClass, hashCode, getUsed)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala: Set(taskMemoryManager, <init>, toString, spill, hashCode, getUsed)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala: Set(equals, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ZippedWithIndexRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, prev, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, startIndex, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, ZippedWithIndexRDD, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, ZippedWithIndexRDDPartition, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala: Set(creationSite, zipPartitions, parent, isLocallyCheckpointed, getOrCompute, partitioner, coalesce, name, count, isCheckpointedAndMaterialized, mapPartitions, union, map, subtract, intersection, scope, asInstanceOf, context, getPreferredLocations, doCheckpoint, synchronized, aggregate, compute, min, fold, getOutputDeterministicLevel, iterator, conf, isInstanceOf, filter, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, ++, flatMap, take, groupBy, ==, clone, distinct, retag, foreach, reduce, zipWithIndex, checkpoint, elementClassTag, sample, toString, ZippedWithIndexRDD, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, clearDependencies, isEmpty, ne, getDependencies, mapPartitionsWithIndex, keyBy, getCreationSite, computeOrReadCheckpoint, dependencies, isReliablyCheckpointed, withScope, log, treeAggregate, index, takeOrdered, firstParent, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/LocalRDDCheckpointData.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/ReliableRDDCheckpointData.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ContextCleaner.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/rdd/RDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverState.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverState.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/Client.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/DeployMessage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/JsonProtocol.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/DriverInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/Master.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/master/ui/MasterPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/deploy/worker/ui/WorkerPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/scope/RDDOperationGraph.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, RDDOperationGraph, name, cached, wait, attachChildNode, copy$default$2, $asInstanceOf, setName, outgoingEdges, RDDOperationNode, productArity, equals, toId, attachChildCluster, asInstanceOf, initializeLogIfNecessary, synchronized, childClusters, $isInstanceOf, makeOperationGraph, logTrace, canEqual, copy$default$4, rootCluster, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, isInstanceOf, <init>, fromId, id, apply, ==, clone, $init$, STAGE_CLUSTER_PREFIX, copy$default$3, copy, toString, logError, !=, edges, getClass, logWarning, makeDotFile, copy$default$1, RDDOperationEdge, ne, getCachedNodes, childNodes, eq, incomingEdges, productIterator, log, callsite, ##, finalize, productElement, hashCode, logDebug, logInfo, RDDOperationCluster.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/UIUtils.scala: Set(unapply, RDDOperationGraph, name, outgoingEdges, RDDOperationNode, toId, asInstanceOf, rootCluster, isInstanceOf, <init>, fromId, id, apply, ==, STAGE_CLUSTER_PREFIX, toString, logError, !=, getClass, makeDotFile, RDDOperationEdge, ne, getCachedNodes, incomingEdges, RDDOperationCluster)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(RDDOperationGraph, name, cached, setName, asInstanceOf, rootCluster, isInstanceOf, <init>, id, apply, ==, toString, !=, ne, RDDOperationCluster)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(RDDOperationGraph, name, outgoingEdges, RDDOperationNode, equals, asInstanceOf, childClusters, makeOperationGraph, rootCluster, isInstanceOf, <init>, id, apply, ==, toString, !=, edges, getClass, RDDOperationEdge, ne, childNodes, incomingEdges, logInfo, RDDOperationCluster)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala: Set(RDDOperationGraph, name, asInstanceOf, logName, isInstanceOf, <init>, id, apply, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/storeTypes.scala: Set(RDDOperationGraph, name, attachChildNode, outgoingEdges, RDDOperationNode, attachChildCluster, asInstanceOf, childClusters, rootCluster, isInstanceOf, <init>, id, apply, ==, toString, edges, RDDOperationEdge, childNodes, eq, incomingEdges, RDDOperationCluster)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(RDDOperationGraph, name, asInstanceOf, isInstanceOf, <init>, id, apply, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorLossReason.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorLossReason.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorLossReason.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorLossReason.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/ExecutorLossReason.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, ExecutorLossReason, wait, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, LossReasonPending, synchronized, exitCode, $isInstanceOf, _message, canEqual, ExecutorKilled, productPrefix, reason, workerLost, notifyAll, isInstanceOf, ExecutorExited, <init>, exitCausedByApp, apply, ==, clone, $init$, copy$default$3, copy, message, toString, !=, getClass, copy$default$1, ne, SlaveLost, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala: Set(unapply, ExecutorLossReason, asInstanceOf, synchronized, reason, workerLost, isInstanceOf, <init>, apply, ==, clone, message, toString, !=, ne, SlaveLost)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala: Set(ExecutorLossReason, asInstanceOf, LossReasonPending, synchronized, ExecutorKilled, reason, isInstanceOf, <init>, apply, ==, message, toString, !=, ne, SlaveLost)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Schedulable.scala: Set(ExecutorLossReason)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala: Set(unapply, ExecutorLossReason, asInstanceOf, synchronized, ExecutorKilled, reason, isInstanceOf, ExecutorExited, <init>, exitCausedByApp, apply, ==, message, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/StandaloneSchedulerBackend.scala: Set(ExecutorLossReason, asInstanceOf, reason, workerLost, isInstanceOf, ExecutorExited, <init>, apply, ==, message, toString, !=, SlaveLost)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala: Set(unapply, ExecutorLossReason, asInstanceOf, reason, isInstanceOf, <init>, apply, ==, message, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/Pool.scala: Set(ExecutorLossReason, reason, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala: Set(ExecutorLossReason)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/DAGSchedulerEvent.scala: Set(ExecutorLossReason, asInstanceOf, reason, isInstanceOf, <init>, ==, message, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala: Set(ExecutorLossReason, wait, asInstanceOf, LossReasonPending, synchronized, ExecutorKilled, reason, isInstanceOf, <init>, apply, ==, message, toString, !=, ne, SlaveLost)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala: Set(ExecutorLossReason, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne, SlaveLost, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedClusterMessage.scala: Set(ExecutorLossReason, asInstanceOf, reason, isInstanceOf, <init>, ==, message, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/api/java/function/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, valueOf, fromString, equals, JobExecutionStatus, SUCCEEDED, getDeclaringClass, RUNNING, notifyAll, compareTo, ordinal, values, toString, getClass, FAILED, hashCode, UNKNOWN.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/api.scala: Set(name, JobExecutionStatus, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/StatusAPIImpl.scala: Set(name, JobExecutionStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala: Set(JobExecutionStatus, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala: Set(JobExecutionStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/LiveEntity.scala: Set(name, JobExecutionStatus, values, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/SparkUI.scala: Set(JobExecutionStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala: Set(name, JobExecutionStatus, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala: Set(name, equals, JobExecutionStatus, RUNNING, values, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/JobExecutionStatus.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala: Set(name, JobExecutionStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobPage.scala: Set(name, JobExecutionStatus, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/ui/jobs/JobsTab.scala: Set(JobExecutionStatus, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/SparkJobInfo.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, partitionLengths, notifyAll, <init>, toString, file, getClass, blockId, SpillInfo, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/SpillInfo.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingPolicy.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	MINIMUM_SIZE_BYTES, notify, MINIMUM_INTERVAL_SECONDS, wait, $asInstanceOf, equals, rolloverSizeBytes, RollingPolicy, asInstanceOf, initializeLogIfNecessary, bytesWritten, synchronized, $isInstanceOf, generateRolledOverFileSuffix, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, rolledOver, isInstanceOf, <init>$default$3, formatter, rolloverIntervalMillis, <init>, shouldRollover, ==, clone, $init$, toString, logError, !=, getClass, logWarning, TimeBasedRollingPolicy, ne, SizeBasedRollingPolicy, <init>$default$2, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala: Set(RollingPolicy, isInstanceOf, <init>, ==, logError, !=, logWarning, TimeBasedRollingPolicy, ne, SizeBasedRollingPolicy, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/core/src/main/scala/org/apache/spark/util/logging/RollingFileAppender.scala: Set(RollingPolicy, bytesWritten, generateRolledOverFileSuffix, rolledOver, <init>, shouldRollover, toString, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] New invalidations:[0m
[0m[[0mdebug[0m] [0m[naha] 	Set()[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Previously invalidated, but (transitively) depend on new invalidations:[0m
[0m[[0mdebug[0m] [0m[naha] 	Set()[0m
[0m[[0mdebug[0m] [0m[naha] All newly invalidated sources after taking into account (previously) recompiled sources:Set()[0m
