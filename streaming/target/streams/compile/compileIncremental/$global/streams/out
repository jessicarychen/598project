[0m[[0mdebug[0m] [0m[naha] [0m
[0m[[0mdebug[0m] [0m[naha] Initial source changes: [0m
[0m[[0mdebug[0m] [0m[naha] 	removed:Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	added: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RecurringTimer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/StateMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/UIUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/status/api/v1/streaming/BatchStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/OutputOperationInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/State.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/RateLimiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/StreamingContextState.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextSender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/HdfsUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Duration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RateLimitedOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] 	modified: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated products: Set()[0m
[0m[[0mdebug[0m] [0m[naha] External API changes: API Changes: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Modified binary dependencies: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Initial directly invalidated sources: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RecurringTimer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/StateMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/UIUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/status/api/v1/streaming/BatchStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/OutputOperationInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/State.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/RateLimiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/StreamingContextState.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextSender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/HdfsUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Duration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RateLimitedOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] [0m
[0m[[0mdebug[0m] [0m[naha] Sources indirectly invalidated by:[0m
[0m[[0mdebug[0m] [0m[naha] 	product: Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	binary dep: Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	external source: Set()[0m
[0m[[0mdebug[0m] [0mAll initially invalidated sources: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RecurringTimer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/StateMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/UIUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/status/api/v1/streaming/BatchStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/OutputOperationInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/State.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/RateLimiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/StreamingContextState.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextSender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/HdfsUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Duration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RateLimitedOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RecurringTimer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverMessage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/StateMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/UIUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/status/api/v1/streaming/BatchStatus.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlock.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/OutputOperationInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/State.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/RateLimiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/StreamingContextState.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextSender.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/HdfsUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Duration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RateLimitedOutputStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Recompiling all 109 sources: invalidated sources (109) exceeded 50.0% of all sources[0m
[0m[[0minfo[0m] [0mCompiling 103 Scala sources and 6 Java sources to /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/target/scala-2.11/classes...[0m
[0m[[0mdebug[0m] [0mGetting org.scala-sbt:compiler-interface:0.13.16:component from component compiler for Scala 2.11.8[0m
[0m[[0mdebug[0m] [0mGetting org.scala-sbt:compiler-interface:0.13.16:component from component compiler for Scala 2.11.8[0m
[0m[[0mdebug[0m] [0mRunning cached compiler 293a4252, interfacing (CompilerInterface) with Scala compiler version 2.11.8[0m
[0m[[0mdebug[0m] [0mCalling Scala compiler with arguments  (CompilerInterface):[0m
[0m[[0mdebug[0m] [0m	-unchecked[0m
[0m[[0mdebug[0m] [0m	-deprecation[0m
[0m[[0mdebug[0m] [0m	-feature[0m
[0m[[0mdebug[0m] [0m	-explaintypes[0m
[0m[[0mdebug[0m] [0m	-Yno-adapted-args[0m
[0m[[0mdebug[0m] [0m	-P:genjavadoc:out=/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/target/java[0m
[0m[[0mdebug[0m] [0m	-P:genjavadoc:strictVisibility=true[0m
[0m[[0mdebug[0m] [0m	-Xplugin:/home/vm1/.ivy2/cache/com.typesafe.genjavadoc/genjavadoc-plugin_2.11.8/jars/genjavadoc-plugin_2.11.8-0.10.jar[0m
[0m[[0mdebug[0m] [0m	-target:jvm-1.8[0m
[0m[[0mdebug[0m] [0m	-sourcepath[0m
[0m[[0mdebug[0m] [0m	/usr/local/spark-2.3.2-bin-hadoop2.7[0m
[0m[[0mdebug[0m] [0m	-bootclasspath[0m
[0m[[0mdebug[0m] [0m	/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/classes:/home/vm1/.ivy2/cache/org.scala-lang/scala-library/jars/scala-library-2.11.8.jar[0m
[0m[[0mdebug[0m] [0m	-classpath[0m
[0m[[0mdebug[0m] [0m	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/target/scala-2.11/classes:/usr/local/spark-2.3.2-bin-hadoop2.7/core/target/scala-2.11/spark-core_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/launcher/target/scala-2.11/spark-launcher_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/tags/target/scala-2.11/spark-tags_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/kvstore/target/scala-2.11/spark-kvstore_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/network-common/target/scala-2.11/spark-network-common_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/network-shuffle/target/scala-2.11/spark-network-shuffle_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/unsafe/target/scala-2.11/spark-unsafe_2.11-2.3.2.jar:/home/vm1/.ivy2/cache/org.spark-project.spark/unused/jars/unused-1.0.0.jar:/home/vm1/.ivy2/cache/com.google.guava/guava/bundles/guava-14.0.1.jar:/home/vm1/.ivy2/cache/org.jpmml/pmml-model/jars/pmml-model-1.2.15.jar:/home/vm1/.ivy2/cache/org.jpmml/pmml-schema/jars/pmml-schema-1.2.15.jar:/home/vm1/.ivy2/cache/org.fusesource.leveldbjni/leveldbjni-all/bundles/leveldbjni-all-1.8.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.core/jackson-core/bundles/jackson-core-2.6.7.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.core/jackson-databind/bundles/jackson-databind-2.6.7.1.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.core/jackson-annotations/bundles/jackson-annotations-2.6.7.jar:/home/vm1/.ivy2/cache/io.netty/netty-all/jars/netty-all-4.1.17.Final.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-lang3/jars/commons-lang3-3.5.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-core/bundles/metrics-core-3.1.5.jar:/home/vm1/.ivy2/cache/org.slf4j/slf4j-api/jars/slf4j-api-1.7.16.jar:/home/vm1/.ivy2/cache/com.google.code.findbugs/jsr305/jars/jsr305-1.3.9.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-crypto/jars/commons-crypto-1.0.0.jar:/home/vm1/.ivy2/cache/com.twitter/chill_2.11/jars/chill_2.11-0.8.4.jar:/home/vm1/.ivy2/cache/com.twitter/chill-java/jars/chill-java-0.8.4.jar:/home/vm1/.ivy2/cache/com.esotericsoftware/kryo-shaded/bundles/kryo-shaded-3.0.3.jar:/home/vm1/.ivy2/cache/com.esotericsoftware/minlog/bundles/minlog-1.3.0.jar:/home/vm1/.ivy2/cache/org.objenesis/objenesis/jars/objenesis-2.1.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro/jars/avro-1.7.7.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-core-asl/jars/jackson-core-asl-1.9.13.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-mapper-asl/jars/jackson-mapper-asl-1.9.13.jar:/home/vm1/.ivy2/cache/org.xerial.snappy/snappy-java/bundles/snappy-java-1.1.2.6.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-compress/jars/commons-compress-1.4.1.jar:/home/vm1/.ivy2/cache/org.tukaani/xz/jars/xz-1.0.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro-mapred/jars/avro-mapred-1.7.7-hadoop2.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7-tests.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7.jar:/home/vm1/.ivy2/cache/org.apache.xbean/xbean-asm5-shaded/bundles/xbean-asm5-shaded-4.4.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-client/jars/hadoop-client-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-common/jars/hadoop-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-annotations/jars/hadoop-annotations-2.6.5.jar:/home/vm1/.ivy2/cache/commons-cli/commons-cli/jars/commons-cli-1.2.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-math3/jars/commons-math3-3.4.1.jar:/home/vm1/.ivy2/cache/xmlenc/xmlenc/jars/xmlenc-0.52.jar:/home/vm1/.ivy2/cache/commons-httpclient/commons-httpclient/jars/commons-httpclient-3.1.jar:/home/vm1/.ivy2/cache/commons-io/commons-io/jars/commons-io-2.4.jar:/home/vm1/.ivy2/cache/commons-net/commons-net/jars/commons-net-3.1.jar:/home/vm1/.ivy2/cache/commons-collections/commons-collections/jars/commons-collections-3.2.2.jar:/home/vm1/.ivy2/cache/log4j/log4j/bundles/log4j-1.2.17.jar:/home/vm1/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.6.jar:/home/vm1/.ivy2/cache/commons-configuration/commons-configuration/jars/commons-configuration-1.6.jar:/home/vm1/.ivy2/cache/commons-digester/commons-digester/jars/commons-digester-1.8.jar:/home/vm1/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:/home/vm1/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:/home/vm1/.ivy2/cache/com.google.protobuf/protobuf-java/bundles/protobuf-java-2.5.0.jar:/home/vm1/.ivy2/cache/com.google.code.gson/gson/jars/gson-2.2.4.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-auth/jars/hadoop-auth-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.directory.server/apacheds-kerberos-codec/bundles/apacheds-kerberos-codec-2.0.0-M15.jar:/home/vm1/.ivy2/cache/org.apache.directory.server/apacheds-i18n/bundles/apacheds-i18n-2.0.0-M15.jar:/home/vm1/.ivy2/cache/org.apache.directory.api/api-asn1-api/bundles/api-asn1-api-1.0.0-M20.jar:/home/vm1/.ivy2/cache/org.apache.directory.api/api-util/bundles/api-util-1.0.0-M20.jar:/home/vm1/.ivy2/cache/org.apache.curator/curator-framework/bundles/curator-framework-2.6.0.jar:/home/vm1/.ivy2/cache/org.apache.curator/curator-client/bundles/curator-client-2.6.0.jar:/home/vm1/.ivy2/cache/org.apache.zookeeper/zookeeper/jars/zookeeper-3.4.6.jar:/home/vm1/.ivy2/cache/jline/jline/jars/jline-0.9.94.jar:/home/vm1/.ivy2/cache/io.netty/netty/bundles/netty-3.9.9.Final.jar:/home/vm1/.ivy2/cache/org.apache.curator/curator-recipes/bundles/curator-recipes-2.6.0.jar:/home/vm1/.ivy2/cache/org.htrace/htrace-core/jars/htrace-core-3.0.4.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-hdfs/jars/hadoop-hdfs-2.6.5.jar:/home/vm1/.ivy2/cache/org.mortbay.jetty/jetty-util/jars/jetty-util-6.1.26.jar:/home/vm1/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.9.1.jar:/home/vm1/.ivy2/cache/xml-apis/xml-apis/jars/xml-apis-1.3.04.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-app/jars/hadoop-mapreduce-client-app-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-common/jars/hadoop-mapreduce-client-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-common/jars/hadoop-yarn-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-api/jars/hadoop-yarn-api-2.6.5.jar:/home/vm1/.ivy2/cache/javax.xml.bind/jaxb-api/jars/jaxb-api-2.2.2.jar:/home/vm1/.ivy2/cache/javax.xml.stream/stax-api/jars/stax-api-1.0-2.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-jaxrs/jars/jackson-jaxrs-1.9.13.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-xc/jars/jackson-xc-1.9.13.jar:/home/vm1/.ivy2/cache/com.google.inject/guice/jars/guice-3.0.jar:/home/vm1/.ivy2/cache/javax.inject/javax.inject/jars/javax.inject-1.jar:/home/vm1/.ivy2/cache/aopalliance/aopalliance/jars/aopalliance-1.0.jar:/home/vm1/.ivy2/cache/org.sonatype.sisu.inject/cglib/jars/cglib-2.2.1-v20090111.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-client/jars/hadoop-yarn-client-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-core/jars/hadoop-mapreduce-client-core-2.6.5.jar:/home/vm1/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.16.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-server-common/jars/hadoop-yarn-server-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-shuffle/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-jobclient/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/vm1/.ivy2/cache/org.codehaus.jettison/jettison/bundles/jettison-1.1.jar:/home/vm1/.ivy2/cache/net.java.dev.jets3t/jets3t/jars/jets3t-0.9.4.jar:/home/vm1/.ivy2/cache/org.apache.httpcomponents/httpcore/jars/httpcore-4.4.1.jar:/home/vm1/.ivy2/cache/org.apache.httpcomponents/httpclient/jars/httpclient-4.5.jar:/home/vm1/.ivy2/cache/commons-codec/commons-codec/jars/commons-codec-1.11.jar:/home/vm1/.ivy2/cache/javax.activation/activation/jars/activation-1.1.1.jar:/home/vm1/.ivy2/cache/org.bouncycastle/bcprov-jdk15on/jars/bcprov-jdk15on-1.52.jar:/home/vm1/.ivy2/cache/com.jamesmurty.utils/java-xmlbuilder/jars/java-xmlbuilder-1.1.jar:/home/vm1/.ivy2/cache/net.iharder/base64/jars/base64-2.3.8.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-plus/jars/jetty-plus-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-webapp/jars/jetty-webapp-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-xml/jars/jetty-xml-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-util/jars/jetty-util-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-servlet/jars/jetty-servlet-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-security/jars/jetty-security-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-server/jars/jetty-server-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/javax.servlet/javax.servlet-api/jars/javax.servlet-api-3.1.0.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-http/jars/jetty-http-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-io/jars/jetty-io-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-jndi/jars/jetty-jndi-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-continuation/jars/jetty-continuation-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-proxy/jars/jetty-proxy-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-client/jars/jetty-client-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-servlets/jars/jetty-servlets-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.slf4j/jul-to-slf4j/jars/jul-to-slf4j-1.7.16.jar:/home/vm1/.ivy2/cache/org.slf4j/jcl-over-slf4j/jars/jcl-over-slf4j-1.7.16.jar:/home/vm1/.ivy2/cache/com.ning/compress-lzf/bundles/compress-lzf-1.0.3.jar:/home/vm1/.ivy2/cache/org.lz4/lz4-java/jars/lz4-java-1.4.0.jar:/home/vm1/.ivy2/cache/com.github.luben/zstd-jni/bundles/zstd-jni-1.3.2-2.jar:/home/vm1/.ivy2/cache/org.roaringbitmap/RoaringBitmap/bundles/RoaringBitmap-0.5.11.jar:/home/vm1/.ivy2/cache/org.json4s/json4s-jackson_2.11/jars/json4s-jackson_2.11-3.2.11.jar:/home/vm1/.ivy2/cache/org.json4s/json4s-core_2.11/jars/json4s-core_2.11-3.2.11.jar:/home/vm1/.ivy2/cache/org.json4s/json4s-ast_2.11/jars/json4s-ast_2.11-3.2.11.jar:/home/vm1/.ivy2/cache/org.scala-lang/scalap/jars/scalap-2.11.8.jar:/home/vm1/.ivy2/cache/org.scala-lang/scala-compiler/jars/scala-compiler-2.11.8.jar:/home/vm1/.ivy2/cache/org.scala-lang/scala-reflect/jars/scala-reflect-2.11.8.jar:/home/vm1/.ivy2/cache/org.scala-lang.modules/scala-xml_2.11/bundles/scala-xml_2.11-1.0.4.jar:/home/vm1/.ivy2/cache/org.scala-lang.modules/scala-parser-combinators_2.11/bundles/scala-parser-combinators_2.11-1.0.4.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.core/jersey-client/jars/jersey-client-2.22.2.jar:/home/vm1/.ivy2/cache/javax.ws.rs/javax.ws.rs-api/jars/javax.ws.rs-api-2.0.1.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.core/jersey-common/jars/jersey-common-2.22.2.jar:/home/vm1/.ivy2/cache/javax.annotation/javax.annotation-api/jars/javax.annotation-api-1.2.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.bundles.repackaged/jersey-guava/bundles/jersey-guava-2.22.2.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/hk2-api/jars/hk2-api-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/hk2-utils/jars/hk2-utils-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2.external/aopalliance-repackaged/jars/aopalliance-repackaged-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2.external/javax.inject/jars/javax.inject-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/hk2-locator/jars/hk2-locator-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.javassist/javassist/bundles/javassist-3.18.1-GA.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/osgi-resource-locator/jars/osgi-resource-locator-1.0.1.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.core/jersey-server/jars/jersey-server-2.22.2.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.media/jersey-media-jaxb/jars/jersey-media-jaxb-2.22.2.jar:/home/vm1/.ivy2/cache/javax.validation/validation-api/jars/validation-api-1.1.0.Final.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet/jars/jersey-container-servlet-2.22.2.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet-core/jars/jersey-container-servlet-core-2.22.2.jar:/home/vm1/.ivy2/cache/com.clearspring.analytics/stream/jars/stream-2.7.0.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-jvm/bundles/metrics-jvm-3.1.5.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-json/bundles/metrics-json-3.1.5.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-graphite/bundles/metrics-graphite-3.1.5.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-scala_2.11/bundles/jackson-module-scala_2.11-2.6.7.1.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-paranamer/bundles/jackson-module-paranamer-2.7.9.jar:/home/vm1/.ivy2/cache/com.thoughtworks.paranamer/paranamer/bundles/paranamer-2.8.jar:/home/vm1/.ivy2/cache/org.apache.ivy/ivy/jars/ivy-2.4.0.jar:/home/vm1/.ivy2/cache/oro/oro/jars/oro-2.0.8.jar:/home/vm1/.ivy2/cache/net.razorvine/pyrolite/jars/pyrolite-4.13.jar:/home/vm1/.ivy2/cache/net.sf.py4j/py4j/jars/py4j-0.10.7.jar[0m
[0m[[0mdebug[0m] [0mScala compilation took 14.554230509 s[0m
[0m[[0mdebug[0m] [0mJava compilation took 0.988021131 s[0m
[0m[[0mdebug[0m] [0mJava analysis took 0.026060084 s[0m
[0m[[0mdebug[0m] [0mJava compile + analysis took 1.028548305 s[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/OutputOperationInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/OutputOperationInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/OutputOperationInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/OutputOperationInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/OutputOperationInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, duration, name, wait, batchTime, startTime, copy$default$2, $asInstanceOf, copy$default$5, failureReason, productArity, equals, description, asInstanceOf, synchronized, $isInstanceOf, OutputOperationInfo, canEqual, copy$default$4, productPrefix, notifyAll, isInstanceOf, endTime, <init>, id, ==, clone, copy$default$7, $init$, copy$default$3, copy, toString, !=, getClass, copy$default$1, copy$default$6, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala: Set(name, batchTime, startTime, failureReason, description, OutputOperationInfo, endTime, <init>, id)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala: Set(batchTime, asInstanceOf, OutputOperationInfo, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala: Set(asInstanceOf, OutputOperationInfo, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala: Set(asInstanceOf, OutputOperationInfo, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(startTime, asInstanceOf, synchronized, OutputOperationInfo, isInstanceOf, <init>, id, ==, clone, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala: Set(failureReason, synchronized, OutputOperationInfo, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala: Set(batchTime, synchronized, OutputOperationInfo, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala: Set(name, batchTime, startTime, failureReason, description, asInstanceOf, OutputOperationInfo, isInstanceOf, endTime, <init>, id, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala: Set(startTime, failureReason, asInstanceOf, OutputOperationInfo, endTime, <init>, id, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/RateLimiter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/RateLimiter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/RateLimiter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/RateLimiter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/RateLimiter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/RateLimiter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, RateLimiter, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, updateRate, waitToPush, logName, notifyAll, isInstanceOf, <init>, getCurrentLimit, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala: Set(asInstanceOf, synchronized, RateLimiter, waitToPush, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala: Set(asInstanceOf, updateRate, isInstanceOf, <init>, getCurrentLimit, ==, getClass, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala: Set(synchronized, <init>, ==, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala: Set(asInstanceOf, updateRate, isInstanceOf, <init>, getCurrentLimit, ==, getClass, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/HdfsUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/HdfsUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/HdfsUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/HdfsUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/HdfsUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getFileSegmentLocations, HdfsUtils, wait, $asInstanceOf, equals, asInstanceOf, checkFileExists, synchronized, $isInstanceOf, checkState, notifyAll, isInstanceOf, getInputStream, ==, clone, getFileSystemForPath, toString, !=, getClass, ne, eq, ##, finalize, hashCode, getOutputStream.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala: Set(getFileSegmentLocations, HdfsUtils, asInstanceOf, isInstanceOf, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala: Set(HdfsUtils, synchronized, checkState, getInputStream, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala: Set(HdfsUtils, asInstanceOf, synchronized, isInstanceOf, ==, getFileSystemForPath, toString, !=, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala: Set(HdfsUtils, synchronized, checkState, getOutputStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogReader.scala: Set(HdfsUtils, checkFileExists, synchronized, getInputStream, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, onReceiverStarted, notify, register, getReceiver, getOrCompute, countByValueAndWindow$default$3, window, name, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, streamUID, ReceiverRateController, countByValueAndWindow, map, mustCheckpoint, onStreamingStarted, updateCheckpointData, equals, rememberDuration, getLatestRate, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, stop, onReceiverError, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, onBatchCompleted, isInstanceOf, PluggableInputDStream, filter, onBatchStarted, publish, persist, checkpointData, <init>, id, rateController, zeroTime, lastValidTime, createBlockRDD, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, onBatchSubmitted, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, start, onOutputOperationCompleted, saveAsTextFiles, ne, transform, dependencies, onReceiverStopped, eq, storageLevel, log, generatedRDDs, onOutputOperationStarted, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, PluggableInputDStream, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/State.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/State.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/State.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/State.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/State.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, isRemoved, wait, $asInstanceOf, equals, isUpdated, asInstanceOf, synchronized, $isInstanceOf, wrapTimingOutState, getOption, notifyAll, isInstanceOf, <init>, StateImpl, wrap, isTimingOut, remove, ==, clone, exists, toString, !=, get, getClass, update, State, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala: Set(isRemoved, isUpdated, asInstanceOf, wrapTimingOutState, isInstanceOf, <init>, StateImpl, wrap, remove, ==, exists, toString, get, State, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, State, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, State, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, PythonTransformFunctionSerializer, register, toRDDQueue, getOrCompute, countByValueAndWindow$default$3, window, asJavaDStream, count, windowDuration, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, PythonTransformed2DStream, union, clearCheckpointData, callForeachRDD, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, stopStreamingContextIfPythonProcessIsDead, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, loads, logTrace, isTraceEnabled, deserialize, PythonTransformedDStream, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, PythonStateDStream, isInstanceOf, PythonReducedWindowedDStream, filter, TransformFunction, PythonTransformFunction, persist, checkpointData, <init>, zeroTime, apply, flatMap, dumps, foreachRDD, countByValue$default$1, PythonDStream, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, getLastFailure, registerSerializer, $init$, pfunc, checkpoint, countByValue, func, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, call, serialize, transform, dependencies, invReduceFunc, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(stopStreamingContextIfPythonProcessIsDead, asInstanceOf, synchronized, ssc, isInstanceOf, <init>, PythonDStream, ==, clone, toString, graph, logError, !=, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala: Set(clearCheckpointData, updateCheckpointData, stopStreamingContextIfPythonProcessIsDead, asInstanceOf, synchronized, ssc, isInstanceOf, filter, <init>, zeroTime, apply, PythonDStream, ==, clearMetadata, checkpointDuration, toString, graph, !=, logWarning, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, compute, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, PIDRateEstimator, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala: Set(PIDRateEstimator, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ContextWaiter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, waitForStopOrError$default$1, synchronized, notifyError, $isInstanceOf, ContextWaiter, waitForStopOrError, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, notifyStop, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(asInstanceOf, synchronized, notifyError, ContextWaiter, isInstanceOf, <init>, ==, clone, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(synchronized, ContextWaiter, waitForStopOrError, <init>, ==, clone, toString, !=, notifyStop, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, FlatMapValuedDStream, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala: Set(window, FlatMapValuedDStream, slideDuration, map, asInstanceOf, context, ssc, <init>, flatMap, foreachRDD, !=, transformWith)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, ShuffledDStream, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala: Set(window, slideDuration, map, asInstanceOf, context, ShuffledDStream, ssc, <init>, flatMap, foreachRDD, !=, transformWith)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RecurringTimer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RecurringTimer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RecurringTimer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RecurringTimer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RecurringTimer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	RecurringTimer, notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, getRestartTime, logTrace, isTraceEnabled, main, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, getStartTime, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, start, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala: Set(RecurringTimer, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, start, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala: Set(RecurringTimer, asInstanceOf, synchronized, getRestartTime, stop, isInstanceOf, getStartTime, <init>, ==, toString, !=, logWarning, start, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala: Set(RecurringTimer, synchronized, stop, <init>, ==, !=, start, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onReceiverStarted, notify, StreamingListenerOutputOperationStarted, StreamingListenerReceiverError, StreamingListenerReceiverStopped, wait, $asInstanceOf, StreamingListener, productArity, onStreamingStarted, equals, asInstanceOf, outputOperationInfo, synchronized, $isInstanceOf, StreamingListenerReceiverStarted, receiverInfo, canEqual, printStats, batchInfos, productPrefix, onReceiverError, StreamingListenerBatchCompleted, notifyAll, onBatchCompleted, isInstanceOf, onBatchStarted, StreamingListenerOutputOperationCompleted, <init>, ==, clone, extractDistribution, StreamingListenerEvent, onBatchSubmitted, $init$, copy, toString, StreamingListenerStreamingStarted, StatsReportListener, !=, StreamingListenerBatchStarted, time, getClass, copy$default$1, batchInfo, onOutputOperationCompleted, ne, StreamingListenerBatchSubmitted, showMillisDistribution, onReceiverStopped, eq, productIterator, onOutputOperationStarted, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(StreamingListenerOutputOperationStarted, StreamingListener, asInstanceOf, synchronized, StreamingListenerBatchCompleted, isInstanceOf, StreamingListenerOutputOperationCompleted, <init>, ==, clone, StreamingListenerEvent, toString, !=, StreamingListenerBatchStarted, time, StreamingListenerBatchSubmitted, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala: Set(<init>, ==, !=, time)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(asInstanceOf, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(StreamingListener, synchronized, <init>, ==, clone, StreamingListenerEvent, toString, StreamingListenerStreamingStarted, !=, time, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(StreamingListener, synchronized, <init>, ==, clone, StreamingListenerEvent, toString, StreamingListenerStreamingStarted, !=, time, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala: Set(onReceiverStarted, StreamingListenerOutputOperationStarted, StreamingListenerReceiverError, StreamingListenerReceiverStopped, StreamingListener, onStreamingStarted, outputOperationInfo, StreamingListenerReceiverStarted, receiverInfo, onReceiverError, StreamingListenerBatchCompleted, onBatchCompleted, onBatchStarted, StreamingListenerOutputOperationCompleted, <init>, onBatchSubmitted, StreamingListenerStreamingStarted, StreamingListenerBatchStarted, time, batchInfo, onOutputOperationCompleted, StreamingListenerBatchSubmitted, onReceiverStopped, onOutputOperationStarted)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(StreamingListener, synchronized, <init>, ==, clone, StreamingListenerEvent, toString, StreamingListenerStreamingStarted, !=, time, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala: Set(StreamingListener, StreamingListenerBatchCompleted, <init>, time, batchInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(StreamingListenerOutputOperationStarted, StreamingListener, asInstanceOf, synchronized, StreamingListenerBatchCompleted, isInstanceOf, StreamingListenerOutputOperationCompleted, <init>, ==, clone, StreamingListenerEvent, toString, !=, StreamingListenerBatchStarted, time, StreamingListenerBatchSubmitted, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala: Set(StreamingListener, synchronized, StreamingListenerBatchCompleted, <init>, ==, !=, batchInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala: Set(StreamingListenerOutputOperationStarted, StreamingListenerReceiverError, StreamingListenerReceiverStopped, StreamingListener, outputOperationInfo, synchronized, StreamingListenerReceiverStarted, receiverInfo, StreamingListenerBatchCompleted, StreamingListenerOutputOperationCompleted, <init>, ==, StreamingListenerStreamingStarted, !=, StreamingListenerBatchStarted, time, batchInfo, ne, StreamingListenerBatchSubmitted)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(StreamingListenerReceiverError, StreamingListenerReceiverStopped, asInstanceOf, synchronized, StreamingListenerReceiverStarted, isInstanceOf, <init>, ==, StreamingListenerEvent, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListenerBus.scala: Set(onReceiverStarted, StreamingListenerOutputOperationStarted, StreamingListenerReceiverError, StreamingListenerReceiverStopped, StreamingListener, onStreamingStarted, asInstanceOf, StreamingListenerReceiverStarted, onReceiverError, StreamingListenerBatchCompleted, onBatchCompleted, isInstanceOf, onBatchStarted, StreamingListenerOutputOperationCompleted, <init>, ==, StreamingListenerEvent, onBatchSubmitted, toString, StreamingListenerStreamingStarted, StreamingListenerBatchStarted, onOutputOperationCompleted, StreamingListenerBatchSubmitted, onReceiverStopped, eq, onOutputOperationStarted)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala: Set(StreamingListener, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala: Set(StreamingListener, asInstanceOf, <init>, time)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(StreamingListener, synchronized, <init>, ==, clone, StreamingListenerEvent, toString, StreamingListenerStreamingStarted, !=, time, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala: Set(synchronized, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, !=, time)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(StreamingListenerReceiverError, StreamingListenerReceiverStopped, asInstanceOf, synchronized, StreamingListenerReceiverStarted, isInstanceOf, <init>, ==, StreamingListenerEvent, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala: Set(asInstanceOf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala: Set(StreamingListener, asInstanceOf, <init>, time)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(StreamingListenerOutputOperationStarted, StreamingListener, asInstanceOf, synchronized, StreamingListenerBatchCompleted, isInstanceOf, StreamingListenerOutputOperationCompleted, <init>, ==, clone, StreamingListenerEvent, toString, !=, StreamingListenerBatchStarted, time, StreamingListenerBatchSubmitted, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(StreamingListener, synchronized, <init>, ==, clone, StreamingListenerEvent, toString, StreamingListenerStreamingStarted, !=, time, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(StreamingListener, synchronized, <init>, ==, clone, StreamingListenerEvent, toString, StreamingListenerStreamingStarted, !=, time, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala: Set(synchronized, receiverInfo, <init>, time, batchInfo, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala: Set(asInstanceOf, receiverInfo, batchInfos, isInstanceOf, <init>, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala: Set(asInstanceOf, outputOperationInfo, isInstanceOf, <init>, ==, toString, !=, batchInfo, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala: Set(StreamingListener, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala: Set(<init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(StreamingListener, synchronized, <init>, ==, clone, StreamingListenerEvent, toString, StreamingListenerStreamingStarted, !=, time, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, FileBasedWriteAheadLogWriter, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, close, ne, eq, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala: Set(FileBasedWriteAheadLogWriter, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, getClass, close, eq, write)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, currentCheckpointFiles, equals, restore, asInstanceOf, initializeLogIfNecessary, data, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, cleanup, ==, clone, $init$, toString, logError, !=, getClass, logWarning, update, ne, DStreamCheckpointData, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(synchronized, <init>, ==, clone, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(restore, asInstanceOf, synchronized, isInstanceOf, <init>, cleanup, ==, !=, getClass, logWarning, update, DStreamCheckpointData, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala: Set(asInstanceOf, data, synchronized, isInstanceOf, <init>, ==, toString, logError, getClass, logWarning, ne, DStreamCheckpointData, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	scalaIntToJavaLong, kManifest, vManifest, kClassTag, vClassTag, fromInputDStream.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onExecutorUnblacklisted, batchDuration, onReceiverStarted, numActiveReceivers, notify, batchTimeToOutputOpIdSparkJobIdPair, wait, startTime, $asInstanceOf, onJobEnd, onApplicationEnd, numTotalProcessedRecords, lastReceivedBatchRecords, lastCompletedBatch, onStreamingStarted, equals, onTaskEnd, getBatchUIData, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, synchronized, $isInstanceOf, numReceivers, numInactiveReceivers, OutputOpId, streamIds, receiverInfo, onOtherEvent, StreamingJobProgressListener, onTaskStart, onReceiverError, runningBatches, notifyAll, onExecutorRemoved, onExecutorAdded, onBatchCompleted, isInstanceOf, onBatchStarted, <init>, onBlockUpdated, ==, onBlockManagerRemoved, SparkJobId, clone, retainedBatches, onExecutorMetricsUpdate, onEnvironmentUpdate, onBatchSubmitted, onBlockManagerAdded, $init$, waitingBatches, onNodeBlacklisted, onApplicationStart, onSpeculativeTaskSubmitted, toString, retainedCompletedBatches, !=, numTotalCompletedBatches, getClass, onExecutorBlacklisted, numUnprocessedBatches, onUnpersistRDD, onOutputOperationCompleted, receivedRecordRateWithBatchTime, ne, onStageSubmitted, onReceiverStopped, eq, onNodeUnblacklisted, lastReceivedBatch, onOutputOperationStarted, ##, finalize, numTotalReceivedRecords, hashCode, streamName.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(batchDuration, synchronized, StreamingJobProgressListener, <init>, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala: Set(batchDuration, numActiveReceivers, startTime, synchronized, numReceivers, numInactiveReceivers, streamIds, receiverInfo, StreamingJobProgressListener, runningBatches, <init>, retainedBatches, waitingBatches, retainedCompletedBatches, numTotalCompletedBatches, receivedRecordRateWithBatchTime, ne, numTotalReceivedRecords, streamName)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala: Set(batchDuration, numActiveReceivers, startTime, numReceivers, numInactiveReceivers, OutputOpId, StreamingJobProgressListener, <init>, SparkJobId, numTotalCompletedBatches, streamName)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala: Set(batchDuration, numActiveReceivers, startTime, numTotalProcessedRecords, getBatchUIData, asInstanceOf, numReceivers, numInactiveReceivers, OutputOpId, receiverInfo, StreamingJobProgressListener, runningBatches, isInstanceOf, <init>, ==, SparkJobId, retainedBatches, waitingBatches, toString, retainedCompletedBatches, numTotalCompletedBatches, numUnprocessedBatches, receivedRecordRateWithBatchTime, ne, numTotalReceivedRecords, streamName)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala: Set(batchDuration, getBatchUIData, asInstanceOf, synchronized, OutputOpId, StreamingJobProgressListener, isInstanceOf, <init>, ==, SparkJobId, toString, ne, eq, streamName)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala: Set(startTime, asInstanceOf, OutputOpId, StreamingJobProgressListener, isInstanceOf, <init>, ==, SparkJobId, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala: Set(StreamingJobProgressListener, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala: Set(asInstanceOf, synchronized, StreamingJobProgressListener, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala: Set(OutputOpId, StreamingJobProgressListener, runningBatches, <init>, waitingBatches, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala: Set(numTotalProcessedRecords, lastReceivedBatchRecords, lastCompletedBatch, numReceivers, StreamingJobProgressListener, runningBatches, <init>, waitingBatches, retainedCompletedBatches, numTotalCompletedBatches, numUnprocessedBatches, lastReceivedBatch, numTotalReceivedRecords)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, scheduleReceivers, ==, clone, toString, !=, getClass, ne, ReceiverSchedulingPolicy, eq, rescheduleReceiver, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, scheduleReceivers, ==, toString, !=, ne, ReceiverSchedulingPolicy, eq, rescheduleReceiver)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	classTag, scalaIntToJavaLong, fromInputDStream.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, create, compute, notifyAll, isInstanceOf, RateEstimator, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala: Set(RateEstimator, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/PIDRateEstimator.scala: Set(synchronized, RateEstimator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(asInstanceOf, create, RateEstimator, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala: Set(compute, RateEstimator)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/StreamingContextState.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/StreamingContextState.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/StreamingContextState.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/StreamingContextState.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/StreamingContextState.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ACTIVE, STOPPED, name, wait, StreamingContextState, valueOf, equals, getDeclaringClass, INITIALIZED, notifyAll, compareTo, ordinal, values, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(name, StreamingContextState, values, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/StreamingContextState.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala: Set(StreamingContextState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, StreamingContextState, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, StateDStream, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala: Set(window, slideDuration, map, asInstanceOf, context, ssc, StateDStream, <init>, flatMap, foreachRDD, !=, transformWith)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, reportError, wait, streamId, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, store, isStarted, stop, notifyAll, isInstanceOf, isStopped, setReceiverId, <init>, preferredLocation, ==, clone, supervisor, toString, attachSupervisor, !=, getClass, onStop, restart, ne, onStart, Receiver, eq, storageLevel, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(synchronized, stop, <init>, ==, clone, toString, !=, ne, Receiver, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala: Set(streamId, asInstanceOf, isInstanceOf, <init>, preferredLocation, ==, ne, Receiver)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(synchronized, stop, <init>, ==, clone, toString, !=, ne, Receiver, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala: Set(streamId, synchronized, stop, <init>, ==, attachSupervisor, !=, onStop, onStart, Receiver)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala: Set(<init>, Receiver)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala: Set(synchronized, store, isStopped, <init>, ==, !=, onStop, restart, onStart, Receiver, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(asInstanceOf, <init>, !=, Receiver)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala: Set(reportError, streamId, asInstanceOf, stop, isInstanceOf, isStopped, <init>, ==, getClass, Receiver, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(reportError, streamId, asInstanceOf, synchronized, isStarted, stop, isInstanceOf, setReceiverId, <init>, preferredLocation, ==, supervisor, toString, !=, ne, Receiver, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala: Set(asInstanceOf, store, <init>, ==, !=, Receiver, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala: Set(asInstanceOf, stop, <init>, Receiver, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(synchronized, stop, <init>, ==, clone, toString, !=, ne, Receiver, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/status/api/v1/streaming/BatchStatus.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/status/api/v1/streaming/BatchStatus.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/status/api/v1/streaming/BatchStatus.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/status/api/v1/streaming/BatchStatus.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/status/api/v1/streaming/BatchStatus.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, valueOf, fromString, equals, getDeclaringClass, COMPLETED, notifyAll, compareTo, PROCESSING, QUEUED, ordinal, BatchStatus, values, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/status/api/v1/streaming/BatchStatus.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala: Set(name, BatchStatus, values, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, FlatMappedDStream, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(reduceByWindow, creationSite, register, getOrCompute, window, setGraph, slideDuration, union, clearCheckpointData, map, mustCheckpoint, updateCheckpointData, rememberDuration, remember, baseScope, asInstanceOf, print, context, synchronized, FlatMappedDStream, compute, isTimeValid, createRDDWithLocalProperties, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, ==, validateAtStart, clearMetadata, slice, checkpointDuration, reduce, checkpoint, graph, !=, getClass, isInitialized, logWarning, parentRememberDuration, transformWith, repartition, transform, dependencies, storageLevel, generatedRDDs, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, WriteAheadLogBasedStoreResult, reduceOption, wait, foldRight, copy$default$2, takeWhile, $asInstanceOf, minBy, WriteAheadLogBasedBlockHandler, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, productArity, toMap, filterNot, equals, toList, isTraversableAgain, ReceivedBlockStoreResult, asInstanceOf, sameElements, initializeLogIfNecessary, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, walRecordHandle, fold, numRecords, logTrace, nonEmpty, canEqual, BlockManagerBasedBlockHandler, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, stop, logName, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, BlockManagerBasedStoreResult, filter, cleanupOldBlocks, CountingIterator, <init>$default$8, GroupedIterator, <init>, toStream, max, buffered, ReceivedBlockHandler, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, checkpointDirToLogDir, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, copy$default$3, copy, toString, copyToArray, length, seq, logError, !=, collect, getClass, logWarning, copy$default$1, hasDefiniteSize, storeBlock, patch, foldLeft, contains, isEmpty, ne, blockId, withPartial, reversed, hasNext, indexOf, reduceLeft, eq, productIterator, sum, log, ##, scanLeft, finalize, productElement, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala: Set(toSet, map, toMap, ReceivedBlockStoreResult, asInstanceOf, synchronized, forall, mkString, logTrace, nonEmpty, isInstanceOf, filter, <init>, ==, clone, checkpointDirToLogDir, foreach, toSeq, toString, logError, logWarning, isEmpty, ne, blockId, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala: Set(WriteAheadLogBasedStoreResult, ReceivedBlockStoreResult, asInstanceOf, walRecordHandle, numRecords, isInstanceOf, <init>, ==, toString, isEmpty, blockId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala: Set(WriteAheadLogBasedBlockHandler, map, ReceivedBlockStoreResult, asInstanceOf, numRecords, BlockManagerBasedBlockHandler, stop, isInstanceOf, filter, cleanupOldBlocks, <init>, ReceivedBlockHandler, ==, foreach, getClass, logWarning, storeBlock, isEmpty, blockId, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, MappedDStream, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(reduceByWindow, creationSite, register, getOrCompute, window, setGraph, slideDuration, union, clearCheckpointData, map, mustCheckpoint, updateCheckpointData, rememberDuration, remember, baseScope, asInstanceOf, print, context, synchronized, compute, isTimeValid, createRDDWithLocalProperties, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, ==, validateAtStart, clearMetadata, slice, checkpointDuration, reduce, checkpoint, MappedDStream, graph, !=, getClass, isInitialized, logWarning, parentRememberDuration, transformWith, repartition, transform, dependencies, storageLevel, generatedRDDs, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, duration, wait, currentInterval, <=, $asInstanceOf, equals, asInstanceOf, <, synchronized, $isInstanceOf, >=, Interval, beginTime, notifyAll, -, isInstanceOf, endTime, <init>, ==, clone, toString, +, !=, getClass, ne, eq, >, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala: Set(Interval, -, <init>, +)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(duration, <=, asInstanceOf, <, synchronized, >=, Interval, beginTime, -, isInstanceOf, endTime, <init>, ==, +, !=, getClass, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala: Set(asInstanceOf, synchronized, >=, Interval, beginTime, -, isInstanceOf, endTime, <init>, ==, +, !=, getClass, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala: Set(asInstanceOf, Interval, beginTime, -, endTime, <init>, +, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlock.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlock.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlock.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlock.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlock.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, productArity, equals, ReceivedBlock, asInstanceOf, synchronized, $isInstanceOf, canEqual, ByteBufferBlock, productPrefix, iterator, notifyAll, isInstanceOf, ArrayBufferBlock, <init>, arrayBuffer, ==, clone, byteBuffer, $init$, copy, toString, !=, getClass, copy$default$1, ne, eq, productIterator, IteratorBlock, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala: Set(ReceivedBlock, asInstanceOf, ByteBufferBlock, iterator, isInstanceOf, ArrayBufferBlock, <init>, arrayBuffer, ==, byteBuffer, toString, !=, getClass, eq, IteratorBlock)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala: Set(ReceivedBlock, asInstanceOf, ByteBufferBlock, iterator, isInstanceOf, ArrayBufferBlock, <init>, arrayBuffer, ==, getClass, IteratorBlock)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/api.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	numRetainedCompletedBatches, batchDuration, numActiveReceivers, notify, duration, avgInputRate, name, wait, numProcessedRecords, batchTime, startTime, streamId, $asInstanceOf, BatchInfo, failureReason, equals, description, numActiveBatches, lastErrorMessage, schedulingDelay, outputOpId, ReceiverInfo, asInstanceOf, numTotalOutputOps, synchronized, avgSchedulingDelay, $isInstanceOf, numReceivers, OutputOperationInfo, numInactiveReceivers, avgProcessingTime, lastErrorTime, executorId, avgEventRate, notifyAll, numCompletedOutputOps, lastError, isInstanceOf, numFailedOutputOps, executorHost, endTime, <init>, StreamingStatistics, processingTime, totalDelay, ==, numReceivedRecords, eventRates, clone, status, isActive, toString, !=, numTotalCompletedBatches, getClass, batchId, jobIds, numActiveOutputOps, avgTotalDelay, ne, inputSize, eq, ##, finalize, hashCode, firstFailureReason, streamName.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala: Set(batchDuration, numActiveReceivers, duration, avgInputRate, name, batchTime, startTime, streamId, BatchInfo, failureReason, description, lastErrorMessage, schedulingDelay, outputOpId, ReceiverInfo, asInstanceOf, avgSchedulingDelay, numReceivers, OutputOperationInfo, numInactiveReceivers, avgProcessingTime, lastErrorTime, executorId, avgEventRate, lastError, isInstanceOf, endTime, <init>, StreamingStatistics, totalDelay, ==, eventRates, status, toString, numTotalCompletedBatches, batchId, jobIds, avgTotalDelay, ne, firstFailureReason, streamName)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onReceiverStarted, notify, wait, $asInstanceOf, streamUID, onStreamingStarted, equals, getLatestRate, asInstanceOf, synchronized, $isInstanceOf, RateController, onReceiverError, notifyAll, onBatchCompleted, isInstanceOf, onBatchStarted, publish, isBackPressureEnabled, <init>, ==, clone, onBatchSubmitted, $init$, toString, !=, getClass, onOutputOperationCompleted, ne, onReceiverStopped, eq, onOutputOperationStarted, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(asInstanceOf, synchronized, RateController, isInstanceOf, <init>, ==, clone, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala: Set(RateController, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(streamUID, asInstanceOf, RateController, isBackPressureEnabled, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(synchronized, <init>, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(synchronized, <init>, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(synchronized, <init>, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala: Set(synchronized, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(streamUID, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala: Set(asInstanceOf, <init>, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala: Set(asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(synchronized, <init>, ==, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Duration.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Duration.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Duration.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Duration.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Duration.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, *, <=, $asInstanceOf, minus, less, Duration, productArity, plus, equals, Durations, isZero, asInstanceOf, prettyPrint, <, seconds, synchronized, $isInstanceOf, lessEq, >=, div, min, canEqual, Milliseconds, productPrefix, notifyAll, -, isInstanceOf, minutes, <init>, max, apply, millis$1, ==, clone, times, $init$, isMultipleOf, copy, toString, +, !=, getClass, copy$default$1, greater, milliseconds, ne, greaterEq, eq, productIterator, /, >, ##, toFormattedString, finalize, Minutes, Seconds, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala: Set(Duration, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/rate/RateEstimator.scala: Set(Duration, <init>, ==, milliseconds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(Duration, synchronized, <init>, apply, ==, clone, toString, +, !=, milliseconds, ne, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala: Set(Duration, asInstanceOf, isInstanceOf, <init>, apply, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala: Set(Duration, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(*, <=, Duration, asInstanceOf, <, synchronized, >=, -, isInstanceOf, <init>, apply, ==, isMultipleOf, +, !=, getClass, milliseconds, /, >, Seconds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala: Set(Duration, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala: Set(*, <=, Duration, asInstanceOf, <, synchronized, -, isInstanceOf, <init>, max, apply, ==, toString, +, getClass, milliseconds, ne, /, >, Seconds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala: Set(Duration, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala: Set(Duration, asInstanceOf, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala: Set(Duration, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala: Set(Duration, asInstanceOf, -, <init>, apply, isMultipleOf, +, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala: Set(Duration, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala: Set(Duration, <, <init>, apply, ==, +, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala: Set(Duration, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala: Set(Duration, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala: Set(Duration, asInstanceOf, <init>, apply, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala: Set(Duration, asInstanceOf, isInstanceOf, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala: Set(Duration, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala: Set(Duration, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, +, !=, milliseconds)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala: Set(Duration, asInstanceOf, isInstanceOf, <init>, apply, ==, +, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala: Set(*, Duration, asInstanceOf, -, isInstanceOf, <init>, apply, ==, !=, milliseconds, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(Duration, asInstanceOf, <, <init>, apply, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(Duration, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, +, !=, milliseconds, eq, /)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala: Set(*, Duration, synchronized, -, <init>, apply, ==, +, !=, milliseconds, ne, /, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala: Set(Duration, -, <init>, apply, isMultipleOf, +)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala: Set(*, Duration, asInstanceOf, <, synchronized, -, isInstanceOf, <init>, apply, ==, isMultipleOf, toString, +, !=, milliseconds, eq, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala: Set(Duration, asInstanceOf, -, isInstanceOf, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala: Set(*, <=, Duration, asInstanceOf, <, >=, -, isInstanceOf, <init>, ==, toString, +, milliseconds, eq, /, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala: Set(Duration, asInstanceOf, <, synchronized, -, <init>, apply, ==, toString, +, !=, milliseconds, ne, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala: Set(Duration, asInstanceOf, isInstanceOf, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala: Set(<=, Duration, <, -, <init>, ==, +, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala: Set(Duration, asInstanceOf, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala: Set(Duration, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala: Set(*, Duration, asInstanceOf, synchronized, >=, -, isInstanceOf, <init>, apply, ==, +, !=, getClass, milliseconds, >)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	fromDStream, classTag, scalaIntToJavaLong.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, unapply, curried, wait, startTime, copy$default$2, $asInstanceOf, path, productArity, equals, seqToParIterator, logFileRegex, asInstanceOf, initializeLogIfNecessary, clean, synchronized, $isInstanceOf, tupled, logTrace, canEqual, readAll, isTraceEnabled, initializeLogIfNecessary$default$2, FileBasedWriteAheadLog, productPrefix, logName, notifyAll, isInstanceOf, endTime, getCallerName, <init>, apply, ==, clone, $init$, copy$default$3, copy, LogInfo, toString, timeToLogFile, logError, !=, getClass, logWarning, copy$default$1, logFilesTologInfo, close, ne, eq, productIterator, write, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala: Set(unapply, asInstanceOf, FileBasedWriteAheadLog, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	fromPairDStream, kManifest, vManifest, scalaIntToJavaLong.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/BlockGenerator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	BlockGeneratorListener, notify, BlockGenerator, onGenerateBlock, wait, $asInstanceOf, onAddData, equals, asInstanceOf, initializeLogIfNecessary, addData, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, updateRate, waitToPush, addDataWithCallback, stop, logName, notifyAll, isInstanceOf, onPushBlock, isStopped, <init>, onError, getCurrentLimit, ==, clone, $init$, isActive, toString, logError, !=, getClass, logWarning, addMultipleDataWithCallback, start, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala: Set(BlockGeneratorListener, BlockGenerator, synchronized, stop, <init>, ==, logError, !=, logWarning, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala: Set(BlockGeneratorListener, BlockGenerator, asInstanceOf, addData, updateRate, stop, isInstanceOf, isStopped, <init>, getCurrentLimit, ==, getClass, logWarning, start, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, start, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, readStreamHeader, read, CheckpointWriteHandler, readShort, readLong, enableResolveObject, Checkpoint, jars, wait, $asInstanceOf, registerValidation, readUTF, readBoolean, master, equals, readFields, asInstanceOf, initializeLogIfNecessary, run, createSparkConf, readClassDescriptor, validate, synchronized, $isInstanceOf, readObject, CheckpointWriter, readFloat, skip, logTrace, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, framework, skipBytes, stop, readUnshared, logName, notifyAll, markSupported, isInstanceOf, compressionCodec, resolveObject, readByte, readLine, readObjectOverride, defaultReadObject, <init>, REGEX, pendingTimes, checkpointTime, ==, clone, read$default$4, checkpointDuration, ObjectInputStreamWithLoader, $init$, MAX_ATTEMPTS, resolveProxyClass, getCheckpointFiles, resolveClass, reset, readDouble, available, checkpointBackupFile, sparkConfPairs, readFully, toString, graph, logError, !=, readTypeString, getClass, logWarning, checkpointDir, mark, close, readUnsignedShort, executor, checkpointFile, ne, serialize, readUnsignedByte, CheckpointReader, readChar, eq, write, readInt, log, PREFIX, getCheckpointFiles$default$2, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala: Set(Checkpoint, asInstanceOf, synchronized, CheckpointWriter, stop, isInstanceOf, <init>, pendingTimes, checkpointTime, ==, checkpointDuration, toString, graph, !=, logWarning, checkpointDir, eq, write, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(read, Checkpoint, jars, master, createSparkConf, validate, synchronized, stop, <init>, ==, clone, checkpointDuration, toString, graph, logError, !=, logWarning, checkpointDir, ne, serialize, CheckpointReader, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	fromReceiverInputDStream, classTag, scalaIntToJavaLong.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	SCALING_INTERVAL_DEFAULT_SECS, onReceiverStarted, notify, SCALING_UP_RATIO_DEFAULT, SCALING_DOWN_RATIO_DEFAULT, wait, $asInstanceOf, onStreamingStarted, equals, asInstanceOf, ExecutorAllocationManager, initializeLogIfNecessary, isDynamicAllocationEnabled, synchronized, MAX_EXECUTORS_KEY, createIfEnabled, $isInstanceOf, logTrace, SCALING_UP_RATIO_KEY, isTraceEnabled, initializeLogIfNecessary$default$2, stop, onReceiverError, logName, notifyAll, onBatchCompleted, isInstanceOf, onBatchStarted, <init>, ==, clone, onBatchSubmitted, $init$, MIN_EXECUTORS_KEY, SCALING_INTERVAL_KEY, toString, logError, !=, SCALING_DOWN_RATIO_KEY, ENABLED_KEY, getClass, logWarning, start, onOutputOperationCompleted, ne, onReceiverStopped, eq, log, onOutputOperationStarted, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(asInstanceOf, ExecutorAllocationManager, synchronized, createIfEnabled, stop, isInstanceOf, <init>, ==, clone, toString, logError, !=, start, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(ExecutorAllocationManager, isDynamicAllocationEnabled, synchronized, stop, <init>, ==, clone, toString, logError, !=, logWarning, start, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ApiStreamingApp, wait, $asInstanceOf, equals, withUI, httpRequest, asInstanceOf, synchronized, $isInstanceOf, BaseStreamingAppResource, notifyAll, isInstanceOf, <init>, attemptId, withListener, ==, clone, uiRoot, $init$, servletContext, toString, !=, getClass, appId, ne, eq, getStreamingRoot, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala: Set(asInstanceOf, BaseStreamingAppResource, isInstanceOf, <init>, withListener, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala: Set(ApiStreamingApp, withUI, asInstanceOf, synchronized, BaseStreamingAppResource, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	scalaIntToJavaLong, kManifest, fromReceiverInputDStream, vManifest, kClassTag, vClassTag.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, calculateNumBatchesToRemember, getOrCompute, countByValueAndWindow$default$3, window, name, count, saveAsObjectFiles$default$2, wait, setGraph, <init>$default$5, $asInstanceOf, mapPartitions, slideDuration, currentCheckpointFiles, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, restore, asInstanceOf, print, context, initializeLogIfNecessary, glom, data, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, <init>$default$4, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, FileInputDStream, logTrace, defaultFilter, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, stop, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, <init>$default$3, persist, checkpointData, <init>, id, rateController, zeroTime, lastValidTime, cleanup, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, FileInputDStreamCheckpointData, $init$, checkpoint, countByValue, toString, graph, logError, !=, batchTimeToSelectedFiles, getClass, isInitialized, logWarning, update, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, start, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, FileInputDStream, defaultFilter, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, copy$default$2, $asInstanceOf, jobData, generateInputMetadataTable, productArity, equals, prefix, asInstanceOf, synchronized, $isInstanceOf, generateInputMetadataRow, canEqual, sparkJobId, productPrefix, notifyAll, isInstanceOf, <init>, ==, SparkJobIdWithUIData, clone, $init$, renderJson, copy, toString, !=, getClass, BatchPage, copy$default$1, render, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala: Set(<init>, BatchPage)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, name, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, stop, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, id, rateController, zeroTime, lastValidTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, InputDStream, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, start, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, InputDStream, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, InputDStream, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, InputDStream, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, InputDStream, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala: Set(asInstanceOf, <init>, InputDStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala: Set(slideDuration, map, rememberDuration, remember, asInstanceOf, context, synchronized, ssc, isInstanceOf, filter, checkpointData, <init>, id, flatMap, ==, clearMetadata, toString, logError, getClass, InputDStream, logWarning, ne, generatedRDDs, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala: Set(asInstanceOf, <init>, InputDStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala: Set(context, synchronized, ssc, <init>, !=, InputDStream, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala: Set(asInstanceOf, <init>, InputDStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala: Set(creationSite, name, count, setGraph, clearCheckpointData, map, updateCheckpointData, rememberDuration, remember, asInstanceOf, synchronized, stop, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, <init>, id, zeroTime, flatMap, ==, validateAtStart, clearMetadata, !=, InputDStream, generateJob, start, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(map, asInstanceOf, ssc, filter, <init>, id, rateController, flatMap, graph, logError, !=, InputDStream, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(asInstanceOf, synchronized, stop, ssc, isInstanceOf, <init>, id, rateController, ==, clone, toString, graph, logError, !=, InputDStream, start, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(name, map, asInstanceOf, context, synchronized, stop, ssc, isInstanceOf, filter, <init>, id, flatMap, ==, toString, graph, logError, !=, logWarning, start, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala: Set(asInstanceOf, <init>, InputDStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala: Set(<init>, !=, InputDStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala: Set(union, map, remember, asInstanceOf, stop, ssc, filter, <init>, checkpoint, InputDStream, start, transform, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, InputDStream, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, InputDStream, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala: Set(asInstanceOf, <init>, InputDStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala: Set(synchronized, <init>, ==, !=, logWarning, start, storageLevel, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala: Set(creationSite, name, count, setGraph, clearCheckpointData, map, updateCheckpointData, rememberDuration, remember, asInstanceOf, synchronized, stop, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, <init>, id, zeroTime, flatMap, ==, validateAtStart, clearMetadata, !=, InputDStream, generateJob, start, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(name, map, asInstanceOf, context, synchronized, stop, ssc, isInstanceOf, filter, <init>, id, flatMap, ==, toString, graph, logError, !=, logWarning, start, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala: Set(asInstanceOf, <init>, InputDStream)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala: Set(asInstanceOf, <init>, ==, !=, start, storageLevel, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala: Set(union, map, remember, asInstanceOf, stop, ssc, filter, <init>, checkpoint, InputDStream, start, transform, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, InputDStream, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextSender.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextSender.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextSender.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextSender.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextSender.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, main, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, RawTextSender, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RateLimitedOutputStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RateLimitedOutputStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RateLimitedOutputStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RateLimitedOutputStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RateLimitedOutputStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, RateLimitedOutputStream, $init$, flush, toString, logError, !=, getClass, logWarning, close, ne, eq, write, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextSender.scala: Set(<init>, ==, RateLimitedOutputStream, logError, !=, close, write, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, WriteAheadLogBackedBlockRDD, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, isValid, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, getBlockIdLocations, mapPartitions$default$2, min, getCheckpointFile, walRecordHandle, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, assertValid, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, WriteAheadLogBackedBlockRDDPartition, groupBy, treeReduce$default$2, ==, isBlockIdValid, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, _locations, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, blockIds, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, walRecordHandles, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, blockId, countByValueApprox, removeBlocks, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(WriteAheadLogBackedBlockRDD, map, asInstanceOf, conf, filter, <init>, id, flatMap, isBlockIdValid, sparkContext, blockIds, logError, !=, walRecordHandles, logWarning, blockId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, processingDelay, submissionTime, wait, batchTime, copy$default$2, $asInstanceOf, BatchInfo, copy$default$5, productArity, equals, schedulingDelay, asInstanceOf, synchronized, $isInstanceOf, numRecords, canEqual, copy$default$4, productPrefix, processingEndTime, notifyAll, isInstanceOf, <init>, streamIdToInputInfo, totalDelay, ==, clone, $init$, copy$default$3, copy, toString, !=, getClass, copy$default$1, outputOperationInfos, copy$default$6, ne, eq, productIterator, processingStartTime, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala: Set(processingDelay, submissionTime, batchTime, BatchInfo, schedulingDelay, numRecords, processingEndTime, <init>, streamIdToInputInfo, totalDelay, outputOperationInfos, processingStartTime)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala: Set(processingDelay, BatchInfo, schedulingDelay, numRecords, processingEndTime, <init>, streamIdToInputInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala: Set(submissionTime, BatchInfo, asInstanceOf, processingEndTime, isInstanceOf, <init>, streamIdToInputInfo, ==, toString, ne, eq, processingStartTime)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala: Set(processingDelay, BatchInfo, asInstanceOf, isInstanceOf, <init>, totalDelay, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(processingDelay, BatchInfo, asInstanceOf, synchronized, isInstanceOf, <init>, totalDelay, ==, clone, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala: Set(processingDelay, BatchInfo, synchronized, <init>, ==, !=, outputOperationInfos)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala: Set(batchTime, BatchInfo, synchronized, numRecords, <init>, streamIdToInputInfo, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala: Set(submissionTime, batchTime, BatchInfo, asInstanceOf, numRecords, processingEndTime, isInstanceOf, <init>, streamIdToInputInfo, ==, toString, !=, outputOperationInfos, eq, processingStartTime)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, FilteredDStream, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(reduceByWindow, creationSite, register, getOrCompute, window, setGraph, slideDuration, union, clearCheckpointData, map, mustCheckpoint, updateCheckpointData, rememberDuration, remember, baseScope, asInstanceOf, print, context, synchronized, compute, isTimeValid, createRDDWithLocalProperties, restoreCheckpointData, ssc, FilteredDStream, initialize, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, ==, validateAtStart, clearMetadata, slice, checkpointDuration, reduce, checkpoint, graph, !=, getClass, isInitialized, logWarning, parentRememberDuration, transformWith, repartition, transform, dependencies, storageLevel, generatedRDDs, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, metricRegistry, synchronized, $isInstanceOf, notifyAll, sourceName, isInstanceOf, <init>, ==, StreamingSource, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(synchronized, <init>, ==, StreamingSource, clone, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, PairDStreamFunctions, $asInstanceOf, fullOuterJoin, join, combineByKey$default$5, reduceByKeyAndWindow$default$5, equals, updateStateByKey, reduceByKeyAndWindow, asInstanceOf, saveAsHadoopFiles$default$6, synchronized, $isInstanceOf, updateStateByKey$default$4, reduceByKey, groupByKeyAndWindow, reduceByKeyAndWindow$default$6, ssc, notifyAll, groupByKey, isInstanceOf, <init>, combineByKey, rightOuterJoin, saveAsNewAPIHadoopFiles$default$6, ==, clone, sparkContext, flatMapValues, saveAsNewAPIHadoopFiles, saveAsHadoopFiles, toString, defaultPartitioner, !=, getClass, leftOuterJoin, mapValues, ne, reduceByKeyAndWindow$default$4, eq, ##, finalize, defaultPartitioner$default$1, hashCode, cogroup, mapWithState.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(PairDStreamFunctions, reduceByKeyAndWindow, asInstanceOf, synchronized, reduceByKey, ssc, isInstanceOf, <init>, ==, sparkContext, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala: Set(PairDStreamFunctions, fullOuterJoin, join, updateStateByKey, reduceByKeyAndWindow, asInstanceOf, reduceByKey, groupByKeyAndWindow, groupByKey, isInstanceOf, <init>, combineByKey, rightOuterJoin, ==, sparkContext, flatMapValues, saveAsNewAPIHadoopFiles, saveAsHadoopFiles, leftOuterJoin, mapValues, ne, cogroup, mapWithState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala: Set(PairDStreamFunctions, asInstanceOf, reduceByKey, ssc, <init>, !=, mapValues)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, name, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, ConstantInputDStream, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, stop, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, id, rateController, zeroTime, lastValidTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, start, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, getFirstFailureReason, equals, CompletedBatchTable, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ActiveBatchTable, ==, columns, toNodeSeq, clone, renderRows, baseRow, toString, !=, BatchTableBase, getClass, ne, getFirstFailureTableCell, eq, createOutputOperationProgressBar, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala: Set(CompletedBatchTable, synchronized, <init>, ActiveBatchTable, toNodeSeq, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toPairDStreamFunctions.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	setCallSite, notify, wait, $asInstanceOf, equals, outputOpId, asInstanceOf, run, result, synchronized, $isInstanceOf, Job, notifyAll, isInstanceOf, setOutputOpId, <init>, id, toOutputOperationInfo, setStartTime, ==, clone, setEndTime, toString, !=, callSite, time, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(setCallSite, asInstanceOf, synchronized, Job, isInstanceOf, <init>, id, ==, !=, time, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala: Set(outputOpId, asInstanceOf, Job, isInstanceOf, setOutputOpId, <init>, toOutputOperationInfo, ==, toString, time, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala: Set(asInstanceOf, Job, isInstanceOf, <init>, ==, time)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala: Set(setCallSite, asInstanceOf, synchronized, Job, isInstanceOf, <init>, id, ==, !=, time)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(outputOpId, asInstanceOf, run, result, synchronized, Job, isInstanceOf, <init>, id, toOutputOperationInfo, setStartTime, ==, clone, setEndTime, toString, !=, time, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala: Set(asInstanceOf, synchronized, Job, isInstanceOf, <init>, ==, toString, !=, time, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, processingDelay, handleJobStart, wait, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, JobSet, canEqual, productPrefix, handleJobCompletion, toBatchInfo, notifyAll, isInstanceOf, hasStarted, <init>, streamIdToInputInfo, totalDelay, ==, hasCompleted, clone, $init$, jobs, copy$default$3, copy, toString, !=, time, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(processingDelay, handleJobStart, asInstanceOf, synchronized, JobSet, handleJobCompletion, toBatchInfo, isInstanceOf, hasStarted, <init>, totalDelay, ==, hasCompleted, clone, jobs, toString, !=, time, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala: Set(asInstanceOf, synchronized, JobSet, isInstanceOf, <init>, ==, jobs, toString, !=, time, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverMessage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverMessage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverMessage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverMessage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverMessage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ReceiverMessage, CleanupOldBlocks, wait, $asInstanceOf, UpdateRateLimit, productArity, equals, threshTime, asInstanceOf, synchronized, $isInstanceOf, canEqual, StopReceiver, productPrefix, notifyAll, isInstanceOf, <init>, ==, clone, elementsPerSecond, $init$, copy, toString, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(CleanupOldBlocks, UpdateRateLimit, asInstanceOf, synchronized, StopReceiver, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala: Set(CleanupOldBlocks, UpdateRateLimit, threshTime, asInstanceOf, StopReceiver, isInstanceOf, <init>, ==, elementsPerSecond, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	appSparkVersion, notify, parent, name, wait, $asInstanceOf, StreamingTab, listener, equals, appName, prefix, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, pages, attachPage, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, basePath, logName, ssc, notifyAll, detach, isInstanceOf, <init>, attach, ==, clone, $init$, toString, logError, !=, getClass, logWarning, headerTabs, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala: Set(parent, name, StreamingTab, listener, asInstanceOf, synchronized, basePath, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala: Set(parent, name, StreamingTab, listener, synchronized, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, StreamingTab, appName, prefix, synchronized, ssc, detach, <init>, attach, ==, clone, toString, logError, !=, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, TransformedDStream, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(reduceByWindow, creationSite, register, getOrCompute, window, setGraph, slideDuration, union, clearCheckpointData, map, mustCheckpoint, updateCheckpointData, rememberDuration, remember, baseScope, asInstanceOf, print, context, synchronized, compute, isTimeValid, createRDDWithLocalProperties, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, ==, validateAtStart, clearMetadata, slice, checkpointDuration, reduce, checkpoint, TransformedDStream, graph, !=, getClass, isInitialized, logWarning, parentRememberDuration, transformWith, repartition, transform, dependencies, storageLevel, generatedRDDs, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(map, remember, synchronized, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, TransformedDStream, toString, graph, logError, !=, logWarning, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/UIUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/UIUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/UIUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/UIUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/UIUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, formatBatchTime, failureReasonCell$default$2, wait, $asInstanceOf, UIUtils, equals, asInstanceOf, shortTimeUnitString, failureReasonCell$default$3, synchronized, convertToTimeUnit, $isInstanceOf, failureReasonCell, notifyAll, isInstanceOf, ==, clone, toString, createOutputOperationFailureForUI, !=, getClass, normalizeDuration, formatBatchTime$default$4, formatBatchTime$default$3, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(formatBatchTime, UIUtils, asInstanceOf, synchronized, isInstanceOf, ==, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala: Set(formatBatchTime, UIUtils, shortTimeUnitString, synchronized, convertToTimeUnit, normalizeDuration, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala: Set(formatBatchTime, UIUtils, asInstanceOf, synchronized, failureReasonCell, isInstanceOf, ==, toString, createOutputOperationFailureForUI, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(formatBatchTime, UIUtils, asInstanceOf, synchronized, isInstanceOf, ==, clone, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala: Set(formatBatchTime, UIUtils, failureReasonCell, toString, createOutputOperationFailureForUI)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, QueueInputDStream, countByValueAndWindow$default$3, window, name, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, queue, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, stop, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, id, rateController, zeroTime, lastValidTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, start, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(QueueInputDStream, name, map, remember, queue, synchronized, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, windowDuration, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, WindowedDStream, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(reduceByWindow, creationSite, register, getOrCompute, window, windowDuration, setGraph, slideDuration, union, clearCheckpointData, map, mustCheckpoint, updateCheckpointData, rememberDuration, remember, baseScope, asInstanceOf, print, context, synchronized, compute, isTimeValid, createRDDWithLocalProperties, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, WindowedDStream, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, ==, validateAtStart, clearMetadata, slice, checkpointDuration, reduce, checkpoint, graph, !=, getClass, isInitialized, logWarning, parentRememberDuration, transformWith, repartition, transform, dependencies, storageLevel, generatedRDDs, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, copy$default$2, $asInstanceOf, path, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, FileBasedWriteAheadLogSegment, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, offset, ==, clone, $init$, copy$default$3, copy, toString, length, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala: Set(path, synchronized, FileBasedWriteAheadLogSegment, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala: Set(path, asInstanceOf, FileBasedWriteAheadLogSegment, isInstanceOf, <init>, offset, ==, toString, length, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala: Set(path, synchronized, FileBasedWriteAheadLogSegment, <init>, offset, ==, length)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala: Set(path, asInstanceOf, synchronized, FileBasedWriteAheadLogSegment, isInstanceOf, <init>, ==, toString, !=, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, withUI, batchesList, httpRequest, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, oneReceiver, operationsList, <init>, attemptId, withListener, oneOperation, ==, clone, uiRoot, receiversList, $init$, servletContext, toString, streamingStatistics, !=, getClass, appId, ApiStreamingRootResource, ne, oneBatch, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingApp.scala: Set(withUI, asInstanceOf, synchronized, isInstanceOf, <init>, ==, ApiStreamingRootResource)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, fold, logTrace, nonEmpty, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, FileBasedWriteAheadLogReader, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, toString, copyToArray, length, seq, logError, !=, collect, getClass, logWarning, hasDefiniteSize, patch, close, foldLeft, contains, isEmpty, ne, withPartial, reversed, hasNext, indexOf, reduceLeft, eq, sum, log, ##, scanLeft, finalize, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala: Set(find, size, map, asInstanceOf, synchronized, mkString, isInstanceOf, filter, <init>, max, ++, grouped, FileBasedWriteAheadLogReader, flatMap, ==, foreach, exists, toString, logError, !=, getClass, logWarning, close, contains, isEmpty, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	inputStreamId, onReceiverStarted, JavaStreamInputInfo, notify, processingDelay, name, submissionTime, wait, batchTime, startTime, copy$default$2, streamId, $asInstanceOf, location, copy$default$5, failureReason, productArity, onStreamingStarted, equals, copy$default$9, description, JavaStreamingListenerReceiverStarted, JavaStreamingListenerEvent, JavaStreamingListenerOutputOperationStarted, lastErrorMessage, schedulingDelay, asInstanceOf, JavaReceiverInfo, outputOperationInfo, JavaOutputOperationInfo, JavaStreamingListenerOutputOperationCompleted, synchronized, $isInstanceOf, JavaStreamingListenerBatchSubmitted, copy$default$8, numRecords, receiverInfo, lastErrorTime, canEqual, copy$default$4, executorId, JavaStreamingListenerBatchCompleted, productPrefix, processingEndTime, onReceiverError, notifyAll, lastError, onBatchCompleted, isInstanceOf, onBatchStarted, JavaStreamingListener, endTime, <init>, streamIdToInputInfo, id, totalDelay, ==, clone, JavaStreamingListenerBatchStarted, metadataDescription, onBatchSubmitted, copy$default$7, copy$default$10, $init$, copy$default$3, copy, metadata, JavaStreamingListenerReceiverStopped, toString, JavaStreamingListenerReceiverError, !=, time, getClass, copy$default$1, batchInfo, outputOperationInfos, PythonStreamingListener, PythonStreamingListenerWrapper, copy$default$6, onOutputOperationCompleted, ne, JavaStreamingListenerStreamingStarted, onReceiverStopped, eq, productIterator, onOutputOperationStarted, processingStartTime, ##, finalize, JavaBatchInfo, productElement, hashCode, active.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala: Set(inputStreamId, onReceiverStarted, JavaStreamInputInfo, processingDelay, name, submissionTime, batchTime, startTime, streamId, location, failureReason, onStreamingStarted, description, JavaStreamingListenerReceiverStarted, JavaStreamingListenerOutputOperationStarted, lastErrorMessage, schedulingDelay, JavaReceiverInfo, outputOperationInfo, JavaOutputOperationInfo, JavaStreamingListenerOutputOperationCompleted, JavaStreamingListenerBatchSubmitted, numRecords, receiverInfo, lastErrorTime, executorId, JavaStreamingListenerBatchCompleted, processingEndTime, onReceiverError, lastError, onBatchCompleted, onBatchStarted, JavaStreamingListener, endTime, <init>, streamIdToInputInfo, id, totalDelay, JavaStreamingListenerBatchStarted, metadataDescription, onBatchSubmitted, metadata, JavaStreamingListenerReceiverStopped, JavaStreamingListenerReceiverError, time, batchInfo, outputOperationInfos, onOutputOperationCompleted, JavaStreamingListenerStreamingStarted, onReceiverStopped, onOutputOperationStarted, processingStartTime, JavaBatchInfo, active)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListenerBus.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	StreamingListenerBus, onExecutorUnblacklisted, notify, post, getTimer, postToAll, wait, $asInstanceOf, onJobEnd, onApplicationEnd, listeners, equals, onTaskEnd, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, onOtherEvent, isTraceEnabled, initializeLogIfNecessary$default$2, onTaskStart, stop, logName, notifyAll, onExecutorRemoved, onExecutorAdded, isInstanceOf, removeListener, <init>, onBlockUpdated, ==, onBlockManagerRemoved, clone, onExecutorMetricsUpdate, onEnvironmentUpdate, onBlockManagerAdded, $init$, onNodeBlacklisted, findListenersByClass, onApplicationStart, onSpeculativeTaskSubmitted, toString, logError, !=, getClass, logWarning, start, onExecutorBlacklisted, doPostEvent, onUnpersistRDD, addListener, ne, onStageSubmitted, eq, onNodeUnblacklisted, log, ##, finalize, hashCode, logDebug, logInfo, removeListenerOnError.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(StreamingListenerBus, post, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, clone, toString, logError, !=, start, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(StreamingListenerBus, post, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, start, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(StreamingListenerBus, post, synchronized, stop, <init>, ==, clone, toString, logError, !=, logWarning, start, addListener, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, UnionDStream, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(reduceByWindow, creationSite, register, getOrCompute, window, setGraph, slideDuration, union, clearCheckpointData, UnionDStream, map, mustCheckpoint, updateCheckpointData, rememberDuration, remember, baseScope, asInstanceOf, print, context, synchronized, compute, isTimeValid, createRDDWithLocalProperties, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, ==, validateAtStart, clearMetadata, slice, checkpointDuration, reduce, checkpoint, graph, !=, getClass, isInitialized, logWarning, parentRememberDuration, transformWith, repartition, transform, dependencies, storageLevel, generatedRDDs, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(UnionDStream, map, remember, synchronized, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, logWarning, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, MillisecondsStatUIData, StreamingPage, BLACK_RIGHT_TRIANGLE_HTML, wait, $asInstanceOf, JsCollector, equals, formatDurationOption, RecordRateUIData, emptyCell, prefix, asInstanceOf, initializeLogIfNecessary, data, addStatement, synchronized, avg, $isInstanceOf, generateHistogramHtml, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, generateDataJs, logName, nextVariableName, notifyAll, isInstanceOf, addPreparedStatement, BLACK_DOWN_TRIANGLE_HTML, <init>, histogramData, max, ==, formattedAvg, clone, $init$, renderJson, timelineData, GraphUIData, toString, logError, !=, getClass, logWarning, render, toHtml, ne, generateTimelineHtml, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala: Set(StreamingPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, MapValuedDStream, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala: Set(window, slideDuration, map, asInstanceOf, context, MapValuedDStream, ssc, <init>, flatMap, foreachRDD, !=, transformWith)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, GlommedDStream, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(reduceByWindow, creationSite, register, getOrCompute, window, setGraph, slideDuration, union, clearCheckpointData, map, mustCheckpoint, updateCheckpointData, rememberDuration, remember, baseScope, asInstanceOf, print, context, synchronized, compute, isTimeValid, createRDDWithLocalProperties, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, GlommedDStream, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, ==, validateAtStart, clearMetadata, slice, checkpointDuration, reduce, checkpoint, graph, !=, getClass, isInitialized, logWarning, parentRememberDuration, transformWith, repartition, transform, dependencies, storageLevel, generatedRDDs, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, MapWithStateDStream, InternalMapWithStateDStream, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, stateSnapshots, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, keyClass, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, valueClass, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, mappedClass, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, stateClass, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, MapWithStateDStreamImpl, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala: Set(asInstanceOf, MapWithStateDStream, stateSnapshots, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala: Set(window, slideDuration, union, asInstanceOf, context, MapWithStateDStream, compute, cache, isInstanceOf, filter, persist, <init>, keyClass, ==, valueClass, repartition, ne, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala: Set(window, slideDuration, map, asInstanceOf, context, MapWithStateDStream, ssc, <init>, flatMap, foreachRDD, keyClass, valueClass, !=, transformWith, MapWithStateDStreamImpl)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onReceiverStarted, notify, wait, $asInstanceOf, onStreamingStarted, equals, asInstanceOf, synchronized, JavaStreamingListenerWrapper, $isInstanceOf, onReceiverError, notifyAll, onBatchCompleted, isInstanceOf, onBatchStarted, <init>, ==, clone, onBatchSubmitted, $init$, toString, !=, getClass, onOutputOperationCompleted, ne, onReceiverStopped, eq, onOutputOperationStarted, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, partitioner, wait, StateSpec, $asInstanceOf, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, canEqual, productPrefix, getTimeoutInterval, getInitialStateRDD, notifyAll, isInstanceOf, initialState, <init>, ==, clone, function, $init$, getPartitioner, StateSpecImpl, copy, toString, !=, getFunction, getClass, copy$default$1, ne, timeout, numPartitions, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala: Set(partitioner, StateSpec, asInstanceOf, isInstanceOf, <init>, ==, function, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala: Set(partitioner, asInstanceOf, getTimeoutInterval, getInitialStateRDD, isInstanceOf, <init>, ==, getPartitioner, StateSpecImpl, !=, getFunction, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala: Set(partitioner, StateSpec, asInstanceOf, <init>, StateSpecImpl, !=, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	batchDuration, notify, wait, startTime, $asInstanceOf, DStreamGraph, clearCheckpointData, updateCheckpointData, equals, rememberDuration, remember, getReceiverInputStreams, asInstanceOf, initializeLogIfNecessary, validate, getInputStreamNameAndID, synchronized, checkpointInProgress, $isInstanceOf, addOutputStream, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, restoreCheckpointData, logName, notifyAll, getMaxInputStreamRememberDuration, setBatchDuration, isInstanceOf, <init>, zeroTime, getInputStreams, getNumReceivers, addInputStream, getOutputStreams, ==, clearMetadata, clone, $init$, generateJobs, toString, logError, !=, getClass, logWarning, restart, start, ne, eq, log, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(batchDuration, DStreamGraph, remember, validate, synchronized, stop, restoreCheckpointData, setBatchDuration, <init>, ==, clone, toString, logError, !=, logWarning, start, ne, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(batchDuration, DStreamGraph, clearCheckpointData, updateCheckpointData, rememberDuration, remember, asInstanceOf, synchronized, checkpointInProgress, addOutputStream, restoreCheckpointData, isInstanceOf, <init>, zeroTime, ==, clearMetadata, !=, getClass, logWarning, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala: Set(batchDuration, DStreamGraph, <init>, addInputStream, ==, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(batchDuration, startTime, DStreamGraph, asInstanceOf, <init>, logError, !=, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(batchDuration, startTime, DStreamGraph, asInstanceOf, synchronized, stop, isInstanceOf, <init>, getInputStreams, ==, clone, toString, logError, !=, start, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala: Set(batchDuration, DStreamGraph, getInputStreamNameAndID, synchronized, <init>, getNumReceivers, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(DStreamGraph, getReceiverInputStreams, asInstanceOf, synchronized, stop, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, start, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala: Set(batchDuration, startTime, DStreamGraph, clearCheckpointData, updateCheckpointData, asInstanceOf, synchronized, stop, getMaxInputStreamRememberDuration, isInstanceOf, <init>, zeroTime, ==, clearMetadata, generateJobs, toString, !=, logWarning, restart, start, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala: Set(startTime, DStreamGraph, asInstanceOf, validate, synchronized, stop, <init>, ==, toString, logError, !=, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala: Set(DStreamGraph, asInstanceOf, synchronized, checkpointInProgress, isInstanceOf, <init>, ==, !=, getClass, logWarning, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	withNamedScope, notify, getActive, getActiveOrCreate, textFileStream, wait, <init>$default$5, getActiveOrCreate$default$4, $asInstanceOf, <init>$default$6, union, equals, remember, jarOfClass, progressListener, asInstanceOf, getOrCreate$default$4, initializeLogIfNecessary, rawSocketStream$default$3, isCheckpointingEnabled, queueStream$default$2, getStartSite, synchronized, sc, $isInstanceOf, <init>$default$4, queueStream, addStreamingListener, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, binaryRecordsStream, stop$default$1, socketStream, stop, logName, notifyAll, getOrCreate$default$3, conf, fileStream, isInstanceOf, savedProperties, getState, <init>, rddToFileName, waiter, scheduler, rawSocketStream, ==, initialCheckpoint, getOrCreate, clone, checkpointDuration, sparkContext, $init$, checkpoint, socketTextStream$default$3, socketTextStream, toString, awaitTermination, graph, logError, !=, awaitTerminationOrTimeout, createNewSparkContext, getClass, logWarning, checkpointDir, getActiveOrCreate$default$3, start, receiverStream, uiTab, getNewInputStreamId, ne, transform, eq, withScope, log, env, ##, finalize, hashCode, logDebug, StreamingContext, logInfo, isCheckpointPresent.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala: Set(<init>, checkpoint, transform, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala: Set(union, asInstanceOf, conf, isInstanceOf, <init>, ==, sparkContext, ne, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala: Set(<init>, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(union, remember, asInstanceOf, synchronized, sc, conf, isInstanceOf, getState, <init>, rddToFileName, scheduler, ==, checkpointDuration, sparkContext, checkpoint, graph, !=, getClass, logWarning, checkpointDir, transform, withScope, logDebug, StreamingContext, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala: Set(<init>, ==, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala: Set(remember, asInstanceOf, synchronized, conf, isInstanceOf, <init>, scheduler, ==, sparkContext, toString, logError, getClass, logWarning, ne, logDebug, StreamingContext, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala: Set(<init>, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala: Set(synchronized, sc, <init>, sparkContext, !=, logWarning, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala: Set(<init>, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala: Set(asInstanceOf, sc, <init>, checkpoint, !=, logDebug, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala: Set(<init>, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala: Set(<init>, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala: Set(sc, <init>, scheduler, ==, graph, !=, logWarning, getNewInputStreamId, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala: Set(<init>, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala: Set(asInstanceOf, sc, conf, <init>, rddToFileName, sparkContext, !=, withScope, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala: Set(synchronized, <init>, ==, !=, logWarning, start, StreamingContext, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala: Set(asInstanceOf, isInstanceOf, <init>, scheduler, ==, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala: Set(<init>, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala: Set(remember, asInstanceOf, synchronized, stop, isInstanceOf, <init>, scheduler, ==, !=, start, logDebug, StreamingContext, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala: Set(union, asInstanceOf, sc, isInstanceOf, <init>, ==, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, scheduler, ==, toString, eq, StreamingContext, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala: Set(asInstanceOf, sc, isInstanceOf, <init>, ==, checkpointDuration, sparkContext, checkpoint, !=, ne, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(asInstanceOf, sc, conf, <init>, scheduler, sparkContext, graph, logError, !=, logWarning, env, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(asInstanceOf, synchronized, sc, addStreamingListener, stop, conf, isInstanceOf, savedProperties, <init>, waiter, scheduler, ==, clone, sparkContext, toString, awaitTermination, graph, logError, !=, start, eq, logDebug, StreamingContext, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala: Set(synchronized, conf, <init>, scheduler, ==, graph, !=, ne, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala: Set(union, sc, <init>, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(asInstanceOf, getStartSite, synchronized, sc, stop, conf, isInstanceOf, <init>, scheduler, ==, sparkContext, toString, awaitTermination, graph, logError, !=, logWarning, checkpointDir, start, ne, eq, env, StreamingContext, logInfo, isCheckpointPresent)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala: Set(asInstanceOf, synchronized, sc, stop, conf, isInstanceOf, <init>, scheduler, ==, initialCheckpoint, checkpointDuration, sparkContext, toString, graph, !=, logWarning, checkpointDir, start, eq, logDebug, StreamingContext, logInfo, isCheckpointPresent)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingTab.scala: Set(progressListener, sc, addStreamingListener, <init>, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala: Set(asInstanceOf, synchronized, sc, stop, conf, <init>, scheduler, ==, checkpointDuration, checkpoint, toString, awaitTermination, graph, logError, !=, logWarning, checkpointDir, ne, StreamingContext, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, sparkContext, graph, !=, getClass, logWarning, ne, logDebug, StreamingContext, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala: Set(<init>, !=, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala: Set(asInstanceOf, <init>, ==, !=, start, StreamingContext, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala: Set(textFileStream, union, remember, jarOfClass, asInstanceOf, sc, queueStream, addStreamingListener, binaryRecordsStream, socketStream, stop, conf, fileStream, getState, <init>, scheduler, rawSocketStream, getOrCreate, sparkContext, checkpoint, socketTextStream, awaitTermination, awaitTerminationOrTimeout, start, receiverStream, transform, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala: Set(<init>, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala: Set(getActive, union, asInstanceOf, synchronized, sc, stop, isInstanceOf, <init>, ==, sparkContext, logError, !=, getClass, start, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala: Set(progressListener, <init>, sparkContext, StreamingContext)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, MapPartitionedDStream, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(reduceByWindow, creationSite, register, getOrCompute, window, setGraph, slideDuration, union, clearCheckpointData, map, mustCheckpoint, updateCheckpointData, rememberDuration, remember, MapPartitionedDStream, baseScope, asInstanceOf, print, context, synchronized, compute, isTimeValid, createRDDWithLocalProperties, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, ==, validateAtStart, clearMetadata, slice, checkpointDuration, reduce, checkpoint, graph, !=, getClass, isInitialized, logWarning, parentRememberDuration, transformWith, repartition, transform, dependencies, storageLevel, generatedRDDs, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, DoCheckpoint, wait, onCheckpointCompletion, copy$default$2, $asInstanceOf, ClearMetadata, productArity, equals, asInstanceOf, initializeLogIfNecessary, synchronized, ClearCheckpointData, $isInstanceOf, JobGenerator, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, GenerateJobs, stop, logName, notifyAll, isInstanceOf, <init>, clearCheckpointDataLater, clock, JobGeneratorEvent, ==, clone, $init$, onBatchCompletion, copy, toString, logError, !=, time, getClass, logWarning, copy$default$1, start, ne, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(asInstanceOf, synchronized, JobGenerator, stop, isInstanceOf, <init>, clock, ==, clone, onBatchCompletion, toString, logError, !=, time, start, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala: Set(onCheckpointCompletion, asInstanceOf, synchronized, JobGenerator, stop, <init>, clearCheckpointDataLater, ==, toString, logError, !=, logWarning, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, streamIdToAllocatedBlocks, writeToLog, hasUnallocatedReceivedBlocks, copy$default$2, $asInstanceOf, addBlock, productArity, BlockAdditionEvent, equals, asInstanceOf, initializeLogIfNecessary, BatchCleanupEvent, synchronized, $isInstanceOf, logTrace, canEqual, isTraceEnabled, ReceivedBlockTrackerLogEvent, initializeLogIfNecessary$default$2, productPrefix, isWriteAheadLogEnabled, stop, logName, notifyAll, AllocatedBlocks, isInstanceOf, getBlocksOfBatchAndStream, getBlocksOfBatch, allocatedBlocks, <init>, ==, clone, checkpointDirToLogDir, cleanupOldBatches, BatchAllocationEvent, times, $init$, receivedBlockInfo, copy, toString, allocateBlocksToBatch, logError, !=, time, getClass, logWarning, copy$default$1, getBlocksOfStream, ne, getUnallocatedBlocks, eq, productIterator, log, ##, finalize, ReceivedBlockTracker, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(hasUnallocatedReceivedBlocks, addBlock, asInstanceOf, synchronized, stop, isInstanceOf, getBlocksOfBatchAndStream, getBlocksOfBatch, <init>, ==, cleanupOldBatches, receivedBlockInfo, toString, allocateBlocksToBatch, logError, !=, logWarning, ne, eq, ReceivedBlockTracker, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, ForEachDStream, getOrCompute, countByValueAndWindow$default$3, window, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala: Set(reduceByWindow, creationSite, register, ForEachDStream, getOrCompute, window, setGraph, slideDuration, union, clearCheckpointData, map, mustCheckpoint, updateCheckpointData, rememberDuration, remember, baseScope, asInstanceOf, print, context, synchronized, compute, isTimeValid, createRDDWithLocalProperties, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, ==, validateAtStart, clearMetadata, slice, checkpointDuration, reduce, checkpoint, graph, !=, getClass, isInitialized, logWarning, parentRememberDuration, transformWith, repartition, transform, dependencies, storageLevel, generatedRDDs, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	inputStreamId, notify, unapply, wait, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, initializeLogIfNecessary, synchronized, getInfo, $isInstanceOf, apply$default$3, numRecords, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, isInstanceOf, <init>$default$3, <init>, apply, cleanup, ==, clone, metadataDescription, $init$, copy$default$3, copy, metadata, toString, logError, !=, METADATA_KEY_DESCRIPTION, InputInfoTracker, getClass, logWarning, copy$default$1, reportInfo, ne, eq, productIterator, log, ##, StreamInputInfo, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala: Set(inputStreamId, numRecords, <init>, metadataDescription, metadata, StreamInputInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala: Set(asInstanceOf, numRecords, isInstanceOf, <init>, ==, toString, eq, StreamInputInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, metadata, toString, logError, METADATA_KEY_DESCRIPTION, InputInfoTracker, getClass, logWarning, reportInfo, ne, StreamInputInfo, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/RateController.scala: Set(numRecords, <init>, apply, StreamInputInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne, eq, StreamInputInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala: Set(inputStreamId, asInstanceOf, synchronized, numRecords, isInstanceOf, <init>, apply, ==, metadataDescription, toString, ne, eq, StreamInputInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(asInstanceOf, numRecords, <init>, apply, logError, !=, InputInfoTracker, logWarning, reportInfo, StreamInputInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, logError, !=, InputInfoTracker, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala: Set(synchronized, numRecords, <init>, apply, ==, !=, ne, StreamInputInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala: Set(asInstanceOf, synchronized, getInfo, isInstanceOf, <init>, apply, cleanup, ==, toString, !=, InputInfoTracker, logWarning, eq, StreamInputInfo, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala: Set(asInstanceOf, numRecords, isInstanceOf, <init>, apply, ==, toString, !=, eq, StreamInputInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisor.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/Receiver.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, notifyAll, WriteAheadLogRecordHandle, <init>, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogWriter.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala: Set(WriteAheadLogRecordHandle, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala: Set(WriteAheadLogRecordHandle, <init>, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala: Set(WriteAheadLogRecordHandle, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala: Set(WriteAheadLogRecordHandle, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala: Set(WriteAheadLogRecordHandle, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala: Set(WriteAheadLogRecordHandle, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala: Set(WriteAheadLogRecordHandle, <init>, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(WriteAheadLogRecordHandle, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogSegment.scala: Set(WriteAheadLogRecordHandle, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLogRecordHandle.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala: Set(WriteAheadLogRecordHandle, <init>, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	isBatchingEnabled, DEFAULT_ROLLING_INTERVAL_SECS, RECEIVER_WAL_ENABLE_CONF_KEY, notify, RECEIVER_WAL_CLOSE_AFTER_WRITE_CONF_KEY, enableReceiverLog, DRIVER_WAL_CLASS_CONF_KEY, wait, $asInstanceOf, getBatchingTimeout, RECEIVER_WAL_MAX_FAILURES_CONF_KEY, DRIVER_WAL_CLOSE_AFTER_WRITE_CONF_KEY, equals, DRIVER_WAL_MAX_FAILURES_CONF_KEY, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, getRollingIntervalSecs, createLogForDriver, RECEIVER_WAL_CLASS_CONF_KEY, RECEIVER_WAL_ROLLING_INTERVAL_CONF_KEY, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, DEFAULT_MAX_FAILURES, isInstanceOf, DRIVER_WAL_BATCHING_CONF_KEY, getMaxFailures, ==, clone, DRIVER_WAL_ROLLING_INTERVAL_CONF_KEY, $init$, toString, logError, !=, getClass, logWarning, WriteAheadLogUtils, ne, shouldCloseFileAfterWrite, DRIVER_WAL_BATCHING_TIMEOUT_CONF_KEY, eq, log, ##, finalize, createLogForReceiver, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala: Set(getBatchingTimeout, asInstanceOf, synchronized, isInstanceOf, ==, toString, logWarning, WriteAheadLogUtils, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala: Set(asInstanceOf, synchronized, createLogForDriver, logTrace, isInstanceOf, ==, clone, toString, logError, logWarning, WriteAheadLogUtils, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala: Set(asInstanceOf, isInstanceOf, ==, toString, logError, !=, WriteAheadLogUtils, createLogForReceiver, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(enableReceiverLog, asInstanceOf, logError, !=, logWarning, WriteAheadLogUtils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala: Set(enableReceiverLog, asInstanceOf, isInstanceOf, ==, getClass, logWarning, WriteAheadLogUtils, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(isBatchingEnabled, enableReceiverLog, asInstanceOf, synchronized, isInstanceOf, ==, toString, logError, !=, logWarning, WriteAheadLogUtils, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, getClass, logWarning, WriteAheadLogUtils, eq, createLogForReceiver)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	deaggregate, notify, read, unapply, curried, wait, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, initializeLogIfNecessary, data, clean, synchronized, aggregate, $isInstanceOf, tupled, logTrace, canEqual, readAll, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, BatchedWriteAheadLog, logName, notifyAll, isInstanceOf, <init>, apply, ==, Record, clone, $init$, copy$default$3, copy, toString, logError, !=, time, getClass, logWarning, copy$default$1, wrappedLog, close, promise, ne, eq, productIterator, write, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala: Set(unapply, asInstanceOf, BatchedWriteAheadLog, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, processingDelay, unapply, duration, name, submissionTime, wait, batchTime, startTime, copy$default$2, $asInstanceOf, <init>$default$6, OutputOpIdAndSparkJobId, copy$default$5, failureReason, productArity, OutputOperationUIData, equals, description, schedulingDelay, outputOpId, asInstanceOf, apply$default$7, synchronized, numCompletedOutputOp, <init>$default$7, $isInstanceOf, numRecords, canEqual, copy$default$4, sparkJobId, productPrefix, processingEndTime, notifyAll, BatchUIData, outputOpIdSparkJobIdPairs, isInstanceOf, apply$default$6, endTime, <init>, streamIdToInputInfo, outputOperations, id, apply, totalDelay, ==, clone, updateOutputOperationInfo, copy$default$7, $init$, copy$default$3, copy, toString, numFailedOutputOp, !=, getClass, copy$default$1, copy$default$6, ne, isFailed, eq, productIterator, numActiveOutputOp, processingStartTime, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala: Set(processingDelay, name, batchTime, startTime, schedulingDelay, synchronized, numRecords, BatchUIData, <init>, apply, totalDelay, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala: Set(processingDelay, duration, name, batchTime, startTime, OutputOpIdAndSparkJobId, failureReason, OutputOperationUIData, description, schedulingDelay, outputOpId, asInstanceOf, numCompletedOutputOp, numRecords, sparkJobId, BatchUIData, outputOpIdSparkJobIdPairs, isInstanceOf, endTime, <init>, outputOperations, apply, totalDelay, ==, toString, numFailedOutputOp, ne, numActiveOutputOp)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala: Set(processingDelay, duration, name, submissionTime, batchTime, OutputOpIdAndSparkJobId, failureReason, OutputOperationUIData, description, schedulingDelay, outputOpId, asInstanceOf, synchronized, numRecords, sparkJobId, BatchUIData, outputOpIdSparkJobIdPairs, isInstanceOf, endTime, <init>, streamIdToInputInfo, outputOperations, id, apply, totalDelay, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala: Set(batchTime, OutputOpIdAndSparkJobId, outputOpId, synchronized, numRecords, BatchUIData, <init>, streamIdToInputInfo, apply, ==, updateOutputOperationInfo, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala: Set(processingDelay, batchTime, failureReason, OutputOperationUIData, schedulingDelay, numCompletedOutputOp, numRecords, BatchUIData, <init>, outputOperations, totalDelay, toString, numFailedOutputOp, isFailed, numActiveOutputOp)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingSource.scala: Set(processingDelay, name, submissionTime, schedulingDelay, processingEndTime, BatchUIData, <init>, apply, totalDelay, processingStartTime)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, onReceiverStarted, notify, register, getReceiver, getOrCompute, countByValueAndWindow$default$3, window, name, count, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, streamUID, ReceiverRateController, countByValueAndWindow, map, mustCheckpoint, onStreamingStarted, updateCheckpointData, equals, rememberDuration, getLatestRate, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, stop, onReceiverError, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, onBatchCompleted, isInstanceOf, filter, onBatchStarted, publish, persist, ReceiverInputDStream, checkpointData, <init>, id, rateController, zeroTime, lastValidTime, createBlockRDD, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, onBatchSubmitted, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, start, onOutputOperationCompleted, saveAsTextFiles, ne, transform, dependencies, onReceiverStopped, eq, storageLevel, log, generatedRDDs, onOutputOperationStarted, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, ReceiverInputDStream, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, ReceiverInputDStream, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, ReceiverInputDStream, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala: Set(ReceiverInputDStream, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala: Set(asInstanceOf, ReceiverInputDStream, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala: Set(synchronized, ReceiverInputDStream, <init>, ==, !=, logWarning, start, storageLevel, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala: Set(creationSite, name, count, setGraph, clearCheckpointData, map, updateCheckpointData, rememberDuration, remember, asInstanceOf, synchronized, stop, restoreCheckpointData, ssc, initialize, isInstanceOf, filter, ReceiverInputDStream, <init>, id, zeroTime, flatMap, ==, validateAtStart, clearMetadata, !=, generateJob, start, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(getReceiver, name, streamUID, map, asInstanceOf, context, synchronized, stop, ssc, isInstanceOf, filter, ReceiverInputDStream, <init>, id, flatMap, ==, toString, graph, logError, !=, logWarning, start, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala: Set(asInstanceOf, ReceiverInputDStream, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala: Set(asInstanceOf, ReceiverInputDStream, <init>, ==, !=, start, storageLevel, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala: Set(union, map, remember, asInstanceOf, stop, ssc, filter, ReceiverInputDStream, <init>, checkpoint, start, transform, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, stop, restoreCheckpointData, ssc, filter, ReceiverInputDStream, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, wait, equals, clean, readAll, notifyAll, <init>, WriteAheadLog, toString, getClass, close, write, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/BatchedWriteAheadLog.scala: Set(clean, readAll, <init>, WriteAheadLog, toString, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala: Set(clean, readAll, <init>, WriteAheadLog, toString, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/WriteAheadLogBackedBlockRDD.scala: Set(read, <init>, WriteAheadLog, toString, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/java/org/apache/spark/streaming/util/WriteAheadLog.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala: Set(read, <init>, WriteAheadLog, toString, getClass, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceivedBlockHandler.scala: Set(clean, <init>, WriteAheadLog, toString, getClass, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala: Set(<init>, WriteAheadLog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala: Set(<init>, WriteAheadLog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/WriteAheadLogUtils.scala: Set(<init>, WriteAheadLog)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unapply, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, createFromPairRDD, wait, copy$default$2, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, stateMap, MapWithStateRDDRecord, localCheckpoint, map, productArity, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, MapWithStateRDD, MapWithStateRDDPartition, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, mappedData, min, getCheckpointFile, partitionedDataRDDPartition, fold, getOutputDeterministicLevel, logTrace, canEqual, treeAggregate$default$4, isTraceEnabled, setFullScan, initializeLogIfNecessary$default$2, zipWithUniqueId, productPrefix, iterator, updateRecordWithData, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, apply, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, copy, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, copy$default$1, previousSessionRDDPartition, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, createFromRDD, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, productIterator, withScope, log, ##, finalize, treeAggregate, index, productElement, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala: Set(parent, getOrCompute, partitioner, createFromPairRDD, stateMap, MapWithStateRDDRecord, map, asInstanceOf, context, MapWithStateRDD, mappedData, isInstanceOf, persist, <init>, apply, flatMap, ==, sparkContext, checkpoint, !=, createFromRDD, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, notify, register, getOrCompute, countByValueAndWindow$default$3, window, count, windowDuration, saveAsObjectFiles$default$2, wait, setGraph, $asInstanceOf, mapPartitions, slideDuration, union, ReducedWindowedDStream, clearCheckpointData, countByValueAndWindow, map, mustCheckpoint, updateCheckpointData, equals, rememberDuration, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, saveAsTextFiles$default$2, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, isInstanceOf, filter, persist, checkpointData, <init>, zeroTime, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, reduce, $init$, checkpoint, countByValue, toString, graph, logError, !=, getClass, isInitialized, logWarning, countByWindow, parentRememberDuration, generateJob, transformWith, repartition, saveAsTextFiles, ne, transform, dependencies, eq, storageLevel, log, generatedRDDs, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala: Set(window, windowDuration, slideDuration, ReducedWindowedDStream, map, asInstanceOf, context, ssc, <init>, flatMap, foreachRDD, !=, transformWith)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	classTag, scalaIntToJavaLong.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Time.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Interval.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StateSpec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ConstantInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStreamCheckpointData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FilteredDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMapValuedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FlatMappedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ForEachDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/GlommedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapPartitionedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapValuedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MappedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/QueueInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReducedWindowedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ShuffledDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/StateDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/TransformedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/UnionDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/WindowedDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverMessage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/BatchInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/InputInfoTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/Job.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobSet.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/OutputOperationInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/AllBatchesTable.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/BatchUIData.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/StateMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/StateMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/StateMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/StateMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/StateMap.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	StateInfo, notify, read, unapply, wait, copy$default$2, $asInstanceOf, empty, getAll, <init>$default$1, productArity, equals, DEFAULT_INITIAL_CAPACITY, deltaChainLength, asInstanceOf, LimitMarker, data, synchronized, $isInstanceOf, create, apply$default$3, canEqual, EmptyStateMap, productPrefix, parentStateMap, notifyAll, num, isInstanceOf, <init>$default$3, StateMap, <init>, OpenHashMapBasedStateMap, apply$default$2, approxSize, remove, toDebugString, apply, ==, clone, shouldCompact, $init$, DELTA_CHAIN_LENGTH_THRESHOLD, copy$default$3, copy, put, apply$default$1, getByTime, toString, !=, get, getClass, copy$default$1, update, ne, updateTime, <init>$default$2, eq, productIterator, write, ##, finalize, deleted, productElement, hashCode, markDeleted.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala: Set(empty, asInstanceOf, create, EmptyStateMap, isInstanceOf, StateMap, <init>, remove, apply, ==, copy, put, getByTime, toString, get, ne, updateTime, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/MapWithStateDStream.scala: Set(getAll, asInstanceOf, isInstanceOf, StateMap, <init>, apply, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, copy$default$2, streamId, $asInstanceOf, blockStoreResult, productArity, equals, setBlockIdInvalid, asInstanceOf, synchronized, $isInstanceOf, numRecords, metadataOption, canEqual, copy$default$4, productPrefix, notifyAll, isInstanceOf, <init>, walRecordHandleOption, ==, isBlockIdValid, clone, $init$, copy$default$3, copy, toString, !=, getClass, copy$default$1, ne, blockId, ReceivedBlockInfo, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceivedBlockTracker.scala: Set(streamId, blockStoreResult, setBlockIdInvalid, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, ne, blockId, ReceivedBlockInfo, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(streamId, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, ne, ReceivedBlockInfo, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(asInstanceOf, numRecords, <init>, walRecordHandleOption, isBlockIdValid, !=, blockId, ReceivedBlockInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala: Set(streamId, blockStoreResult, asInstanceOf, numRecords, metadataOption, isInstanceOf, <init>, ==, getClass, blockId, ReceivedBlockInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/SocketInputDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, onReceiverStarted, notify, register, getReceiver, getOrCompute, countByValueAndWindow$default$3, window, name, count, reportError, saveAsObjectFiles$default$2, wait, setGraph, streamId, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, streamUID, ReceiverRateController, countByValueAndWindow, map, mustCheckpoint, onStreamingStarted, updateCheckpointData, equals, rememberDuration, getLatestRate, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, SocketReceiver, compute, saveAsObjectFiles, receive, countByValue$default$2, mapPartitions$default$2, isTimeValid, store, saveAsTextFiles$default$2, logTrace, isStarted, isTraceEnabled, initializeLogIfNecessary$default$2, createRDDWithLocalProperties, stop, onReceiverError, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, onBatchCompleted, isInstanceOf, filter, onBatchStarted, publish, isStopped, persist, setReceiverId, checkpointData, <init>, preferredLocation, id, rateController, zeroTime, lastValidTime, createBlockRDD, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, onBatchSubmitted, reduce, $init$, checkpoint, countByValue, supervisor, toString, bytesToLines, attachSupervisor, graph, logError, !=, getClass, isInitialized, logWarning, onStop, countByWindow, parentRememberDuration, generateJob, restart, transformWith, repartition, start, onOutputOperationCompleted, saveAsTextFiles, ne, onStart, transform, dependencies, onReceiverStopped, eq, storageLevel, log, generatedRDDs, onOutputOperationStarted, ##, finalize, hashCode, setContext, logDebug, SocketInputDStream, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, SocketReceiver, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, bytesToLines, graph, logError, !=, logWarning, start, ne, storageLevel, setContext, logDebug, SocketInputDStream, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, errorInfo, copy$default$2, $asInstanceOf, copy$default$5, state, productArity, equals, lastErrorMessage, asInstanceOf, ReceiverErrorInfo, synchronized, receiverId, $isInstanceOf, lastErrorTime, canEqual, copy$default$4, productPrefix, notifyAll, lastError, isInstanceOf, <init>, scheduledLocations, ==, clone, copy$default$7, $init$, endpoint, copy$default$3, copy, toString, toReceiverInfo, !=, getClass, copy$default$1, copy$default$6, ne, eq, productIterator, ##, finalize, productElement, hashCode, ReceiverTrackingInfo, runningExecutor.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverSchedulingPolicy.scala: Set(state, asInstanceOf, isInstanceOf, <init>, scheduledLocations, ==, ne, ReceiverTrackingInfo, runningExecutor)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(name, errorInfo, state, asInstanceOf, ReceiverErrorInfo, synchronized, receiverId, lastErrorTime, isInstanceOf, <init>, scheduledLocations, ==, endpoint, toString, toReceiverInfo, !=, ne, eq, ReceiverTrackingInfo, runningExecutor)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/RawTextHelper.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, splitAndCountPartitions, wait, $asInstanceOf, subtract, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, max, ==, clone, topK, warmUp, toString, !=, getClass, ne, RawTextHelper, add, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLogRandomReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, read, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, FileBasedWriteAheadLogRandomReader, clone, toString, !=, getClass, close, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/util/FileBasedWriteAheadLog.scala: Set(read, asInstanceOf, synchronized, isInstanceOf, <init>, ==, FileBasedWriteAheadLogRandomReader, toString, !=, getClass, close, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobScheduler.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	e, notify, JobStarted, reportError, wait, startTime, copy$default$2, $asInstanceOf, getPendingTimes, productArity, equals, submitJobSet, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, job, ErrorReported, logTrace, isStarted, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, stop, inputInfoTracker, logName, ssc, notifyAll, isInstanceOf, BATCH_TIME_PROPERTY_KEY, JobCompleted, <init>, OUTPUT_OP_ID_PROPERTY_KEY, clock, listenerBus, ==, clone, JobSchedulerEvent, $init$, copy, toString, msg, logError, !=, receiverTracker, getClass, logWarning, copy$default$1, start, ne, completedTime, JobScheduler, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(e, synchronized, stop, ssc, <init>, listenerBus, ==, clone, toString, logError, !=, logWarning, start, ne, JobScheduler, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/FileInputDStream.scala: Set(e, asInstanceOf, synchronized, inputInfoTracker, ssc, isInstanceOf, <init>, clock, ==, toString, logError, getClass, logWarning, ne, JobScheduler, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala: Set(startTime, asInstanceOf, inputInfoTracker, ssc, <init>, logError, !=, receiverTracker, logWarning, JobScheduler)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala: Set(synchronized, ssc, BATCH_TIME_PROPERTY_KEY, <init>, OUTPUT_OP_ID_PROPERTY_KEY, ==, !=, ne, JobScheduler)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(e, reportError, asInstanceOf, synchronized, isStarted, stop, ssc, isInstanceOf, <init>, clock, listenerBus, ==, toString, msg, logError, !=, logWarning, start, ne, JobScheduler, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/JobGenerator.scala: Set(e, reportError, startTime, submitJobSet, asInstanceOf, synchronized, stop, inputInfoTracker, ssc, isInstanceOf, <init>, clock, ==, toString, !=, receiverTracker, logWarning, start, JobScheduler, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala: Set(e, startTime, getPendingTimes, asInstanceOf, synchronized, stop, ssc, <init>, ==, toString, msg, logError, !=, logWarning, ne, JobScheduler, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, textFileStream, wait, $asInstanceOf, union, equals, remember, jarOfClass, asInstanceOf, synchronized, $isInstanceOf, queueStream, addStreamingListener, binaryRecordsStream, socketStream, stop, JavaStreamingContext, ssc, notifyAll, fileStream, isInstanceOf, getState, <init>, rawSocketStream, ==, getOrCreate, clone, sparkContext, checkpoint, socketTextStream, toString, awaitTermination, !=, awaitTerminationOrTimeout, getClass, start, receiverStream, close, ne, transformToPair, transform, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	classTag, scalaIntToJavaLong.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/python/PythonDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaMapWithStateDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaPairDStream.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaReceiverInputDStream.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, copy$default$2, streamId, $asInstanceOf, location, copy$default$5, productArity, equals, lastErrorMessage, ReceiverInfo, asInstanceOf, synchronized, $isInstanceOf, copy$default$8, lastErrorTime, canEqual, copy$default$4, executorId, productPrefix, notifyAll, lastError, isInstanceOf, <init>, ==, clone, copy$default$7, $init$, copy$default$3, copy, toString, !=, getClass, copy$default$1, copy$default$6, ne, eq, productIterator, ##, finalize, productElement, hashCode, active.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingListenerWrapper.scala: Set(name, streamId, location, lastErrorMessage, ReceiverInfo, lastErrorTime, executorId, lastError, <init>, active)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTrackingInfo.scala: Set(name, lastErrorMessage, ReceiverInfo, asInstanceOf, lastErrorTime, executorId, lastError, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingPage.scala: Set(name, streamId, location, lastErrorMessage, ReceiverInfo, synchronized, lastErrorTime, executorId, lastError, <init>, ne, active)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/status/api/v1/streaming/ApiStreamingRootResource.scala: Set(name, streamId, location, lastErrorMessage, ReceiverInfo, asInstanceOf, lastErrorTime, executorId, lastError, isInstanceOf, <init>, ==, toString, ne, active)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/StreamingListener.scala: Set(ReceiverInfo, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/ui/StreamingJobProgressListener.scala: Set(streamId, ReceiverInfo, synchronized, <init>, ==, !=, ne, active)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala: Set(name, streamId, ReceiverInfo, asInstanceOf, synchronized, lastErrorTime, executorId, isInstanceOf, <init>, ==, toString, !=, ne, eq, active)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ordering, canBuildFrom, mkOrderingOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/dstream/RawInputDStream.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	reduceByWindow, creationSite, onReceiverStarted, notify, register, getReceiver, getOrCompute, countByValueAndWindow$default$3, window, name, count, reportError, saveAsObjectFiles$default$2, wait, setGraph, streamId, $asInstanceOf, mapPartitions, slideDuration, union, clearCheckpointData, streamUID, ReceiverRateController, countByValueAndWindow, map, mustCheckpoint, onStreamingStarted, updateCheckpointData, equals, rememberDuration, getLatestRate, remember, baseScope, asInstanceOf, print, context, initializeLogIfNecessary, glom, synchronized, $isInstanceOf, compute, saveAsObjectFiles, countByValue$default$2, mapPartitions$default$2, isTimeValid, store, saveAsTextFiles$default$2, logTrace, isStarted, blockPushingThread, isTraceEnabled, initializeLogIfNecessary$default$2, RawInputDStream, createRDDWithLocalProperties, stop, onReceiverError, restoreCheckpointData, logName, ssc, notifyAll, initialize, cache, onBatchCompleted, isInstanceOf, filter, onBatchStarted, publish, isStopped, persist, setReceiverId, checkpointData, <init>, preferredLocation, id, rateController, zeroTime, lastValidTime, createBlockRDD, flatMap, foreachRDD, countByValue$default$1, ==, validateAtStart, clearMetadata, clone, countByValueAndWindow$default$4, slice, checkpointDuration, onBatchSubmitted, reduce, $init$, checkpoint, countByValue, supervisor, RawNetworkReceiver, toString, attachSupervisor, graph, logError, !=, getClass, isInitialized, logWarning, onStop, countByWindow, parentRememberDuration, generateJob, restart, transformWith, repartition, start, onOutputOperationCompleted, saveAsTextFiles, ne, onStart, transform, dependencies, onReceiverStopped, eq, storageLevel, log, generatedRDDs, onOutputOperationStarted, ##, finalize, hashCode, setContext, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/streaming/src/main/scala/org/apache/spark/streaming/StreamingContext.scala: Set(name, map, remember, synchronized, RawInputDStream, stop, restoreCheckpointData, ssc, filter, <init>, ==, clone, checkpointDuration, checkpoint, toString, graph, logError, !=, logWarning, start, ne, storageLevel, setContext, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] New invalidations:[0m
[0m[[0mdebug[0m] [0m[naha] 	Set()[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Previously invalidated, but (transitively) depend on new invalidations:[0m
[0m[[0mdebug[0m] [0m[naha] 	Set()[0m
[0m[[0mdebug[0m] [0m[naha] All newly invalidated sources after taking into account (previously) recompiled sources:Set()[0m
