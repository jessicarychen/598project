[0m[[0mdebug[0m] [0m[naha] [0m
[0m[[0mdebug[0m] [0m[naha] Initial source changes: [0m
[0m[[0mdebug[0m] [0m[naha] 	removed:Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	added: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceOptions.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF21.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CodecStreams.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF14.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecutionException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF16.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF17.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/SaveMode.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF18.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SourceOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF5.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTaskRetryException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/r/MapPartitionsRWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF9.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF7.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/test/ExamplePointUDT.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF8.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF10.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF1.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDF.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/VariableSubstitution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF3.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF6.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GroupedIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/RecordReaderIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonInferSchema.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF13.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/SchemaColumnConvertNotSupportedException.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF0.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BucketingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF12.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CoGroupedIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryStatus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF4.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/BoundOrdering.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetricInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF20.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/UnsafeRowSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortPrefixUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/Statistics.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FailureSafeParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/FileRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetLogRedirector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriteStatsTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF11.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ForeachWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF22.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/filters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ExperimentalMethods.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF15.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/HiveSerDe.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF19.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] 	modified: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated products: Set()[0m
[0m[[0mdebug[0m] [0m[naha] External API changes: API Changes: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Modified binary dependencies: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Initial directly invalidated sources: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceOptions.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF21.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CodecStreams.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF14.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecutionException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF16.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF17.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/SaveMode.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF18.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SourceOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF5.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTaskRetryException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/r/MapPartitionsRWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF9.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF7.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/test/ExamplePointUDT.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF8.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF10.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF1.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDF.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/VariableSubstitution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF3.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF6.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GroupedIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/RecordReaderIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonInferSchema.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF13.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/SchemaColumnConvertNotSupportedException.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF0.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BucketingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF12.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CoGroupedIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryStatus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF4.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/BoundOrdering.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetricInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF20.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/UnsafeRowSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortPrefixUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/Statistics.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FailureSafeParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/FileRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetLogRedirector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriteStatsTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF11.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ForeachWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF22.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/filters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ExperimentalMethods.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF15.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/HiveSerDe.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF19.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] [0m
[0m[[0mdebug[0m] [0m[naha] Sources indirectly invalidated by:[0m
[0m[[0mdebug[0m] [0m[naha] 	product: Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	binary dep: Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	external source: Set()[0m
[0m[[0mdebug[0m] [0mAll initially invalidated sources: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceOptions.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF21.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CodecStreams.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF14.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecutionException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF16.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF17.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/SaveMode.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF18.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SourceOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF5.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTaskRetryException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/r/MapPartitionsRWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF9.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF7.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/test/ExamplePointUDT.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF8.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF10.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF1.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDF.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/VariableSubstitution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF3.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF6.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GroupedIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/RecordReaderIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonInferSchema.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF13.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/SchemaColumnConvertNotSupportedException.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF0.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BucketingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF12.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CoGroupedIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryStatus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF4.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/BoundOrdering.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetricInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF20.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/UnsafeRowSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortPrefixUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/Statistics.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FailureSafeParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/FileRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetLogRedirector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriteStatsTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF11.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ForeachWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF22.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/filters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ExperimentalMethods.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF15.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/HiveSerDe.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF19.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceOptions.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF21.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CodecStreams.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF14.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecutionException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF16.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF17.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/SaveMode.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF18.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SourceOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF5.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTaskRetryException.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/r/MapPartitionsRWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF9.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF7.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/test/ExamplePointUDT.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF8.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF10.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF1.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDF.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/VariableSubstitution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF3.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF6.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GroupedIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/RecordReaderIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonInferSchema.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF13.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/SchemaColumnConvertNotSupportedException.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF0.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BucketingUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF12.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CoGroupedIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryStatus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF4.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/BoundOrdering.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetricInfo.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF20.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/UnsafeRowSerializer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortPrefixUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/Statistics.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FailureSafeParser.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/FileRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetLogRedirector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriteStatsTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF11.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ForeachWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF22.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/filters.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ExperimentalMethods.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF15.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/HiveSerDe.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF19.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/package-info.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] Recompiling all 414 sources: invalidated sources (414) exceeded 50.0% of all sources[0m
[0m[[0minfo[0m] [0mCompiling 319 Scala sources and 95 Java sources to /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/target/scala-2.11/classes...[0m
[0m[[0mdebug[0m] [0mGetting org.scala-sbt:compiler-interface:0.13.16:component from component compiler for Scala 2.11.8[0m
[0m[[0mdebug[0m] [0mGetting org.scala-sbt:compiler-interface:0.13.16:component from component compiler for Scala 2.11.8[0m
[0m[[0mdebug[0m] [0mRunning cached compiler 2ad5adad, interfacing (CompilerInterface) with Scala compiler version 2.11.8[0m
[0m[[0mdebug[0m] [0mCalling Scala compiler with arguments  (CompilerInterface):[0m
[0m[[0mdebug[0m] [0m	-unchecked[0m
[0m[[0mdebug[0m] [0m	-deprecation[0m
[0m[[0mdebug[0m] [0m	-feature[0m
[0m[[0mdebug[0m] [0m	-explaintypes[0m
[0m[[0mdebug[0m] [0m	-Yno-adapted-args[0m
[0m[[0mdebug[0m] [0m	-P:genjavadoc:out=/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/target/java[0m
[0m[[0mdebug[0m] [0m	-P:genjavadoc:strictVisibility=true[0m
[0m[[0mdebug[0m] [0m	-Xplugin:/home/vm1/.ivy2/cache/com.typesafe.genjavadoc/genjavadoc-plugin_2.11.8/jars/genjavadoc-plugin_2.11.8-0.10.jar[0m
[0m[[0mdebug[0m] [0m	-target:jvm-1.8[0m
[0m[[0mdebug[0m] [0m	-sourcepath[0m
[0m[[0mdebug[0m] [0m	/usr/local/spark-2.3.2-bin-hadoop2.7[0m
[0m[[0mdebug[0m] [0m	-bootclasspath[0m
[0m[[0mdebug[0m] [0m	/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/classes:/home/vm1/.ivy2/cache/org.scala-lang/scala-library/jars/scala-library-2.11.8.jar[0m
[0m[[0mdebug[0m] [0m	-classpath[0m
[0m[[0mdebug[0m] [0m	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/target/scala-2.11/classes:/usr/local/spark-2.3.2-bin-hadoop2.7/common/sketch/target/scala-2.11/spark-sketch_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/tags/target/scala-2.11/spark-tags_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/core/target/scala-2.11/spark-core_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/launcher/target/scala-2.11/spark-launcher_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/kvstore/target/scala-2.11/spark-kvstore_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/network-common/target/scala-2.11/spark-network-common_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/network-shuffle/target/scala-2.11/spark-network-shuffle_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/common/unsafe/target/scala-2.11/spark-unsafe_2.11-2.3.2.jar:/usr/local/spark-2.3.2-bin-hadoop2.7/sql/catalyst/target/scala-2.11/spark-catalyst_2.11-2.3.2.jar:/home/vm1/.ivy2/cache/org.spark-project.spark/unused/jars/unused-1.0.0.jar:/home/vm1/.ivy2/cache/com.google.guava/guava/bundles/guava-14.0.1.jar:/home/vm1/.ivy2/cache/org.jpmml/pmml-model/jars/pmml-model-1.2.15.jar:/home/vm1/.ivy2/cache/org.jpmml/pmml-schema/jars/pmml-schema-1.2.15.jar:/home/vm1/.ivy2/cache/org.fusesource.leveldbjni/leveldbjni-all/bundles/leveldbjni-all-1.8.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.core/jackson-databind/bundles/jackson-databind-2.6.7.1.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.core/jackson-annotations/bundles/jackson-annotations-2.6.7.jar:/home/vm1/.ivy2/cache/io.netty/netty-all/jars/netty-all-4.1.17.Final.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-lang3/jars/commons-lang3-3.5.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-core/bundles/metrics-core-3.1.5.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-crypto/jars/commons-crypto-1.0.0.jar:/home/vm1/.ivy2/cache/com.twitter/chill_2.11/jars/chill_2.11-0.8.4.jar:/home/vm1/.ivy2/cache/com.twitter/chill-java/jars/chill-java-0.8.4.jar:/home/vm1/.ivy2/cache/com.esotericsoftware/kryo-shaded/bundles/kryo-shaded-3.0.3.jar:/home/vm1/.ivy2/cache/com.esotericsoftware/minlog/bundles/minlog-1.3.0.jar:/home/vm1/.ivy2/cache/org.objenesis/objenesis/jars/objenesis-2.1.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro/jars/avro-1.7.7.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-core-asl/jars/jackson-core-asl-1.9.13.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-mapper-asl/jars/jackson-mapper-asl-1.9.13.jar:/home/vm1/.ivy2/cache/org.xerial.snappy/snappy-java/bundles/snappy-java-1.1.2.6.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-compress/jars/commons-compress-1.4.1.jar:/home/vm1/.ivy2/cache/org.tukaani/xz/jars/xz-1.0.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro-mapred/jars/avro-mapred-1.7.7-hadoop2.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7-tests.jar:/home/vm1/.ivy2/cache/org.apache.avro/avro-ipc/jars/avro-ipc-1.7.7.jar:/home/vm1/.ivy2/cache/org.apache.xbean/xbean-asm5-shaded/bundles/xbean-asm5-shaded-4.4.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-client/jars/hadoop-client-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-common/jars/hadoop-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-annotations/jars/hadoop-annotations-2.6.5.jar:/home/vm1/.ivy2/cache/commons-cli/commons-cli/jars/commons-cli-1.2.jar:/home/vm1/.ivy2/cache/org.apache.commons/commons-math3/jars/commons-math3-3.4.1.jar:/home/vm1/.ivy2/cache/xmlenc/xmlenc/jars/xmlenc-0.52.jar:/home/vm1/.ivy2/cache/commons-httpclient/commons-httpclient/jars/commons-httpclient-3.1.jar:/home/vm1/.ivy2/cache/commons-io/commons-io/jars/commons-io-2.4.jar:/home/vm1/.ivy2/cache/commons-net/commons-net/jars/commons-net-3.1.jar:/home/vm1/.ivy2/cache/commons-collections/commons-collections/jars/commons-collections-3.2.2.jar:/home/vm1/.ivy2/cache/log4j/log4j/bundles/log4j-1.2.17.jar:/home/vm1/.ivy2/cache/commons-lang/commons-lang/jars/commons-lang-2.6.jar:/home/vm1/.ivy2/cache/commons-configuration/commons-configuration/jars/commons-configuration-1.6.jar:/home/vm1/.ivy2/cache/commons-digester/commons-digester/jars/commons-digester-1.8.jar:/home/vm1/.ivy2/cache/commons-beanutils/commons-beanutils/jars/commons-beanutils-1.7.0.jar:/home/vm1/.ivy2/cache/commons-beanutils/commons-beanutils-core/jars/commons-beanutils-core-1.8.0.jar:/home/vm1/.ivy2/cache/com.google.protobuf/protobuf-java/bundles/protobuf-java-2.5.0.jar:/home/vm1/.ivy2/cache/com.google.code.gson/gson/jars/gson-2.2.4.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-auth/jars/hadoop-auth-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.directory.server/apacheds-kerberos-codec/bundles/apacheds-kerberos-codec-2.0.0-M15.jar:/home/vm1/.ivy2/cache/org.apache.directory.server/apacheds-i18n/bundles/apacheds-i18n-2.0.0-M15.jar:/home/vm1/.ivy2/cache/org.apache.directory.api/api-asn1-api/bundles/api-asn1-api-1.0.0-M20.jar:/home/vm1/.ivy2/cache/org.apache.directory.api/api-util/bundles/api-util-1.0.0-M20.jar:/home/vm1/.ivy2/cache/org.apache.curator/curator-framework/bundles/curator-framework-2.6.0.jar:/home/vm1/.ivy2/cache/org.apache.curator/curator-client/bundles/curator-client-2.6.0.jar:/home/vm1/.ivy2/cache/org.apache.zookeeper/zookeeper/jars/zookeeper-3.4.6.jar:/home/vm1/.ivy2/cache/jline/jline/jars/jline-0.9.94.jar:/home/vm1/.ivy2/cache/io.netty/netty/bundles/netty-3.9.9.Final.jar:/home/vm1/.ivy2/cache/org.apache.curator/curator-recipes/bundles/curator-recipes-2.6.0.jar:/home/vm1/.ivy2/cache/org.htrace/htrace-core/jars/htrace-core-3.0.4.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-hdfs/jars/hadoop-hdfs-2.6.5.jar:/home/vm1/.ivy2/cache/org.mortbay.jetty/jetty-util/jars/jetty-util-6.1.26.jar:/home/vm1/.ivy2/cache/xerces/xercesImpl/jars/xercesImpl-2.9.1.jar:/home/vm1/.ivy2/cache/xml-apis/xml-apis/jars/xml-apis-1.3.04.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-app/jars/hadoop-mapreduce-client-app-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-common/jars/hadoop-mapreduce-client-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-common/jars/hadoop-yarn-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-api/jars/hadoop-yarn-api-2.6.5.jar:/home/vm1/.ivy2/cache/javax.xml.bind/jaxb-api/jars/jaxb-api-2.2.2.jar:/home/vm1/.ivy2/cache/javax.xml.stream/stax-api/jars/stax-api-1.0-2.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-jaxrs/jars/jackson-jaxrs-1.9.13.jar:/home/vm1/.ivy2/cache/org.codehaus.jackson/jackson-xc/jars/jackson-xc-1.9.13.jar:/home/vm1/.ivy2/cache/com.google.inject/guice/jars/guice-3.0.jar:/home/vm1/.ivy2/cache/javax.inject/javax.inject/jars/javax.inject-1.jar:/home/vm1/.ivy2/cache/aopalliance/aopalliance/jars/aopalliance-1.0.jar:/home/vm1/.ivy2/cache/org.sonatype.sisu.inject/cglib/jars/cglib-2.2.1-v20090111.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-client/jars/hadoop-yarn-client-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-core/jars/hadoop-mapreduce-client-core-2.6.5.jar:/home/vm1/.ivy2/cache/org.slf4j/slf4j-log4j12/jars/slf4j-log4j12-1.7.16.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-yarn-server-common/jars/hadoop-yarn-server-common-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-shuffle/jars/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/vm1/.ivy2/cache/org.apache.hadoop/hadoop-mapreduce-client-jobclient/jars/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/vm1/.ivy2/cache/org.codehaus.jettison/jettison/bundles/jettison-1.1.jar:/home/vm1/.ivy2/cache/net.java.dev.jets3t/jets3t/jars/jets3t-0.9.4.jar:/home/vm1/.ivy2/cache/org.apache.httpcomponents/httpcore/jars/httpcore-4.4.1.jar:/home/vm1/.ivy2/cache/org.apache.httpcomponents/httpclient/jars/httpclient-4.5.jar:/home/vm1/.ivy2/cache/commons-codec/commons-codec/jars/commons-codec-1.11.jar:/home/vm1/.ivy2/cache/javax.activation/activation/jars/activation-1.1.1.jar:/home/vm1/.ivy2/cache/org.bouncycastle/bcprov-jdk15on/jars/bcprov-jdk15on-1.52.jar:/home/vm1/.ivy2/cache/com.jamesmurty.utils/java-xmlbuilder/jars/java-xmlbuilder-1.1.jar:/home/vm1/.ivy2/cache/net.iharder/base64/jars/base64-2.3.8.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-plus/jars/jetty-plus-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-webapp/jars/jetty-webapp-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-xml/jars/jetty-xml-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-util/jars/jetty-util-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-servlet/jars/jetty-servlet-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-security/jars/jetty-security-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-server/jars/jetty-server-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/javax.servlet/javax.servlet-api/jars/javax.servlet-api-3.1.0.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-http/jars/jetty-http-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-io/jars/jetty-io-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-jndi/jars/jetty-jndi-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-continuation/jars/jetty-continuation-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-proxy/jars/jetty-proxy-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-client/jars/jetty-client-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.eclipse.jetty/jetty-servlets/jars/jetty-servlets-9.3.24.v20180605.jar:/home/vm1/.ivy2/cache/org.slf4j/jul-to-slf4j/jars/jul-to-slf4j-1.7.16.jar:/home/vm1/.ivy2/cache/org.slf4j/jcl-over-slf4j/jars/jcl-over-slf4j-1.7.16.jar:/home/vm1/.ivy2/cache/com.ning/compress-lzf/bundles/compress-lzf-1.0.3.jar:/home/vm1/.ivy2/cache/org.lz4/lz4-java/jars/lz4-java-1.4.0.jar:/home/vm1/.ivy2/cache/com.github.luben/zstd-jni/bundles/zstd-jni-1.3.2-2.jar:/home/vm1/.ivy2/cache/org.roaringbitmap/RoaringBitmap/bundles/RoaringBitmap-0.5.11.jar:/home/vm1/.ivy2/cache/org.json4s/json4s-jackson_2.11/jars/json4s-jackson_2.11-3.2.11.jar:/home/vm1/.ivy2/cache/org.json4s/json4s-core_2.11/jars/json4s-core_2.11-3.2.11.jar:/home/vm1/.ivy2/cache/org.json4s/json4s-ast_2.11/jars/json4s-ast_2.11-3.2.11.jar:/home/vm1/.ivy2/cache/org.scala-lang/scalap/jars/scalap-2.11.8.jar:/home/vm1/.ivy2/cache/org.scala-lang/scala-compiler/jars/scala-compiler-2.11.8.jar:/home/vm1/.ivy2/cache/org.scala-lang/scala-reflect/jars/scala-reflect-2.11.8.jar:/home/vm1/.ivy2/cache/org.scala-lang.modules/scala-xml_2.11/bundles/scala-xml_2.11-1.0.4.jar:/home/vm1/.ivy2/cache/org.scala-lang.modules/scala-parser-combinators_2.11/bundles/scala-parser-combinators_2.11-1.0.4.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.core/jersey-client/jars/jersey-client-2.22.2.jar:/home/vm1/.ivy2/cache/javax.ws.rs/javax.ws.rs-api/jars/javax.ws.rs-api-2.0.1.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.core/jersey-common/jars/jersey-common-2.22.2.jar:/home/vm1/.ivy2/cache/javax.annotation/javax.annotation-api/jars/javax.annotation-api-1.2.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.bundles.repackaged/jersey-guava/bundles/jersey-guava-2.22.2.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/hk2-api/jars/hk2-api-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/hk2-utils/jars/hk2-utils-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2.external/aopalliance-repackaged/jars/aopalliance-repackaged-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2.external/javax.inject/jars/javax.inject-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/hk2-locator/jars/hk2-locator-2.4.0-b34.jar:/home/vm1/.ivy2/cache/org.javassist/javassist/bundles/javassist-3.18.1-GA.jar:/home/vm1/.ivy2/cache/org.glassfish.hk2/osgi-resource-locator/jars/osgi-resource-locator-1.0.1.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.core/jersey-server/jars/jersey-server-2.22.2.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.media/jersey-media-jaxb/jars/jersey-media-jaxb-2.22.2.jar:/home/vm1/.ivy2/cache/javax.validation/validation-api/jars/validation-api-1.1.0.Final.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet/jars/jersey-container-servlet-2.22.2.jar:/home/vm1/.ivy2/cache/org.glassfish.jersey.containers/jersey-container-servlet-core/jars/jersey-container-servlet-core-2.22.2.jar:/home/vm1/.ivy2/cache/com.clearspring.analytics/stream/jars/stream-2.7.0.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-jvm/bundles/metrics-jvm-3.1.5.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-json/bundles/metrics-json-3.1.5.jar:/home/vm1/.ivy2/cache/io.dropwizard.metrics/metrics-graphite/bundles/metrics-graphite-3.1.5.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-scala_2.11/bundles/jackson-module-scala_2.11-2.6.7.1.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.module/jackson-module-paranamer/bundles/jackson-module-paranamer-2.7.9.jar:/home/vm1/.ivy2/cache/com.thoughtworks.paranamer/paranamer/bundles/paranamer-2.8.jar:/home/vm1/.ivy2/cache/org.apache.ivy/ivy/jars/ivy-2.4.0.jar:/home/vm1/.ivy2/cache/oro/oro/jars/oro-2.0.8.jar:/home/vm1/.ivy2/cache/net.razorvine/pyrolite/jars/pyrolite-4.13.jar:/home/vm1/.ivy2/cache/net.sf.py4j/py4j/jars/py4j-0.10.7.jar:/home/vm1/.ivy2/cache/org.codehaus.janino/janino/jars/janino-3.0.8.jar:/home/vm1/.ivy2/cache/org.codehaus.janino/commons-compiler/jars/commons-compiler-3.0.8.jar:/home/vm1/.ivy2/cache/org.antlr/antlr4-runtime/jars/antlr4-runtime-4.7.jar:/home/vm1/.ivy2/cache/com.univocity/univocity-parsers/jars/univocity-parsers-2.5.9.jar:/home/vm1/.ivy2/cache/org.apache.orc/orc-core/jars/orc-core-1.4.4-nohive.jar:/home/vm1/.ivy2/cache/io.airlift/aircompressor/jars/aircompressor-0.8.jar:/home/vm1/.ivy2/cache/org.apache.orc/orc-mapreduce/jars/orc-mapreduce-1.4.4-nohive.jar:/home/vm1/.ivy2/cache/org.apache.parquet/parquet-column/jars/parquet-column-1.8.3.jar:/home/vm1/.ivy2/cache/org.apache.parquet/parquet-common/jars/parquet-common-1.8.3.jar:/home/vm1/.ivy2/cache/org.apache.parquet/parquet-encoding/jars/parquet-encoding-1.8.3.jar:/home/vm1/.ivy2/cache/org.apache.parquet/parquet-hadoop/jars/parquet-hadoop-1.8.3.jar:/home/vm1/.ivy2/cache/org.apache.parquet/parquet-format/jars/parquet-format-2.3.1.jar:/home/vm1/.ivy2/cache/org.apache.parquet/parquet-jackson/jars/parquet-jackson-1.8.3.jar:/home/vm1/.ivy2/cache/org.apache.arrow/arrow-vector/jars/arrow-vector-0.8.0.jar:/home/vm1/.ivy2/cache/org.apache.arrow/arrow-format/jars/arrow-format-0.8.0.jar:/home/vm1/.ivy2/cache/com.vlkan/flatbuffers/jars/flatbuffers-1.2.0-3f79e055.jar:/home/vm1/.ivy2/cache/org.apache.arrow/arrow-memory/jars/arrow-memory-0.8.0.jar:/home/vm1/.ivy2/cache/com.google.code.findbugs/jsr305/jars/jsr305-3.0.2.jar:/home/vm1/.ivy2/cache/org.slf4j/slf4j-api/jars/slf4j-api-1.7.25.jar:/home/vm1/.ivy2/cache/joda-time/joda-time/jars/joda-time-2.9.9.jar:/home/vm1/.ivy2/cache/com.fasterxml.jackson.core/jackson-core/bundles/jackson-core-2.7.9.jar:/home/vm1/.ivy2/cache/com.carrotsearch/hppc/bundles/hppc-0.7.2.jar[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala:46: class ProcessingTime in package streaming is deprecated: use Trigger.ProcessingTime(intervalMs)[0m
[0m[[33mwarn[0m] [0mcase class ProcessingTimeExecutor(processingTime: ProcessingTime, clock: Clock = new SystemClock())[0m
[0m[[33mwarn[0m] [0m                                                  ^[0m
[0m[[33mwarn[0m] [0m/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala:46: class ProcessingTime in package streaming is deprecated: use Trigger.ProcessingTime(intervalMs)[0m
[0m[[33mwarn[0m] [0mcase class ProcessingTimeExecutor(processingTime: ProcessingTime, clock: Clock = new SystemClock())[0m
[0m[[33mwarn[0m] [0m           ^[0m
[0m[[33mwarn[0m] [0mtwo warnings found[0m
[0m[[0mdebug[0m] [0mScala compilation took 46.577276096 s[0m
[0m[[0minfo[0m] [0mNote: Some input files use or override a deprecated API.[0m
[0m[[0minfo[0m] [0mNote: Recompile with -Xlint:deprecation for details.[0m
[0m[[0mdebug[0m] [0mJava compilation took 2.512196124 s[0m
[0m[[0mdebug[0m] [0mJava analysis took 0.428988801 s[0m
[0m[[0mdebug[0m] [0mJava compile + analysis took 2.98908381 s[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ExternalAppendOnlyUnsafeRowArray, wait, $asInstanceOf, DefaultInitialSizeOfInMemoryBuffer, equals, clear, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, generateIterator, toString, length, logError, !=, getClass, logWarning, isEmpty, ne, add, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala: Set(ExternalAppendOnlyUnsafeRowArray, clear, asInstanceOf, isInstanceOf, <init>, ==, generateIterator, toString, length, !=, logWarning, isEmpty, ne, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala: Set(ExternalAppendOnlyUnsafeRowArray, clear, asInstanceOf, isInstanceOf, <init>, ==, generateIterator, toString, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala: Set(ExternalAppendOnlyUnsafeRowArray, clear, <init>, ==, generateIterator, length, !=, isEmpty, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala: Set(ExternalAppendOnlyUnsafeRowArray, clear, asInstanceOf, isInstanceOf, <init>, ==, generateIterator, toString, length, !=, getClass, isEmpty, ne, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	used, notify, unapply, wait, HybridRowQueue, copy$default$2, $asInstanceOf, productArity, RowQueue, equals, taskMemoryManager, asInstanceOf, page, numFields, synchronized, allocateArray, DiskRowQueue, $isInstanceOf, canEqual, memManager, copy$default$4, productPrefix, notifyAll, InMemoryRowQueue, allocatePage, protected$freePage, isInstanceOf, freePage, <init>, remove, apply, ==, serMgr, clone, fields, tempDir, acquireMemory, $init$, freeArray, copy$default$3, copy, toString, file, !=, freeMemory, spill, getClass, copy$default$1, close, getMode, ne, add, numQueues, eq, productIterator, ##, finalize, productElement, hashCode, getUsed.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala: Set(HybridRowQueue, taskMemoryManager, asInstanceOf, isInstanceOf, <init>, remove, apply, ==, copy, !=, close, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	EpochCoordinatorMessage, notify, GetCurrentEpoch, wait, onNetworkError, EpochCoordinatorRef, copy$default$2, $asInstanceOf, epoch, productArity, equals, asInstanceOf, initializeLogIfNecessary, synchronized, self, $isInstanceOf, create, receive, ReportPartitionOffset, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, stop, logName, notifyAll, partitionId, isInstanceOf, <init>, offset, onError, ==, SetReaderPartitions, IncrementAndGetEpoch, receiveAndReply, clone, EpochCoordinator, $init$, onDisconnected, copy$default$3, copy, SetWriterPartitions, message, toString, logError, !=, get, StopContinuousExecutionWrites, onConnected, getClass, logWarning, copy$default$1, onStop, ne, onStart, CommitPartitionEpoch, rpcEnv, numPartitions, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(EpochCoordinatorRef, epoch, asInstanceOf, synchronized, create, stop, isInstanceOf, <init>, offset, ==, IncrementAndGetEpoch, toString, !=, get, StopContinuousExecutionWrites, ne, rpcEnv, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(EpochCoordinatorRef, asInstanceOf, partitionId, isInstanceOf, <init>, ==, SetWriterPartitions, message, toString, logError, get, ne, CommitPartitionEpoch, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(GetCurrentEpoch, EpochCoordinatorRef, epoch, asInstanceOf, ReportPartitionOffset, partitionId, isInstanceOf, <init>, offset, ==, copy, toString, !=, get, getClass, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(EpochCoordinatorRef, asInstanceOf, isInstanceOf, <init>, ==, SetReaderPartitions, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, metadataDir, <init>, ==, clone, $init$, ancestorIsMetadataDirectory, toString, logError, !=, getClass, logWarning, hasMetadata, ne, addBatch, eq, FileStreamSink, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, logWarning, hasMetadata, ne, eq, FileStreamSink, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(asInstanceOf, logTrace, isInstanceOf, <init>, ==, ancestorIsMetadataDirectory, toString, !=, getClass, logWarning, ne, eq, FileStreamSink, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, ==, toString, logWarning, hasMetadata, ne, eq, FileStreamSink, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(metadataDir, <init>, ==, toString, FileStreamSink, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createDataReader, DataReaderFactory, preferredLocations.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(createDataReader, DataReaderFactory, preferredLocations)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(DataReaderFactory)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala: Set(createDataReader, DataReaderFactory, preferredLocations)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(DataReaderFactory)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(DataReaderFactory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(createDataReader, DataReaderFactory, preferredLocations)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(createDataReader, DataReaderFactory, preferredLocations)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileIndex.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	partitionSchema, notify, refresh, wait, PartitionDirectory, copy$default$2, $asInstanceOf, listFiles, productArity, equals, inputFiles, asInstanceOf, metadataOpsTimeNs, synchronized, $isInstanceOf, canEqual, productPrefix, files, notifyAll, isInstanceOf, rootPaths, <init>, ==, FileIndex, clone, sizeInBytes, $init$, copy, values, toString, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, rootPaths, <init>, ==, FileIndex, values, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala: Set(partitionSchema, PartitionDirectory, listFiles, asInstanceOf, isInstanceOf, <init>, ==, FileIndex, values, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(partitionSchema, PartitionDirectory, listFiles, asInstanceOf, metadataOpsTimeNs, files, isInstanceOf, rootPaths, <init>, ==, FileIndex, values, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala: Set(PartitionDirectory, asInstanceOf, files, isInstanceOf, rootPaths, <init>, ==, FileIndex, values, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(partitionSchema, PartitionDirectory, listFiles, inputFiles, asInstanceOf, metadataOpsTimeNs, isInstanceOf, rootPaths, <init>, ==, FileIndex, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(partitionSchema, refresh, asInstanceOf, isInstanceOf, <init>, ==, FileIndex, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, ==, FileIndex, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala: Set(refresh, asInstanceOf, isInstanceOf, <init>, ==, FileIndex, sizeInBytes, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(refresh, asInstanceOf, isInstanceOf, rootPaths, <init>, ==, FileIndex, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala: Set(partitionSchema, inputFiles, asInstanceOf, isInstanceOf, <init>, ==, FileIndex, sizeInBytes, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, isInstanceOf, rootPaths, <init>, ==, FileIndex, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, FileIndex, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, files, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(partitionSchema, PartitionDirectory, listFiles, inputFiles, asInstanceOf, metadataOpsTimeNs, isInstanceOf, rootPaths, <init>, ==, FileIndex, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, ==, FileIndex, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(partitionSchema, asInstanceOf, files, isInstanceOf, rootPaths, <init>, ==, toString, !=, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, FileIndex, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(<init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, ==, FileIndex, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, FileIndex, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, ==, FileIndex, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, FileIndex, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, files, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(partitionSchema, PartitionDirectory, listFiles, inputFiles, asInstanceOf, metadataOpsTimeNs, isInstanceOf, rootPaths, <init>, ==, FileIndex, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, ==, FileIndex, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, files, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createDataReaderFactories, readSchema, DataSourceReader.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(createDataReaderFactories, readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(readSchema)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(createDataReaderFactories, readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataSourceReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(readSchema)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala: Set(DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(createDataReaderFactories, readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala: Set(DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala: Set(DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(createDataReaderFactories, readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(readSchema)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(createDataReaderFactories, readSchema, DataSourceReader)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF15.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF15.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF15.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF15.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF15.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF15.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF15)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF15.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	DecimalWriter, notify, FloatWriter, nullable, name, count, wait, $asInstanceOf, BooleanWriter, LongWriter, DateWriter, root, equals, asInstanceOf, BinaryWriter, synchronized, StructWriter, $isInstanceOf, create, finish, StringWriter, notifyAll, isInstanceOf, setValue, elementWriter, ShortWriter, <init>, schema, ==, clone, DoubleWriter, reset, toString, ArrowWriter, TimestampWriter, !=, getClass, dataType, ArrowFieldWriter, setNull, valueVector, ByteWriter, ne, ArrayWriter, eq, IntegerWriter, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala: Set(root, asInstanceOf, create, finish, <init>, schema, reset, ArrowWriter, !=, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala: Set(root, create, finish, <init>, schema, ==, reset, ArrowWriter, !=, write)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, fileCleanupDelayMs, metadataPath, parseVersion, wait, $asInstanceOf, equals, isDeletingExpiredLog, asInstanceOf, initializeLogIfNecessary, minBatchesToRetain, synchronized, $isInstanceOf, logTrace, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, logName, notifyAll, purge, isInstanceOf, VERSION, getLatest, <init>, compactLogs, allFiles, defaultCompactInterval, isBatchFile, ==, clone, purgeAfter, batchIdToPath, compactInterval, $init$, fileManager, FileStreamSourceLog, toString, logError, !=, get, getClass, logWarning, batchFilesFilter, ne, serialize, pathToBatchId, add, getOrderedBatchFiles, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(metadataPath, asInstanceOf, synchronized, logTrace, purge, isInstanceOf, VERSION, getLatest, <init>, allFiles, ==, FileStreamSourceLog, toString, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, nullable, asNondeterministic, wait, copy$default$2, $asInstanceOf, productArity, equals, inputTypes, asInstanceOf, f, synchronized, $isInstanceOf, canEqual, productPrefix, notifyAll, isInstanceOf, UserDefinedFunction, <init>, apply, withName, asNonNullable, ==, clone, $init$, copy$default$3, copy, toString, !=, deterministic, getClass, copy$default$1, dataType, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala: Set(asInstanceOf, f, isInstanceOf, UserDefinedFunction, <init>, apply, ==, dataType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(nullable, inputTypes, asInstanceOf, f, isInstanceOf, UserDefinedFunction, <init>, apply, asNonNullable, dataType, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(nullable, inputTypes, asInstanceOf, f, isInstanceOf, UserDefinedFunction, <init>, apply, withName, asNonNullable, ==, toString, dataType, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, newNaturalAscendingOrdering, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, treeString$default$2, unpersist, sortBy$default$2, find, parent, isLocallyCheckpointed, simpleString, getOrCompute, children, distinct$default$2, partitioner, coalesce, name, count, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, numberedTreeString, union, coalesce$default$3, zip, resetMetrics, printSchema, localCheckpoint, map, productArity, subtract, verboseStringWithSuffix, equals, pipe$default$5, intersection, treeString, schemaString, sortBy$default$3, argString, foreachPartition, countApprox$default$2, subqueries, scope, executeQuery, asInstanceOf, context, transformExpressions, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, doExecute, glom, generateTreeString, prepare, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, left, repartition$default$2, generateTreeString$default$6, allAttributes, nodeName, aggregate, numPartitionsInRdd2, $isInstanceOf, compute, doPrepare, rdd2, mapPartitions$default$2, min, getCheckpointFile, fold, rdd1, getOutputDeterministicLevel, logTrace, asCode, canEqual, expressions, treeAggregate$default$4, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, zipWithUniqueId, productPrefix, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, mapProductIterator, getNarrowAncestors, CartesianProductExec, collectFirst, cache, getNumPartitions, otherCopyArgs, missingInput, isInstanceOf, stringArgs, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, doCanonicalize, collectLeaves, references, persist, newMutableProjection$default$3, checkpointData, <init>, isCheckpointed, outputOrdering, generateTreeString$default$5, id, foreachUp, mapChildren, mapPartitionsWithIndexInternal, condition, countApproxDistinct, schema, max, outputDeterministicLevel, randomSampleWithRange, transformExpressionsDown, prettyJson, toDebugString, apply, ++, flatMap, take, countByValue$default$1, executeCollect, groupBy, treeReduce$default$2, ==, producedAttributes, fastEquals, sqlContext, randomSplit$default$2, origin, transformExpressionsUp, groupBy$default$4, clone, newMutableProjection, newPredicate, distinct, retag, sameResult, foreach, treeReduce, toLocalIterator, p, jsonFields, subexpressionEliminationEnabled, sparkContext, reduce, saveAsTextFile, outputPartitioning, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, copy$default$3, sample, copy, executeCollectPublic, inputSet, pipe$default$7, prepareSubqueries, toString, mapPartitionsInternal, preferredLocations, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, partitions, collect, getClass, longMetric, pipe, logWarning, getPartitions, output, copy$default$1, pipe$default$4, transformDown, transformAllExpressions, cartesian, mapExpressions, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, executeBroadcast, ne, requiredChildOrdering, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, transform, randomSplit, top, coalesce$default$2, withNewChildren, getCreationSite, computeOrReadCheckpoint, dependencies, statePrefix, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, waitForSubqueries, isReliablyCheckpointed, productIterator, toJSON, withScope, log, doExecuteBroadcast, executeToIterator, right, UnsafeCartesianRDD, ##, containsChild, newOrdering, finalize, treeAggregate, productElement, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, asInstanceOf, left, aggregate, expressions, conf, CartesianProductExec, isInstanceOf, <init>, outputOrdering, condition, schema, apply, ==, distinct, p, sparkContext, outputPartitioning, copy, toString, output, isEmpty, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, batches, wait, $asInstanceOf, commit, getBatch, DATE_FORMAT, sourceSchema, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, SCHEMA_TIMESTAMP, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, SCHEMA_REGULAR, <init>, schema, lastOffsetCommitted, ==, currentOffset, clone, $init$, toString, getOffset, logError, !=, getClass, logWarning, shortName, ne, TextSocketSource, createSource, eq, TextSocketSourceProvider, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, lowerBound, batchSize, JDBC_TXN_ISOLATION_LEVEL, JDBC_CUSTOM_DATAFRAME_COLUMN_TYPES, JDBC_NUM_PARTITIONS, url, JDBC_PARTITION_COLUMN, equals, asInstanceOf, isTruncate, customSchema, partitionColumn, synchronized, asProperties, $isInstanceOf, JDBC_BATCH_INSERT_SIZE, asConnectionProperties, sessionInitStatement, JDBC_LOWER_BOUND, isolationLevel, JDBC_URL, driverClass, JDBC_BATCH_FETCH_SIZE, fetchSize, JDBC_TABLE_NAME, notifyAll, isInstanceOf, upperBound, <init>, JDBC_SESSION_INIT_STATEMENT, createTableColumnTypes, JDBC_TRUNCATE, ==, clone, JDBCOptions, JDBC_DRIVER_CLASS, createTableOptions, toString, JDBC_CREATE_TABLE_OPTIONS, !=, JDBC_UPPER_BOUND, getClass, ne, JDBC_CREATE_TABLE_COLUMN_TYPES, numPartitions, eq, ##, finalize, table, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala: Set(url, asInstanceOf, asProperties, sessionInitStatement, fetchSize, isInstanceOf, <init>, ==, JDBCOptions, toString, !=, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(lowerBound, JDBC_NUM_PARTITIONS, url, JDBC_PARTITION_COLUMN, asInstanceOf, JDBC_LOWER_BOUND, JDBC_URL, JDBC_TABLE_NAME, isInstanceOf, upperBound, <init>, ==, JDBCOptions, toString, !=, JDBC_UPPER_BOUND, ne, numPartitions, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala: Set(url, equals, asInstanceOf, JDBC_BATCH_FETCH_SIZE, isInstanceOf, <init>, ==, JDBCOptions, !=, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(batchSize, JDBC_NUM_PARTITIONS, url, asInstanceOf, customSchema, asConnectionProperties, isolationLevel, driverClass, isInstanceOf, <init>, createTableColumnTypes, ==, JDBCOptions, createTableOptions, toString, !=, getClass, numPartitions, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala: Set(lowerBound, JDBC_NUM_PARTITIONS, url, isTruncate, partitionColumn, JDBC_LOWER_BOUND, upperBound, <init>, ==, JDBCOptions, JDBC_UPPER_BOUND, numPartitions, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(lowerBound, url, asInstanceOf, customSchema, asProperties, isInstanceOf, upperBound, <init>, ==, JDBCOptions, toString, !=, numPartitions, eq, table)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	WriterCommitMessage.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala: Set(WriterCommitMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala: Set(WriterCommitMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala: Set(WriterCommitMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala: Set(WriterCommitMessage)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(WriterCommitMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(WriterCommitMessage)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala: Set(WriterCommitMessage)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/WriterCommitMessage.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/RowIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, RowIterator, wait, $asInstanceOf, equals, asInstanceOf, advanceNext, synchronized, $isInstanceOf, fromScala, toScala, notifyAll, isInstanceOf, <init>, getRow, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala: Set(RowIterator, asInstanceOf, advanceNext, fromScala, toScala, isInstanceOf, <init>, getRow, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala: Set(RowIterator, asInstanceOf, advanceNext, toScala, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	readBooleans, readLong, readFloats, readBoolean, VectorizedValuesReader, readLongs, readFloat, readDoubles, readInteger, readByte, readBytes, readDouble, readBinary, readIntegers.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, generateProcessRow, wait, groupingAttributes, foldRight, takeWhile, $asInstanceOf, generateOutput, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, processRow, min, scanRight, fold, logTrace, nonEmpty, initializeBuffer, initializeAggregateFunctions, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, expressionAggInitialProjection, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, flatMap, take, outputForEmptyGroupingKeyWithoutInput, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, aggregateFunctions, TungstenAggregationIterator, toString, copyToArray, length, seq, logError, !=, generateResultProjection, collect, getClass, logWarning, hasDefiniteSize, patch, foldLeft, contains, allImperativeAggregateFunctions, isEmpty, ne, withPartial, allImperativeAggregateFunctionPositions, reversed, hasNext, indexOf, reduceLeft, groupingProjection, eq, sum, log, ##, scanLeft, finalize, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(groupingAttributes, map, equals, asInstanceOf, aggregate, forall, mkString, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, outputForEmptyGroupingKeyWithoutInput, ==, exists, next, zipWithIndex, TungstenAggregationIterator, toString, length, !=, logWarning, contains, isEmpty, ne, hasNext, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, convertTz, synchronized, $isInstanceOf, expandUDT, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, SPARK_METADATA_KEY, logName, notifyAll, isInstanceOf, prepareForRead, <init>, ==, clone, $init$, ParquetReadSupport, toString, logError, !=, clipParquetSchema, SPARK_ROW_REQUESTED_SCHEMA, getClass, logWarning, ne, init, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala: Set(asInstanceOf, SPARK_METADATA_KEY, isInstanceOf, <init>, ==, ParquetReadSupport, toString, !=, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, convertTz, SPARK_METADATA_KEY, isInstanceOf, <init>, ==, ParquetReadSupport, toString, SPARK_ROW_REQUESTED_SCHEMA, getClass, logWarning, ne, eq, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, wait, ProcessingTime, intervalMs, $asInstanceOf, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, create, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, apply, ==, clone, $init$, copy, toString, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, ProcessingTime, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, apply, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(ProcessingTime, asInstanceOf, isInstanceOf, <init>, apply, ==, copy, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala: Set(ProcessingTime, intervalMs, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala: Set(ProcessingTime, intervalMs, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(ProcessingTime, intervalMs, asInstanceOf, synchronized, create, isInstanceOf, <init>, apply, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, awaitInitialization, curried, name, wait, currentBatchId, copy$default$2, $asInstanceOf, outputMode, commit, explain, state, continuousSources, productArity, equals, inputRows, reportTimeTaken, asInstanceOf, initializeLogIfNecessary, committedOffsets, finishTrigger, streamDeathCause, synchronized, queryExecutionThread, $isInstanceOf, exception, streamMetadata, streamMetrics, tupled, awaitProgressLockCondition, updateStatusMessage, logTrace, canEqual, ExecutionStats, lastExecution, awaitEpoch, isTraceEnabled, initializeLogIfNecessary$default$2, triggerClock, offsetSeqMetadata, productPrefix, stop, EPOCH_COORDINATOR_ID_KEY, logName, notifyAll, pollingDelayMs, postEvent, isInstanceOf, startTrigger, awaitOffset, sink, runActivatedStream, <init>, offsetLog, resolvedCheckpointRoot, id, eventTimeStats, apply, lastProgress, noNewData, processAllAvailable, stateOperators, ==, clone, status, currentEpochCoordinatorId, minLogEntriesToMaintain, sparkSession, prettyIdString, $init$, getBatchDescriptionString, isActive, recentProgress, availableOffsets, copy$default$3, copy, toString, awaitTermination, ContinuousExecution, logError, !=, commitLog, getClass, logWarning, stopSources, copy$default$1, start, uniqueSources, START_EPOCH_KEY, checkpointFile, ne, sources, newData, runId, watermarkMsMap, eq, productIterator, log, addOffset, logicalPlan, ##, finalize, productElement, hashCode, logDebug, logInfo, explainInternal, currentStatus, trigger, awaitProgressLock.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, outputMode, state, asInstanceOf, synchronized, exception, triggerClock, notifyAll, isInstanceOf, sink, <init>, id, apply, ==, sparkSession, toString, ContinuousExecution, !=, logWarning, start, ne, sources, runId, trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala: Set(commit, asInstanceOf, synchronized, isInstanceOf, <init>, id, apply, ==, toString, ContinuousExecution, ne, sources, eq, addOffset, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(unapply, commit, asInstanceOf, EPOCH_COORDINATOR_ID_KEY, isInstanceOf, <init>, apply, ==, toString, ContinuousExecution, logError, START_EPOCH_KEY, ne, sources, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(asInstanceOf, EPOCH_COORDINATOR_ID_KEY, isInstanceOf, <init>, apply, ==, copy, toString, ContinuousExecution, !=, getClass, start, START_EPOCH_KEY, ne, sources, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(name, asInstanceOf, EPOCH_COORDINATOR_ID_KEY, isInstanceOf, <init>, apply, ==, toString, ContinuousExecution, sources)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, CompressibleColumnAccessor, notifyAll, initialize, isInstanceOf, ==, clone, $init$, toString, !=, getClass, extractTo, decompress, extractSingle, ne, hasNext, eq, underlyingBuffer, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala: Set(asInstanceOf, CompressibleColumnAccessor, initialize, isInstanceOf, ==, decompress, extractSingle)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(asInstanceOf, initialize, isInstanceOf, ==, toString, decompress, ne, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala: Set(==, underlyingBuffer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala: Set(CompressibleColumnAccessor, decompress, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala: Set(asInstanceOf, CompressibleColumnAccessor, initialize, isInstanceOf, ==, decompress, extractSingle)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newByteEncoder, newScalaDecimalEncoder, newBoxedDoubleEncoder, newBoxedFloatEncoder, newByteArrayEncoder, newIntArrayEncoder, newBooleanArrayEncoder, newFloatEncoder, newMapEncoder, newProductEncoder, newDoubleArrayEncoder, newBoxedLongEncoder, newBoxedShortEncoder, newSequenceEncoder, newIntEncoder, newShortArrayEncoder, newJavaDecimalEncoder, newStringEncoder, newBooleanEncoder, StringToColumn, newLongArrayEncoder, newShortEncoder, newFloatArrayEncoder, newBoxedIntEncoder, localSeqToDatasetHolder, newBoxedBooleanEncoder, newDoubleEncoder, newTimeStampEncoder, symbolToColumn, newLongEncoder, newDateEncoder, newProductArrayEncoder, rddToDatasetHolder, newBoxedByteEncoder, newSetEncoder, newStringArrayEncoder.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, ne, addBatch, eq, ##, finalize, hashCode, ForeachSink.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, ForeachSink)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getTableNames, notify, rowToRBytes, dfToRowRDD, createStructType, gapply, wait, SERIALIZED_R_DATA_SCHEMA, getOrCreateSparkSession, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, setSparkContextSessionConf, synchronized, dfToCols, $isInstanceOf, readSqlObject, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, writeSqlObject, createDF, isInstanceOf, ==, createStructField, clone, $init$, SQLUtils, toString, dapply, getSessionConf, logError, !=, getClass, logWarning, getJavaSparkContext, bytesToRow, ne, eq, getTables, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala: Set(rowToRBytes, SERIALIZED_R_DATA_SCHEMA, asInstanceOf, isInstanceOf, ==, SQLUtils, toString, bytesToRow, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/r/MapPartitionsRWrapper.scala: Set(rowToRBytes, SERIALIZED_R_DATA_SCHEMA, asInstanceOf, isInstanceOf, ==, SQLUtils, toString, !=, bytesToRow, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createReader, ReadSupportWithSchema.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(createReader, ReadSupportWithSchema)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, Sink, ==, clone, toString, !=, getClass, ne, addBatch, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, isInstanceOf, Sink, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(asInstanceOf, isInstanceOf, Sink, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala: Set(Sink)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, isInstanceOf, Sink, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(asInstanceOf, isInstanceOf, Sink, ==, toString, !=, ne, addBatch, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala: Set(asInstanceOf, synchronized, isInstanceOf, Sink, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(asInstanceOf, isInstanceOf, Sink, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, isInstanceOf, Sink, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(asInstanceOf, synchronized, isInstanceOf, Sink, ==, clone, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala: Set(Sink)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(asInstanceOf, isInstanceOf, Sink, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, isSplitable, wait, $asInstanceOf, infer, equals, asInstanceOf, synchronized, $isInstanceOf, JsonDataSource, inferSchema, notifyAll, readFile, isInstanceOf, <init>, apply, ==, clone, toString, !=, inferFromDataset, getClass, MultiLineJsonDataSource, TextInputJsonDataSource, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, inferFromDataset, TextInputJsonDataSource, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(isSplitable, JsonDataSource, inferSchema, readFile, isInstanceOf, <init>, apply, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, register, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, ==, clone, $init$, toString, logError, !=, getClass, logWarning, DriverRegistry, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(register, asInstanceOf, isInstanceOf, ==, toString, !=, getClass, logWarning, DriverRegistry)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions.scala: Set(register, ==, toString, getClass, DriverRegistry, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, tableIdent, verboseString, partitionSpec, semanticHash, noscan, wait, stats, AnalyzePartitionCommand, copy$default$2, $asInstanceOf, numberedTreeString, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy$default$3, copy, inputSet, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, tableIdent, partitionSpec, AnalyzePartitionCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, initialize, isInstanceOf, ==, clone, $init$, NullableColumnAccessor, toString, !=, getClass, extractTo, ne, hasNext, eq, underlyingBuffer, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(asInstanceOf, initialize, isInstanceOf, ==, toString, ne, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala: Set(==, NullableColumnAccessor, underlyingBuffer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala: Set(hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala: Set(asInstanceOf, initialize, isInstanceOf, ==, NullableColumnAccessor)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala: Set(asInstanceOf, initialize, isInstanceOf, ==, NullableColumnAccessor)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, columnStats, synchronized, $isInstanceOf, build, nulls, initialize$default$3, notifyAll, initialize, isInstanceOf, ==, clone, $init$, appendFrom, toString, NullableColumnBuilder, initialize$default$2, !=, getClass, buildNonNulls, nullCount, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(nulls, ==, buildNonNulls, nullCount)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(asInstanceOf, columnStats, initialize, isInstanceOf, ==, NullableColumnBuilder)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala: Set(asInstanceOf, columnStats, build, isInstanceOf, ==, appendFrom, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(nulls, ==, buildNonNulls, nullCount)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala: Set(columnStats, nulls, NullableColumnBuilder, nullCount)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(asInstanceOf, columnStats, initialize, isInstanceOf, ==, NullableColumnBuilder)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, commit, getBatch, sourceSchema, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, RateSourceProvider, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, SCHEMA, logName, notifyAll, isInstanceOf, VERSION, <init>, schema, clock, ==, clone, $init$, RateStreamSource, createContinuousReader, toString, getOffset, logError, !=, getClass, logWarning, shortName, ne, createSource, eq, log, valueAtSecond, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(asInstanceOf, RateSourceProvider, SCHEMA, isInstanceOf, <init>, ==, toString, !=, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	supportedCustomMetrics, notify, wait, $asInstanceOf, commit, getRange, productArity, COMMITTED, equals, stateStoreId, HDFSBackedStateStoreProvider, asInstanceOf, initializeLogIfNecessary, doMaintenance, synchronized, $isInstanceOf, UPDATING, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, iterator, logName, notifyAll, latestIterator, isInstanceOf, version, <init>, id, remove, cleanup, hasCommitted, ==, clone, $init$, HDFSBackedStateStore, put, toString, metrics, getStore, logError, !=, get, getClass, logWarning, close, ne, init, eq, productIterator, STATE, log, ##, finalize, productElement, hashCode, abort, logDebug, ABORTED, logInfo, MapType.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF6.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF6.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF6.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF6.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF6.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	UDF6, call.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(UDF6, call)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF6.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(UDF6, call)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ObjectColumnStats, count, wait, $asInstanceOf, ShortColumnStats, lowerBound, forAttribute, equals, asInstanceOf, NoopColumnStats, synchronized, $isInstanceOf, StringColumnStats, lower, collectedStatistics, columnType, upper, notifyAll, gatherNullStats, DecimalColumnStats, isInstanceOf, BooleanColumnStats, upperBound, BinaryColumnStats, <init>, schema, ==, clone, ColumnStatisticsSchema, sizeInBytes, $init$, FloatColumnStats, toString, ByteColumnStats, gatherStats, !=, getClass, IntColumnStats, nullCount, ColumnStats, gatherValueStats, ne, eq, PartitionStatistics, ##, finalize, hashCode, LongColumnStats, DoubleColumnStats.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(count, lowerBound, forAttribute, asInstanceOf, isInstanceOf, upperBound, <init>, schema, ==, ColumnStatisticsSchema, toString, nullCount, ne, eq, PartitionStatistics)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala: Set(asInstanceOf, collectedStatistics, isInstanceOf, <init>, ==, sizeInBytes, toString, ColumnStats, eq, PartitionStatistics)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(ObjectColumnStats, ShortColumnStats, asInstanceOf, StringColumnStats, columnType, DecimalColumnStats, isInstanceOf, BooleanColumnStats, BinaryColumnStats, <init>, ==, FloatColumnStats, ByteColumnStats, IntColumnStats, ColumnStats, LongColumnStats, DoubleColumnStats)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala: Set(gatherStats, nullCount, ColumnStats)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createReader, ReadSupport.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(createReader, ReadSupport)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, partitionBy, equals, rowsBetween, asInstanceOf, synchronized, $isInstanceOf, rangeBetween, notifyAll, isInstanceOf, <init>, WindowSpec, ==, clone, orderBy, withAggregate, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala: Set(partitionBy, rowsBetween, rangeBetween, <init>, WindowSpec, orderBy)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala: Set(equals, asInstanceOf, isInstanceOf, <init>, WindowSpec, ==, withAggregate, toString, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getPackages, addClass, notify, findResource, assertionLock, wait, $asInstanceOf, setDefaultAssertionStatus, findSystemClass, getURLs, equals, desiredAssertionStatus, asInstanceOf, initializeLogIfNecessary, getPermissions, cacheManager, synchronized, $isInstanceOf, clearAssertionStatus, externalCatalog, setSigners, getParent, logTrace, classAssertionStatus, getResource, isTraceEnabled, findLoadedClass, initializeLogIfNecessary$default$2, findLibrary, logName, notifyAll, globalTempViewManager, addURL, getResourceAsStream, isInstanceOf, loadClass, getPackage, NonClosableMutableURLClassLoader, warehousePath, <init>, defineClass, findClass, SharedState, setClassAssertionStatus, ==, clone, sparkContext, statusStore, findResources, $init$, jarClassLoader, resolveClass, isAncestor, toString, logError, !=, setPackageAssertionStatus, getClass, logWarning, getResources, getClassLoadingLock, close, ne, eq, log, definePackage, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(asInstanceOf, cacheManager, isInstanceOf, <init>, SharedState, ==, toString, !=, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(externalCatalog, globalTempViewManager, <init>, SharedState, clone, sparkContext, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(<init>, SharedState, sparkContext, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, cacheManager, isInstanceOf, <init>, SharedState, ==, toString, !=, getClass, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(asInstanceOf, cacheManager, <init>, SharedState, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(addURL, NonClosableMutableURLClassLoader, <init>, SharedState, ==, sparkContext, jarClassLoader, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, SharedState, ==, clone, sparkContext, toString, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(asInstanceOf, cacheManager, isInstanceOf, <init>, SharedState, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, cacheManager, isInstanceOf, <init>, SharedState, ==, sparkContext, toString, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala: Set(asInstanceOf, cacheManager, isInstanceOf, <init>, SharedState, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, unapply, find, simpleString, children, refresh, maxRowsPerPartition, RDDConversions, verboseString, semanticHash, execute, executeCollectIterator, wait, stats, LogicalRDD, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, productToRowRdd, copy$default$5, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, outputObjectType, schemaString, argString, subqueries, executeQuery, asInstanceOf, RDDScanExec, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, computeStats, logName, notifyAll, conf, mapProductIterator, ExternalRDDScanExec, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, rdd, isStreaming, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, constraints, sameResult, foreach, p, jsonFields, resolve, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, newInstance, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, outputObjAttr, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, resolveQuoted, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, ExternalRDD, logInfo, rowToRowRdd.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, RDDConversions, map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, rdd, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, toString, !=, output, ne, transform, eq, rowToRowRdd)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, stats, LogicalRDD, map, asInstanceOf, RDDScanExec, expressions, conf, ExternalRDDScanExec, isInstanceOf, rdd, isStreaming, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, outputObjAttr, ne, eq, ExternalRDD)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, find, simpleString, children, execute, LogicalRDD, resetMetrics, map, treeString, asInstanceOf, expressions, outputSet, transformUp, conf, isInstanceOf, rdd, isStreaming, <init>, outputOrdering, mapChildren, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, resolve, outputPartitioning, copy, toString, !=, collect, output, ne, transform, resolveQuoted, executeToIterator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(unapply, LogicalRDD, map, schemaString, asInstanceOf, synchronized, expressions, conf, isInstanceOf, rdd, isStreaming, <init>, schema, apply, ==, sqlContext, clone, foreach, sparkContext, newInstance, copy, toString, logWarning, ne, ExternalRDD)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, simpleString, children, execute, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, nodeName, expressions, conf, isInstanceOf, rdd, references, <init>, outputOrdering, schema, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, !=, longMetric, logWarning, output, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, generateProcessRow, wait, groupingAttributes, foldRight, takeWhile, $asInstanceOf, generateOutput, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, processRow, min, scanRight, fold, logTrace, nonEmpty, initializeBuffer, initializeAggregateFunctions, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, /:, toIterator, addString, initialize, to, collectFirst, drop, isInstanceOf, filter, expressionAggInitialProjection, SortBasedAggregationIterator, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, flatMap, take, outputForEmptyGroupingKeyWithoutInput, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, aggregateFunctions, toString, copyToArray, length, seq, logError, !=, generateResultProjection, collect, getClass, logWarning, hasDefiniteSize, patch, foldLeft, contains, allImperativeAggregateFunctions, isEmpty, ne, withPartial, allImperativeAggregateFunctionPositions, processCurrentSortedGroup, reversed, hasNext, indexOf, reduceLeft, groupingProjection, eq, sum, log, ##, scanLeft, finalize, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala: Set(map, asInstanceOf, aggregate, nonEmpty, isInstanceOf, SortBasedAggregationIterator, <init>, ++, flatMap, outputForEmptyGroupingKeyWithoutInput, ==, toString, isEmpty, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessor.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getInterval, toIterable, withFilter, toTraversable, notify, withPadding, ColumnarIterator, anyNull, find, span, toBuffer, count, getShort, reduceOption, wait, getUTF8String, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, writer, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, generate, toList, isNullAt, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, getDouble, numFields, reduceLeftOption, getString, synchronized, sliding, partition, aggregate, bind, $isInstanceOf, forall, getStruct, create, mkString, min, scanRight, setBoolean, fold, logTrace, nonEmpty, isTraceEnabled, initializeLogIfNecessary$default$2, getMap, setByte, logName, notifyAll, /:, getByte, toIterator, addString, initialize, to, setInt, collectFirst, drop, genericGet, isInstanceOf, filter, canonicalize, getArray, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, flatMap, getFloat, take, getDecimal, GenerateColumnAccessor, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, getInt, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, MutableUnsafeRow, padTo, $init$, toSeq, next, zipWithIndex, copy, toString, getBoolean, copyToArray, length, seq, setDecimal, logError, !=, get, setShort, collect, getClass, logWarning, setNullAt, genericMutableRowType, update, hasDefiniteSize, setLong, patch, foldLeft, contains, isEmpty, ne, withPartial, newCodeGenContext, reversed, hasNext, indexOf, reduceLeft, eq, sum, log, ##, scanLeft, finalize, getLong, setFloat, hashCode, zipAll, logDebug, getBinary, product, logInfo, setDouble.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(ColumnarIterator, count, reduceOption, zip, map, generate, asInstanceOf, forall, mkString, nonEmpty, initialize, isInstanceOf, filter, <init>, ++, flatMap, GenerateColumnAccessor, ==, foreach, toArray, reduce, zipWithIndex, toString, length, get, isEmpty, ne, hasNext, indexOf, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala: Set(getShort, getUTF8String, writer, map, asInstanceOf, getDouble, getStruct, setBoolean, getMap, setByte, getByte, to, setInt, isInstanceOf, getArray, <init>, getFloat, getDecimal, ==, clone, getInt, MutableUnsafeRow, copy, toString, getBoolean, length, setDecimal, get, setShort, getClass, setNullAt, update, setLong, eq, getLong, setFloat, getBinary, setDouble)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, commit, equals, asInstanceOf, initializeLogIfNecessary, ConsoleWriter, synchronized, $isInstanceOf, createWriterFactory, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, printRows, isInstanceOf, <init>, spark, ==, clone, $init$, toString, logError, !=, isTruncated, numRowsToShow, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, abort, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala: Set(asInstanceOf, ConsoleWriter, isInstanceOf, <init>, spark, ==, toString, isTruncated, numRowsToShow, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ParquetWriteSupport, setSchema, wait, SPARK_ROW_SCHEMA, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, getName, finalizeWrite, isInstanceOf, <init>, makeArrayWriter, ==, clone, $init$, toString, logError, !=, getClass, logWarning, prepareForWrite, ne, init, eq, write, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(ParquetWriteSupport, setSchema, SPARK_ROW_SCHEMA, asInstanceOf, getName, isInstanceOf, <init>, ==, toString, getClass, logWarning, ne, eq, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, copy$default$5, resetMetrics, printSchema, buildSideKeyGenerator, join, map, productArity, verboseStringWithSuffix, equals, treeString, buildSide, schemaString, argString, streamedPlan, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, left, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, buildPlan, logTrace, asCode, canEqual, expressions, canonicalized, createResultProjection, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, leftKeys, rightKeys, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, condition, schema, ShuffledHashJoinExec, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, buildKeys, p, jsonFields, copy$default$7, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, streamSideKeyGenerator, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, joinType, getClass, streamedKeys, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, copy$default$6, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, right, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, buildSide, asInstanceOf, left, expressions, conf, isInstanceOf, leftKeys, rightKeys, <init>, outputOrdering, condition, schema, ShuffledHashJoinExec, apply, ==, p, sparkContext, outputPartitioning, copy, toString, joinType, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, buildSide, asInstanceOf, left, expressions, transformUp, conf, isInstanceOf, leftKeys, rightKeys, <init>, outputOrdering, condition, ShuffledHashJoinExec, apply, flatMap, ==, foreach, outputPartitioning, toString, joinType, ne, requiredChildOrdering, withNewChildren, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF21.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF21.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF21.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF21.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF21.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF21.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF21.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF21)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	decodeToBinary, decodeToDouble, decodeToFloat, Dictionary, decodeToLong, decodeToInt.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala: Set(Dictionary)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala: Set(Dictionary)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/Dictionary.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF9.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF9.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF9.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF9.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF9.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	UDF9, call.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF9.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(UDF9, call)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(UDF9, call)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	customOperatorOptimizationRules, resourceLoader, notify, customCheckRules, listenerManager, optimizer, catalog, wait, analyzer, $asInstanceOf, equals, parentState, asInstanceOf, synchronized, customPostHocResolutionRules, $isInstanceOf, newBuilder, BaseSessionStateBuilder, build, customPlanningStrategies, udfRegistration, notifyAll, conf, isInstanceOf, WithTestConf, extensions, mergeSparkConf, sqlParser, <init>, ==, clone, createQueryExecution, $init$, session, functionRegistry, planner, toString, !=, NewBuilder, getClass, customResolutionRules, streamingQueryManager, overrideConfs, ne, eq, experimentalMethods, createClone, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(resourceLoader, listenerManager, optimizer, catalog, analyzer, parentState, BaseSessionStateBuilder, udfRegistration, conf, sqlParser, <init>, ==, createQueryExecution, session, functionRegistry, planner, !=, NewBuilder, streamingQueryManager, ne, experimentalMethods, createClone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(listenerManager, catalog, asInstanceOf, synchronized, BaseSessionStateBuilder, build, udfRegistration, conf, isInstanceOf, extensions, sqlParser, <init>, ==, clone, session, toString, streamingQueryManager, ne, experimentalMethods)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala: Set(catalog, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(conf, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala: Set(catalog, asInstanceOf, build, conf, isInstanceOf, <init>, ==, session, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala: Set(analyzer, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, asInstanceOf, synchronized, notifyAll, conf, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(catalog, analyzer, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala: Set(conf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala: Set(asInstanceOf, conf, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(listenerManager, optimizer, catalog, analyzer, asInstanceOf, conf, isInstanceOf, sqlParser, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(customOperatorOptimizationRules, resourceLoader, customCheckRules, listenerManager, optimizer, catalog, analyzer, parentState, customPostHocResolutionRules, newBuilder, BaseSessionStateBuilder, build, customPlanningStrategies, udfRegistration, conf, WithTestConf, extensions, mergeSparkConf, sqlParser, <init>, clone, createQueryExecution, session, functionRegistry, planner, NewBuilder, customResolutionRules, streamingQueryManager, overrideConfs, ne, experimentalMethods, createClone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(optimizer, asInstanceOf, conf, isInstanceOf, <init>, ==, planner, experimentalMethods)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(listenerManager, catalog, conf, <init>, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(catalog, asInstanceOf, newBuilder, build, conf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(catalog, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(asInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, session, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(catalog, asInstanceOf, conf, sqlParser, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, getClass, streamingQueryManager, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala: Set(conf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala: Set(catalog, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, session, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala: Set(conf, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(asInstanceOf, synchronized, conf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(conf, isInstanceOf, <init>, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(listenerManager, catalog, asInstanceOf, synchronized, BaseSessionStateBuilder, build, udfRegistration, conf, isInstanceOf, extensions, sqlParser, <init>, ==, clone, session, toString, streamingQueryManager, ne, experimentalMethods)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(conf, isInstanceOf, <init>, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(conf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala: Set(catalog, conf, <init>, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(asInstanceOf, isInstanceOf, sqlParser, <init>, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(analyzer, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, getClass, streamingQueryManager, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(listenerManager, catalog, asInstanceOf, conf, isInstanceOf, sqlParser, <init>, ==, session, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala: Set(equals, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(analyzer, asInstanceOf, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(optimizer, analyzer, asInstanceOf, conf, isInstanceOf, <init>, ==, planner, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, session, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, session, toString, !=, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala: Set(resourceLoader, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala: Set(conf, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(catalog, analyzer, asInstanceOf, isInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala: Set(analyzer, asInstanceOf, isInstanceOf, <init>, ==, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(catalog, asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala: Set(catalog, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, bloomFilter, equals, approxQuantile, asInstanceOf, synchronized, $isInstanceOf, notifyAll, DataFrameStatFunctions, isInstanceOf, crosstab, cov, <init>, ==, clone, toString, !=, sampleBy, getClass, freqItems, countMinSketch, ne, eq, ##, finalize, hashCode, corr.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(asInstanceOf, DataFrameStatFunctions, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	StateStoreOps.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, copy$default$5, resetMetrics, printSchema, map, productArity, resultExpressions, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, requiredChildDistributionExpressions, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, SortAggregateExec, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, groupingExpressions, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, initialInputBufferOffset, p, jsonFields, copy$default$7, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, aggregateExpressions, innerChildren, aggregateAttributes, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, copy$default$6, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala: Set(children, map, resultExpressions, requiredChildDistributionExpressions, asInstanceOf, SortAggregateExec, expressions, conf, isInstanceOf, child, <init>, groupingExpressions, apply, flatMap, ==, sqlContext, initialInputBufferOffset, toString, aggregateExpressions, aggregateAttributes, transformDown, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, generateProcessRow, wait, groupingAttributes, foldRight, takeWhile, $asInstanceOf, generateOutput, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, processRow, min, scanRight, fold, logTrace, nonEmpty, initializeBuffer, initializeAggregateFunctions, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, expressionAggInitialProjection, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, flatMap, take, outputForEmptyGroupingKeyWithoutInput, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, destructiveIterator, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, aggregateFunctions, toString, copyToArray, length, seq, logError, !=, generateResultProjection, collect, getClass, logWarning, hasDefiniteSize, ObjectAggregationIterator, addInput, patch, foldLeft, contains, allImperativeAggregateFunctions, isEmpty, ne, SortBasedAggregator, withPartial, allImperativeAggregateFunctionPositions, reversed, hasNext, indexOf, reduceLeft, groupingProjection, eq, sum, log, ##, scanLeft, finalize, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala: Set(map, asInstanceOf, aggregate, nonEmpty, isInstanceOf, <init>, ++, flatMap, outputForEmptyGroupingKeyWithoutInput, ==, exists, toString, ObjectAggregationIterator, isEmpty, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonInferSchema.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonInferSchema.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonInferSchema.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonInferSchema.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonInferSchema.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, infer, equals, asInstanceOf, synchronized, $isInstanceOf, JsonInferSchema, compatibleType, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(infer, JsonInferSchema, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, OrcFilters, clone, toString, createFilter, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(asInstanceOf, isInstanceOf, ==, OrcFilters, createFilter, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, awaitInitialization, curried, name, wait, currentBatchId, copy$default$2, $asInstanceOf, outputMode, explain, state, productArity, equals, inputRows, reportTimeTaken, asInstanceOf, initializeLogIfNecessary, committedOffsets, finishTrigger, streamDeathCause, synchronized, queryExecutionThread, $isInstanceOf, exception, streamMetadata, streamMetrics, tupled, awaitProgressLockCondition, updateStatusMessage, logTrace, canEqual, ExecutionStats, lastExecution, isTraceEnabled, initializeLogIfNecessary$default$2, triggerClock, offsetSeqMetadata, productPrefix, stop, logName, notifyAll, pollingDelayMs, postEvent, isInstanceOf, startTrigger, awaitOffset, sink, runActivatedStream, <init>, offsetLog, resolvedCheckpointRoot, id, eventTimeStats, apply, lastProgress, noNewData, processAllAvailable, stateOperators, ==, clone, status, minLogEntriesToMaintain, sparkSession, prettyIdString, $init$, getBatchDescriptionString, isActive, recentProgress, availableOffsets, copy$default$3, copy, toString, awaitTermination, logError, !=, commitLog, MicroBatchExecution, getClass, logWarning, stopSources, copy$default$1, start, uniqueSources, checkpointFile, ne, sources, newData, runId, watermarkMsMap, eq, productIterator, log, logicalPlan, ##, finalize, productElement, hashCode, logDebug, logInfo, explainInternal, currentStatus, trigger, awaitProgressLock.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, outputMode, state, asInstanceOf, synchronized, exception, triggerClock, notifyAll, isInstanceOf, sink, <init>, id, apply, ==, sparkSession, toString, !=, MicroBatchExecution, logWarning, start, ne, sources, runId, trigger)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, close, ne, OrcOutputWriter, eq, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, getClass, close, OrcOutputWriter, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDF.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDF.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDF.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDF.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDF.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	genCode, notify, treeString$default$2, find, nullable, sql, simpleString, children, name, verboseString, semanticHash, flatArguments, wait, copy$default$2, $asInstanceOf, semanticEquals, numberedTreeString, copy$default$5, map, productArity, verboseStringWithSuffix, equals, treeString, argString, asInstanceOf, generateTreeString, childrenResolved, synchronized, PythonUDF, generateTreeString$default$6, nodeName, $isInstanceOf, asCode, canEqual, canonicalized, copy$default$4, makeCopy, transformUp, productPrefix, notifyAll, mapProductIterator, prettyName, collectFirst, otherCopyArgs, isInstanceOf, eval, stringArgs, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, prettyJson, apply, flatMap, resolved, ==, fastEquals, origin, checkInputDataTypes, clone, foreach, p, jsonFields, $init$, evalType, copy$default$3, func, copy, udfDeterministic, toString, !=, deterministic, doGenCode, innerChildren, collect, getClass, copy$default$1, dataType, foldable, transformDown, copy$default$6, ne, eval$default$1, transform, withNewChildren, eq, productIterator, toJSON, ##, containsChild, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala: Set(find, sql, children, semanticEquals, map, asInstanceOf, PythonUDF, isInstanceOf, <init>, apply, ==, func, copy, !=, dataType, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(nullable, sql, children, name, map, asInstanceOf, PythonUDF, isInstanceOf, <init>, apply, ==, func, udfDeterministic, toString, dataType, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala: Set(sql, map, asInstanceOf, PythonUDF, isInstanceOf, <init>, apply, ==, toString, dataType, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala: Set(nullable, sql, map, asInstanceOf, PythonUDF, isInstanceOf, <init>, apply, flatMap, ==, toString, dataType, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala: Set(sql, simpleString, name, map, asInstanceOf, PythonUDF, isInstanceOf, <init>, apply, ==, evalType, toString, collect, getClass, dataType, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, sql, children, semanticEquals, map, asInstanceOf, PythonUDF, transformUp, isInstanceOf, references, <init>, apply, flatMap, ==, foreach, evalType, !=, deterministic, dataType, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala: Set(sql, name, map, asInstanceOf, PythonUDF, isInstanceOf, <init>, apply, ==, func, udfDeterministic, toString, dataType, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala: Set(sql, map, asInstanceOf, PythonUDF, isInstanceOf, <init>, apply, flatMap, ==, func, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF0.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF0.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF0.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF0.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF0.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	UDF0, call.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF0.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(UDF0, call)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(UDF0, call)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, commit, equals, removeByValueCondition, asInstanceOf, initializeLogIfNecessary, synchronized, allStateStoreNames, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, removeByKeyCondition, logName, notifyAll, joinSide, isInstanceOf, SymmetricHashJoinStateManager, <init>, ==, clone, $init$, toString, metrics, logError, !=, get, getClass, logWarning, ne, eq, log, abortIfNeeded, ##, finalize, hashCode, logDebug, logInfo, append.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(commit, removeByValueCondition, asInstanceOf, allStateStoreNames, removeByKeyCondition, joinSide, isInstanceOf, SymmetricHashJoinStateManager, <init>, ==, toString, metrics, !=, get, getClass, ne, eq, append)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, readSchema, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, requestedColumnIds, ==, clone, OrcUtils, $init$, toString, logError, !=, getClass, logWarning, ne, extensionsForCompressionCodecNames, eq, listOrcFiles, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(readSchema, asInstanceOf, isInstanceOf, requestedColumnIds, ==, OrcUtils, !=, getClass, extensionsForCompressionCodecNames, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	StreamWriter, createWriterFactory, abort, commit.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala: Set(StreamWriter, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala: Set(StreamWriter, createWriterFactory, abort, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(StreamWriter, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala: Set(StreamWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(StreamWriter, createWriterFactory, abort, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(StreamWriter, commit)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(StreamWriter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala: Set(StreamWriter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala: Set(StreamWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, extraPlanningStrategies, canEvaluateWithinJoin, equals, canEvaluate, StreamingRelationStrategy, asInstanceOf, pruneFilterProject, initializeLogIfNecessary, synchronized, splitDisjunctivePredicates, FlatMapGroupsWithStateStrategy, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, splitConjunctivePredicates, JoinSelection, logName, notifyAll, prunePlans, conf, isInstanceOf, StatefulAggregationStrategy, StreamingDeduplicationStrategy, SpecialLimits, <init>, apply, ==, clone, Aggregation, planLater, sparkContext, $init$, toString, singleRowRdd, strategies, SparkPlanner, logError, !=, getClass, logWarning, replaceAlias, BasicOperators, ne, InMemoryScans, numPartitions, eq, experimentalMethods, log, plan, ##, finalize, hashCode, logDebug, StreamingJoinStrategy, logInfo, collectPlaceholders.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(conf, <init>, apply, ==, sparkContext, SparkPlanner, !=, ne, experimentalMethods, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, conf, isInstanceOf, <init>, apply, ==, clone, sparkContext, toString, logWarning, ne, numPartitions, experimentalMethods, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(StreamingRelationStrategy, asInstanceOf, pruneFilterProject, FlatMapGroupsWithStateStrategy, JoinSelection, conf, isInstanceOf, StatefulAggregationStrategy, StreamingDeduplicationStrategy, SpecialLimits, <init>, apply, ==, Aggregation, planLater, sparkContext, toString, singleRowRdd, SparkPlanner, BasicOperators, ne, InMemoryScans, numPartitions, eq, plan, StreamingJoinStrategy)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(extraPlanningStrategies, conf, <init>, apply, clone, sparkContext, SparkPlanner, ne, experimentalMethods, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(extraPlanningStrategies, StreamingRelationStrategy, asInstanceOf, FlatMapGroupsWithStateStrategy, conf, isInstanceOf, StatefulAggregationStrategy, StreamingDeduplicationStrategy, <init>, apply, ==, sparkContext, strategies, SparkPlanner, experimentalMethods, plan, StreamingJoinStrategy, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(conf, <init>, apply, ==, sparkContext, SparkPlanner, !=, ne, experimentalMethods, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, SparkPlanner, !=, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(conf, <init>, apply, sparkContext, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, ne, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, apply, ==, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, asInstanceOf, synchronized, notifyAll, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(asInstanceOf, pruneFilterProject, conf, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala: Set(conf, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala: Set(asInstanceOf, conf, <init>, apply, ==, toString, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, ne, numPartitions, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, sparkContext, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(extraPlanningStrategies, conf, <init>, apply, clone, sparkContext, SparkPlanner, ne, experimentalMethods, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(extraPlanningStrategies, StreamingRelationStrategy, asInstanceOf, FlatMapGroupsWithStateStrategy, conf, isInstanceOf, StatefulAggregationStrategy, StreamingDeduplicationStrategy, <init>, apply, ==, sparkContext, strategies, SparkPlanner, experimentalMethods, plan, StreamingJoinStrategy, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(conf, <init>, apply, sparkContext, toString, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, getClass, ne, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, logError, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(asInstanceOf, <init>, apply, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(asInstanceOf, conf, <init>, apply, ==, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, getClass, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala: Set(logTrace, conf, <init>, apply, ==, toString, !=, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, logError, !=, ne, eq, plan, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, getClass, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala: Set(asInstanceOf, logTrace, conf, isInstanceOf, <init>, apply, ==, toString, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala: Set(conf, isInstanceOf, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(asInstanceOf, synchronized, conf, isInstanceOf, <init>, apply, ==, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(conf, isInstanceOf, <init>, apply, ==, sparkContext, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, conf, isInstanceOf, <init>, apply, ==, clone, sparkContext, toString, logWarning, ne, numPartitions, experimentalMethods, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(conf, isInstanceOf, <init>, apply, ==, sparkContext, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(conf, <init>, apply, ==, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala: Set(conf, <init>, apply, toString, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala: Set(equals, asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, logWarning, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, plan, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, SparkPlanner, !=, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, getClass, logWarning, ne, eq, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(asInstanceOf, logTrace, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, getClass, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, ne, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(<init>, apply, sparkContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, logWarning, ne, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, sparkContext, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala: Set(conf, <init>, apply, ==, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, logWarning, numPartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, logWarning, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, sparkContext, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, apply, ==, sparkContext, toString, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getTimeoutTimestamp, wait, $asInstanceOf, equals, setTimeoutDuration, asInstanceOf, synchronized, hasTimedOut, $isInstanceOf, getOption, notifyAll, isInstanceOf, setTimeoutTimestamp, getCurrentProcessingTimeMs, remove, ==, createForBatch, clone, exists, createForStreaming, getCurrentWatermarkMs, hasUpdated, toString, !=, get, getClass, GroupStateImpl, update, ne, hasRemoved, eq, NO_TIMESTAMP, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(getTimeoutTimestamp, asInstanceOf, hasTimedOut, isInstanceOf, setTimeoutTimestamp, remove, ==, exists, createForStreaming, hasUpdated, toString, !=, get, GroupStateImpl, ne, hasRemoved, eq, NO_TIMESTAMP)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala: Set(asInstanceOf, isInstanceOf, ==, createForBatch, exists, toString, get, GroupStateImpl, update, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggregationIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, generateProcessRow, wait, groupingAttributes, foldRight, takeWhile, $asInstanceOf, generateOutput, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, processRow, min, scanRight, fold, logTrace, nonEmpty, initializeBuffer, initializeAggregateFunctions, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, expressionAggInitialProjection, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, aggregateFunctions, AggregationIterator, toString, copyToArray, length, seq, logError, !=, generateResultProjection, collect, getClass, logWarning, hasDefiniteSize, patch, foldLeft, contains, allImperativeAggregateFunctions, isEmpty, ne, withPartial, allImperativeAggregateFunctionPositions, reversed, hasNext, indexOf, reduceLeft, groupingProjection, eq, sum, log, ##, scanLeft, finalize, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala: Set(generateProcessRow, groupingAttributes, generateOutput, size, map, asInstanceOf, aggregate, processRow, initializeAggregateFunctions, isInstanceOf, expressionAggInitialProjection, <init>, flatMap, ==, foreach, next, aggregateFunctions, AggregationIterator, !=, collect, isEmpty, ne, hasNext, groupingProjection, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala: Set(generateProcessRow, groupingAttributes, generateOutput, map, equals, asInstanceOf, aggregate, processRow, nonEmpty, initializeAggregateFunctions, isInstanceOf, expressionAggInitialProjection, <init>, max, flatMap, ==, foreach, next, aggregateFunctions, AggregationIterator, length, !=, generateResultProjection, collect, contains, isEmpty, ne, hasNext, groupingProjection, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala: Set(generateOutput, map, aggregate, forall, processRow, initializeBuffer, <init>, flatMap, ==, next, aggregateFunctions, AggregationIterator, length, hasNext, groupingProjection)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala: Set(map, asInstanceOf, aggregate, nonEmpty, isInstanceOf, <init>, ++, flatMap, ==, toString, isEmpty, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala: Set(map, asInstanceOf, aggregate, nonEmpty, isInstanceOf, <init>, ++, flatMap, ==, exists, toString, isEmpty, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(groupingAttributes, map, equals, asInstanceOf, aggregate, forall, mkString, nonEmpty, isInstanceOf, filter, <init>, max, ++, flatMap, ==, exists, next, zipWithIndex, toString, length, !=, logWarning, contains, isEmpty, ne, hasNext, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BucketingUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BucketingUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BucketingUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BucketingUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BucketingUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, BucketingUtils, isInstanceOf, ==, clone, bucketIdToString, toString, !=, getClass, getBucketId, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(asInstanceOf, BucketingUtils, isInstanceOf, ==, toString, getClass, getBucketId, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(asInstanceOf, BucketingUtils, isInstanceOf, ==, bucketIdToString, toString, !=, getBucketId, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamOptions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	FileStreamOptions, notify, wait, $asInstanceOf, equals, maxFilesPerTrigger, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, optionMapWithoutPath, logTrace, latestFirst, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, maxFileAgeMs, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, fileNameOnly, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(FileStreamOptions, maxFilesPerTrigger, asInstanceOf, synchronized, optionMapWithoutPath, logTrace, latestFirst, isInstanceOf, maxFileAgeMs, <init>, ==, toString, logWarning, ne, fileNameOnly, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, curried, generateRowIterator, name, wait, copy$default$2, $asInstanceOf, productArity, equals, generate, asInstanceOf, Buffer, bufferValues, synchronized, $isInstanceOf, genComputeHash, initializeAggregateHashMap, tupled, groupingKeySignature, canEqual, generateEquals, productPrefix, notifyAll, isInstanceOf, generateClose, <init>, apply, groupingKeys, buffVars, VectorizedHashMapGenerator, generateFindOrInsert, ==, clone, $init$, copy, toString, !=, getClass, copy$default$1, dataType, generateHashFunction, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(equals, generate, asInstanceOf, isInstanceOf, <init>, apply, VectorizedHashMapGenerator, ==, copy, toString, !=, dataType, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/test/ExamplePointUDT.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/test/ExamplePointUDT.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/test/ExamplePointUDT.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/test/ExamplePointUDT.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/test/ExamplePointUDT.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	acceptsType, notify, unapply, sql, simpleString, x, wait, defaultConcreteType, $asInstanceOf, equals, json, y, asInstanceOf, synchronized, $isInstanceOf, deserialize, typeName, existsRecursively, notifyAll, ExamplePoint, isInstanceOf, sameType, <init>, asNullable, prettyJson, userClass, ==, clone, pyUDT, catalogString, defaultSize, toString, !=, sqlType, serializedPyClass, getClass, ne, serialize, jsonValue, eq, ##, finalize, hashCode, ExamplePointUDT.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	stringWithStats, notify, simpleString, IncrementalExecution, wait, currentBatchId, $asInstanceOf, outputMode, state, equals, analyzed, asInstanceOf, initializeLogIfNecessary, assertAnalyzed, assertSupported, synchronized, $isInstanceOf, numStateStores, codegen, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, offsetSeqMetadata, codegenToSeq, withCachedData, logName, notifyAll, debug, isInstanceOf, <init>, prepareForExecution, stringOrError, toRdd, ==, logical, sparkPlan, clone, checkpointLocation, sparkSession, $init$, optimizedPlan, planner, toString, logError, !=, getClass, logWarning, ne, runId, hiveResultString, eq, executedPlan, log, preparations, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(IncrementalExecution, currentBatchId, outputMode, state, asInstanceOf, synchronized, offsetSeqMetadata, isInstanceOf, <init>, toRdd, ==, logical, sparkSession, toString, !=, ne, runId, executedPlan, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(IncrementalExecution, currentBatchId, outputMode, state, asInstanceOf, offsetSeqMetadata, isInstanceOf, <init>, ==, logical, sparkSession, toString, logError, !=, logWarning, ne, runId, eq, executedPlan, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(stringWithStats, simpleString, IncrementalExecution, asInstanceOf, codegen, debug, isInstanceOf, <init>, ==, logical, sparkSession, toString, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(IncrementalExecution, currentBatchId, outputMode, analyzed, asInstanceOf, offsetSeqMetadata, isInstanceOf, <init>, ==, logical, sparkSession, toString, !=, logWarning, ne, runId, eq, executedPlan, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, BasicColumnBuilder, ByteColumnBuilder, wait, ColumnBuilder, apply$default$4, MAX_BATCH_SIZE_IN_BYTE, $asInstanceOf, columnName, equals, ArrayColumnBuilder, NativeColumnBuilder, StringColumnBuilder, asInstanceOf, initializeLogIfNecessary, columnStats, synchronized, isWorthCompressing, $isInstanceOf, apply$default$3, IntColumnBuilder, build, buffer, logTrace, BooleanColumnBuilder, nulls, isTraceEnabled, StructColumnBuilder, initializeLogIfNecessary$default$2, columnType, initialize$default$3, logName, notifyAll, schemes, initialize, isInstanceOf, ShortColumnBuilder, DecimalColumnBuilder, CompactDecimalColumnBuilder, <init>, FloatColumnBuilder, apply$default$2, apply, ==, NullColumnBuilder, clone, compressionEncoders, $init$, BinaryColumnBuilder, appendFrom, ensureFreeSpace, toString, initialize$default$2, logError, !=, DoubleColumnBuilder, getClass, logWarning, buildNonNulls, ComplexColumnBuilder, nullCount, LongColumnBuilder, ne, MapColumnBuilder, DEFAULT_INITIAL_BUFFER_SIZE, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(ColumnBuilder, columnName, NativeColumnBuilder, isWorthCompressing, nulls, columnType, schemes, <init>, apply, ==, compressionEncoders, buildNonNulls, nullCount, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(BasicColumnBuilder, ByteColumnBuilder, ColumnBuilder, MAX_BATCH_SIZE_IN_BYTE, columnName, ArrayColumnBuilder, NativeColumnBuilder, StringColumnBuilder, asInstanceOf, columnStats, IntColumnBuilder, buffer, BooleanColumnBuilder, StructColumnBuilder, columnType, initialize, isInstanceOf, ShortColumnBuilder, DecimalColumnBuilder, CompactDecimalColumnBuilder, <init>, FloatColumnBuilder, apply, ==, NullColumnBuilder, BinaryColumnBuilder, ensureFreeSpace, DoubleColumnBuilder, ComplexColumnBuilder, LongColumnBuilder, MapColumnBuilder, DEFAULT_INITIAL_BUFFER_SIZE)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala: Set(ColumnBuilder, MAX_BATCH_SIZE_IN_BYTE, asInstanceOf, columnStats, build, isInstanceOf, <init>, apply, ==, appendFrom, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(ColumnBuilder, columnName, NativeColumnBuilder, isWorthCompressing, nulls, columnType, schemes, <init>, apply, ==, compressionEncoders, buildNonNulls, nullCount, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala: Set(ColumnBuilder, columnName, columnStats, buffer, nulls, ensureFreeSpace, nullCount)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(BasicColumnBuilder, ByteColumnBuilder, ColumnBuilder, MAX_BATCH_SIZE_IN_BYTE, columnName, ArrayColumnBuilder, NativeColumnBuilder, StringColumnBuilder, asInstanceOf, columnStats, IntColumnBuilder, buffer, BooleanColumnBuilder, StructColumnBuilder, columnType, initialize, isInstanceOf, ShortColumnBuilder, DecimalColumnBuilder, CompactDecimalColumnBuilder, <init>, FloatColumnBuilder, apply, ==, NullColumnBuilder, BinaryColumnBuilder, ensureFreeSpace, DoubleColumnBuilder, ComplexColumnBuilder, LongColumnBuilder, MapColumnBuilder, DEFAULT_INITIAL_BUFFER_SIZE)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	dataSchema, partitionSchema, notify, HadoopFsRelation, wait, copy$default$2, $asInstanceOf, location, copy$default$5, productArity, overlappedPartCols, equals, inputFiles, asInstanceOf, unhandledFilters, synchronized, $isInstanceOf, canEqual, copy$default$4, productPrefix, notifyAll, isInstanceOf, options, partitionSchemaOption, <init>, schema, ==, bucketSpec, sqlContext, clone, sparkSession, sizeInBytes, $init$, copy$default$3, copy, toString, !=, getClass, copy$default$1, copy$default$6, ne, eq, productIterator, ##, finalize, productElement, hashCode, fileFormat, needConversion.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(dataSchema, partitionSchema, HadoopFsRelation, location, asInstanceOf, unhandledFilters, isInstanceOf, options, <init>, schema, ==, bucketSpec, sparkSession, toString, !=, ne, eq, fileFormat, needConversion)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala: Set(partitionSchema, HadoopFsRelation, location, asInstanceOf, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(partitionSchema, HadoopFsRelation, location, asInstanceOf, isInstanceOf, <init>, schema, ==, bucketSpec, sparkSession, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(dataSchema, partitionSchema, HadoopFsRelation, location, asInstanceOf, isInstanceOf, options, partitionSchemaOption, <init>, schema, ==, bucketSpec, sqlContext, sparkSession, toString, getClass, ne, eq, fileFormat)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(partitionSchema, HadoopFsRelation, overlappedPartCols, asInstanceOf, isInstanceOf, <init>, schema, ==, bucketSpec, sqlContext, sparkSession, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(dataSchema, partitionSchema, HadoopFsRelation, asInstanceOf, isInstanceOf, <init>, ==, sparkSession)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(dataSchema, partitionSchema, HadoopFsRelation, location, asInstanceOf, isInstanceOf, options, <init>, schema, ==, bucketSpec, sqlContext, sparkSession, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala: Set(HadoopFsRelation, location, asInstanceOf, isInstanceOf, <init>, schema, ==, sizeInBytes, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(HadoopFsRelation, location, asInstanceOf, isInstanceOf, <init>, ==, sparkSession, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(dataSchema, HadoopFsRelation, location, asInstanceOf, isInstanceOf, <init>, schema, ==, sqlContext, sparkSession, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, HadoopFsRelation, location, asInstanceOf, isInstanceOf, partitionSchemaOption, <init>, sparkSession, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	genCode, notify, fieldIndex, treeString$default$2, anyNull, find, nullable, sql, simpleString, children, getShort, verboseString, semanticHash, flatArguments, wait, copy$default$2, $asInstanceOf, semanticEquals, numberedTreeString, size, MutableAggregationBufferImpl, aggBufferAttributes, map, productArity, verboseStringWithSuffix, equals, treeString, inputTypes, argString, aggBufferSchema, isNullAt, withNewMutableAggBufferOffset, asInstanceOf, initializeLogIfNecessary, generateTreeString, getDouble, childrenResolved, getString, synchronized, generateTreeString$default$6, nodeName, $isInstanceOf, getStruct, mkString, BufferSetterGetterUtils, getAs, logTrace, asCode, canEqual, getTimestamp, canonicalized, getDate, copy$default$4, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, getMap, inputAggBufferOffset, logName, notifyAll, mapProductIterator, getByte, initialize, prettyName, collectFirst, otherCopyArgs, isInstanceOf, createSetters, eval, stringArgs, underlyingInputBuffer, ScalaUDAF, collectLeaves, references, <init>, merge, generateTreeString$default$5, foreachUp, mapChildren, schema, InputAggregationBuffer, prettyJson, apply, flatMap, getFloat, getDecimal, resolved, ==, fastEquals, origin, checkInputDataTypes, clone, foreach, getInt, p, jsonFields, getValuesMap, $init$, toSeq, withNewInputAggBufferOffset, toAggregateExpression, getList, udaf, copy$default$3, copy, toString, getBoolean, length, getSeq, defaultResult, logError, !=, get, deterministic, doGenCode, innerChildren, collect, getJavaMap, getClass, logWarning, copy$default$1, update, toAggString, dataType, foldable, transformDown, inputAggBufferAttributes, mutableAggBufferOffset, ne, eval$default$1, transform, withNewChildren, eq, productIterator, toJSON, log, underlyingBuffer, ##, containsChild, finalize, getLong, createGetters, getBufferOffset, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(nullable, sql, children, map, inputTypes, asInstanceOf, isInstanceOf, ScalaUDAF, <init>, apply, ==, toSeq, udaf, toString, length, dataType, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala: Set(sql, map, ScalaUDAF, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF17.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF17.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF17.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF17.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF17.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF17.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF17.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF17)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, unapply, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, wait, stats, $asInstanceOf, numberedTreeString, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, kv, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, VariableName, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy, inputSet, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, SetCommand, ne, transform, withNewChildren, ResetCommand, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, SetCommand, ne, ResetCommand)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	injectOptimizerRule, notify, SparkSessionExtensions, wait, buildOptimizerRules, $asInstanceOf, buildCheckRules, injectResolutionRule, equals, injectParser, asInstanceOf, synchronized, $isInstanceOf, CheckRuleBuilder, buildParser, buildPostHocResolutionRules, notifyAll, injectCheckRule, StrategyBuilder, isInstanceOf, buildPlannerStrategies, <init>, RuleBuilder, ==, clone, injectPostHocResolutionRule, toString, !=, getClass, ParserBuilder, ne, eq, buildResolutionRules, ##, finalize, injectPlannerStrategy, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(SparkSessionExtensions, asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(SparkSessionExtensions, buildOptimizerRules, buildCheckRules, buildParser, buildPostHocResolutionRules, buildPlannerStrategies, <init>, clone, ne, buildResolutionRules)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	parsePathFragment, notify, unapply, curried, PartitioningUtils, wait, copy$default$2, $asInstanceOf, path, resolvePartitions, emptySpec, productArity, equals, PartitionValues, asInstanceOf, synchronized, columnNames, $isInstanceOf, tupled, PartitionSpec, canEqual, productPrefix, notifyAll, partitionColumnsSchema, isInstanceOf, PartitionPath, parsePartition, <init>, listConflictingPartitionColumns, normalizePartitionSpec, partitionColumns, parsePathFragmentAsSeq, apply, ==, clone, $init$, literals, inferPartitionColumnValue, validatePartitionColumn, copy, values, toString, !=, partitions, getClass, copy$default$1, getPathFragment, ne, parsePartitions, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(unapply, PartitioningUtils, path, asInstanceOf, isInstanceOf, <init>, normalizePartitionSpec, apply, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(PartitioningUtils, asInstanceOf, isInstanceOf, <init>, normalizePartitionSpec, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(parsePathFragment, PartitioningUtils, path, asInstanceOf, isInstanceOf, <init>, partitionColumns, apply, ==, copy, toString, !=, partitions, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(PartitioningUtils, asInstanceOf, isInstanceOf, <init>, normalizePartitionSpec, partitionColumns, apply, ==, toString, partitions, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala: Set(PartitioningUtils, path, asInstanceOf, PartitionSpec, isInstanceOf, PartitionPath, <init>, partitionColumns, apply, ==, values, toString, partitions, ne, parsePartitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(path, asInstanceOf, PartitionSpec, isInstanceOf, PartitionPath, <init>, partitionColumns, apply, ==, partitions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(parsePathFragment, PartitioningUtils, path, asInstanceOf, isInstanceOf, <init>, partitionColumns, apply, ==, toString, !=, partitions, getPathFragment, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(PartitioningUtils, path, asInstanceOf, isInstanceOf, <init>, partitionColumns, apply, ==, validatePartitionColumn, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(path, asInstanceOf, PartitionSpec, isInstanceOf, <init>, apply, ==, toString, !=, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(unapply, PartitioningUtils, path, asInstanceOf, isInstanceOf, <init>, normalizePartitionSpec, apply, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(path, PartitionSpec, <init>, apply, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, printSchema, map, logicalRelation, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, query, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy$default$3, copy, inputSet, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, overwrite, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, InsertIntoDataSourceCommand, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, query, expressions, conf, collectFirst, isInstanceOf, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, toString, !=, output, overwrite, ne, transform, InsertIntoDataSourceCommand, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, evaluate, udfs, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, ArrowEvalPythonExec, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, udfs, ArrowEvalPythonExec, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	visitMultiInsertQuery, visitShowTables, visitUnsupportedHiveNativeCommands, notify, visitCast, visitNamedExpression, visitArithmeticUnary, astBuilder, parseExpression, visitRepairTable, visitSubscript, visitSampleByBytes, visitInterval, isExplainableStatement, createStructType, visitTablePropertyList, visitSample, visitStatementDefault, visitShowPartitions, visitGroupingSet, visitManageResource, visitExists, visitIdentifier, visitPartitionSpecLocation, visitRowConstructor, visitIdentifierList, wait, visitSetOperation, visitNumericLiteral, $asInstanceOf, visitRenameTable, visitValueExpressionDefault, visitPrimitiveDataType, visitTinyIntLiteral, visitJoinType, visitClearCache, visitHintStatement, visitIntervalField, visitResource, visitStruct, visitIntervalValue, visitSampleByBucket, visitChildren, InsertDirParams, visitSetTableProperties, SparkSqlParser, visitOrderedIdentifier, visitAddTableColumns, visitTableName, equals, createSchema, visitCreateView, visitDescribeFuncName, visitIntervalLiteral, visitSampleByRows, visitDecimalLiteral, visitNestedConstantList, visitDropFunction, visitDescribeTable, visitParenthesizedExpression, visitComplexDataType, parseFunctionIdentifier, visitAddTablePartition, InsertTableParams, parseDataType, visitRecoverPartitions, asInstanceOf, visitErrorNode, initializeLogIfNecessary, visitIdentifierSeq, visitFunctionIdentifier, visitInlineTableDefault2, visitDropDatabase, visitDereference, visitIdentifierComment, visitOrderedIdentifierList, visitAnalyze, visitQuery, visitTablePropertyValue, visitComparison, visitSetConfiguration, visitQualifiedName, visitExplain, synchronized, visitUncacheTable, visitStorageHandler, visitShowDatabases, parseTableSchema, $isInstanceOf, visitBooleanLiteral, visitTable, visitRowFormatDelimited, aggregateResult, withRepartitionByExpression, visitCreateHiveTable, visitBucketSpec, visitDescribeFunction, visitLocationSpec, visitTableFileFormat, visitJoinCriteria, visitWindows, visitInlineTable, logTrace, visitTypeConstructor, visitUnsetTableProperties, visitConstantList, isTraceEnabled, visitBooleanDefault, initializeLogIfNecessary$default$2, visitBigDecimalLiteral, visitLast, visitQuotedIdentifier, visitArithmeticBinary, logName, notifyAll, visitChangeColumn, visitCreateTableHeader, visitQuotedIdentifierAlternative, visitFailNativeCommand, visitTableValuedFunction, visitLogicalNot, visitWindowRef, isInstanceOf, visitBooleanValue, visitPredicateOperator, visitNullLiteral, visitQuerySpecification, visitComplexColType, visitAggregation, typedVisit, visitUnquotedIdentifier, visitShowCreateTable, visitQueryOrganization, visitComplexColTypeList, visitResetConfiguration, visitColumnReference, visitSetQuantifier, visitDescribeColName, visitShowFunctions, visitRefreshResource, <init>, visitCreateTableLike, visitNamedWindow, visitIntegerLiteral, visitTableProvider, parseTableIdentifier, visitInsertOverwriteDir, visitBigIntLiteral, visitRefreshTable, visitCtes, visitAlterViewQuery, visitSingleInsertQuery, visitPartitionSpec, visitLogicalBinary, visitSubqueryExpression, visitConstantDefault, expression, visitCreateTempViewUsing, shouldVisitNextChild, visitSetTableLocation, parse, visitSingleTableSchema, visitSortItem, visitInlineTableDefault1, ==, visitPredicate, visit, visitSingleExpression, clone, visitSmallIntLiteral, visitCreateDatabase, visitInsertIntoTable, $init$, visitTerminal, visitFromClause, visitSearchedCase, visitIdentifierCommentList, visitCreateFunction, visitLoadData, visitSetTableSerDe, visitSingleDataType, visitAliasedQuery, TableHeader, toString, visitDropTable, visitFrameBound, visitRelation, visitSkewSpec, defaultResult, visitFunctionName, logError, !=, visitStar, visitSingleTableIdentifier, visitSingleStatement, visitShowTblProperties, visitStringLiteral, visitShowColumns, visitInsertOverwriteTable, SparkSqlAstBuilder, getClass, visitUse, visitGenericFileFormat, logWarning, visitFirst, visitPredicated, visitQueryTermDefault, visitCacheTable, visitShowTable, visitTruncateTable, visitColTypeList, visitHint, visitAliasedRelation, visitColType, visitComparisonOperator, visitRowFormatSerde, parsePlan, visitSetDatabaseProperties, visitSubquery, visitFunctionCall, visitNamedExpressionSeq, visitNamedQuery, visitTableAlias, visitTablePropertyKey, visitMultiInsertQueryBody, visitTableProperty, ne, withScriptIOSchema, visitQueryPrimaryDefault, visitPartitionVal, visitWindowDef, visitRenameTablePartition, visitStringConstant, visitWindowFrame, visitJoinRelation, visitDescribeDatabase, visitCreateTable, visitColPosition, visitWhenClause, eq, visitSimpleCase, visitCreateFileFormat, visitTableIdentifier, visitSampleByPercentile, log, visitSingleFunctionIdentifier, visitPosition, plan, visitDropTablePartitions, ##, visitDoubleLiteral, finalize, hashCode, logDebug, visitArithmeticOperator, visitExpression, visitNonOptionalPartitionSpec, logInfo, visitNonReserved, visitFunctionTable, visitLateralView, visitInsertOverwriteHiveDir.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(parseExpression, SparkSqlParser, asInstanceOf, isInstanceOf, <init>, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(SparkSqlParser, <init>, clone, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	BaseStreamingSink.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSink.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(BaseStreamingSink)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, RunningExecutionTable, wait, $asInstanceOf, equals, prefix, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, row, AllExecutionsPage, isTraceEnabled, initializeLogIfNecessary$default$2, ExecutionTable, logName, FailedExecutionTable, notifyAll, isInstanceOf, CompletedExecutionTable, <init>, ==, toNodeSeq, clone, $init$, renderJson, header, toString, logError, !=, getClass, logWarning, render, baseHeader, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala: Set(AllExecutionsPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	defaultFormats.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ForeachWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ForeachWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ForeachWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ForeachWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ForeachWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, process, asInstanceOf, synchronized, $isInstanceOf, ForeachWriter, open, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, close, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(asInstanceOf, ForeachWriter, isInstanceOf, <init>, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala: Set(process, ForeachWriter, open, <init>, close)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	verifyBatchIds, HDFSMetadataLog, notify, metadataPath, parseVersion, wait, FileContextManager, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, create, logTrace, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, open, logName, notifyAll, FileSystemManager, rename, purge, isInstanceOf, getLatest, <init>, mkdirs, FileManager, isBatchFile, ==, clone, purgeAfter, batchIdToPath, exists, delete, $init$, fileManager, toString, logError, !=, get, getClass, logWarning, batchFilesFilter, ne, serialize, pathToBatchId, add, getOrderedBatchFiles, eq, log, ##, list, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala: Set(asInstanceOf, <init>, toString, !=, add, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(<init>, ==, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, isInstanceOf, getLatest, <init>, ==, exists, toString, !=, get, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, get, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(metadataPath, asInstanceOf, synchronized, logTrace, purge, isInstanceOf, getLatest, <init>, ==, toString, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, isInstanceOf, getLatest, <init>, ==, exists, toString, !=, get, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(HDFSMetadataLog, metadataPath, parseVersion, <init>, ==, toString, !=, get, add, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(metadataPath, asInstanceOf, purge, isInstanceOf, getLatest, <init>, ==, exists, toString, !=, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala: Set(HDFSMetadataLog, parseVersion, asInstanceOf, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala: Set(HDFSMetadataLog, parseVersion, <init>, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(verifyBatchIds, HDFSMetadataLog, getLatest, <init>, ==, get, logWarning, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(metadataPath, asInstanceOf, synchronized, create, purge, isInstanceOf, getLatest, <init>, ==, toString, !=, get, ne, add, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala: Set(HDFSMetadataLog, metadataPath, parseVersion, getLatest, <init>, FileManager, ==, batchIdToPath, delete, fileManager, toString, get, batchFilesFilter, pathToBatchId, add, list, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(metadataPath, asInstanceOf, synchronized, create, purge, isInstanceOf, getLatest, <init>, ==, toString, !=, get, ne, add, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(metadataPath, asInstanceOf, isInstanceOf, <init>, ==, delete, toString, logError, !=, get, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(metadataPath, asInstanceOf, purge, isInstanceOf, getLatest, <init>, ==, exists, toString, !=, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(metadataPath, asInstanceOf, synchronized, create, purge, isInstanceOf, getLatest, <init>, ==, toString, !=, get, ne, add, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(metadataPath, asInstanceOf, isInstanceOf, <init>, ==, delete, toString, logError, !=, get, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(metadataPath, asInstanceOf, purge, isInstanceOf, getLatest, <init>, ==, exists, toString, !=, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(metadataPath, asInstanceOf, synchronized, logTrace, purge, isInstanceOf, getLatest, <init>, ==, toString, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(metadataPath, asInstanceOf, synchronized, logTrace, purge, isInstanceOf, getLatest, <init>, ==, toString, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(verifyBatchIds, HDFSMetadataLog, getLatest, <init>, ==, get, logWarning, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala: Set(asInstanceOf, <init>, toString, !=, add, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(<init>, ==, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, evaluate, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, EvalPythonExec, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, expressions, outputSet, transformUp, isInstanceOf, references, EvalPythonExec, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala: Set(map, asInstanceOf, expressions, isInstanceOf, EvalPythonExec, <init>, schema, apply, flatMap, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, EvalPythonExec, <init>, schema, apply, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, expressions, outputSet, transformUp, isInstanceOf, references, EvalPythonExec, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, expressions, outputSet, transformUp, isInstanceOf, references, EvalPythonExec, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, format, wait, $asInstanceOf, insertInto, partitionBy, equals, json, jdbc, asInstanceOf, orc, parquet, sortBy, csv, synchronized, option, $isInstanceOf, text, notifyAll, isInstanceOf, options, bucketBy, <init>, ==, clone, mode, toString, !=, getClass, saveAsTable, save, ne, DataFrameWriter, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(jdbc, asInstanceOf, isInstanceOf, <init>, ==, mode, toString, !=, DataFrameWriter, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(format, json, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, DataFrameWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, reader, equals, asInstanceOf, synchronized, $isInstanceOf, DataSourceReaderHolder, canEqual, notifyAll, isInstanceOf, ==, clone, $init$, toString, !=, getClass, output, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(reader, asInstanceOf, isInstanceOf, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(reader, asInstanceOf, isInstanceOf, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala: Set(reader, asInstanceOf, DataSourceReaderHolder, isInstanceOf, ==, toString, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(reader, asInstanceOf, DataSourceReaderHolder, isInstanceOf, ==, toString, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(reader, asInstanceOf, isInstanceOf, ==, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(reader, asInstanceOf, isInstanceOf, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(reader, asInstanceOf, isInstanceOf, ==, toString, !=, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(reader, asInstanceOf, synchronized, isInstanceOf, ==, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(reader, asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	defaultFormats.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newByteEncoder, newScalaDecimalEncoder, newBoxedDoubleEncoder, newBoxedFloatEncoder, newByteArrayEncoder, newIntArrayEncoder, newBooleanArrayEncoder, newFloatEncoder, newMapEncoder, newProductEncoder, newDoubleArrayEncoder, newBoxedLongEncoder, newBoxedShortEncoder, newSequenceEncoder, newIntEncoder, newShortArrayEncoder, newJavaDecimalEncoder, newStringEncoder, newBooleanEncoder, StringToColumn, newLongArrayEncoder, newShortEncoder, newFloatArrayEncoder, newBoxedIntEncoder, localSeqToDatasetHolder, newBoxedBooleanEncoder, newDoubleEncoder, newTimeStampEncoder, symbolToColumn, newLongEncoder, newDateEncoder, newProductArrayEncoder, rddToDatasetHolder, newBoxedByteEncoder, newSetEncoder, newStringArrayEncoder.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ContinuousReadSupport, createContinuousReader.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(ContinuousReadSupport)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(ContinuousReadSupport, createContinuousReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(ContinuousReadSupport)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(ContinuousReadSupport, createContinuousReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/BoundOrdering.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/BoundOrdering.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/BoundOrdering.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/BoundOrdering.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/BoundOrdering.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, BoundOrdering, wait, copy$default$2, $asInstanceOf, RowBoundOrdering, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, bound, offset, RangeBoundOrdering, ==, clone, $init$, ordering, copy$default$3, copy, toString, !=, getClass, copy$default$1, current, ne, eq, compare, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala: Set(BoundOrdering, RowBoundOrdering, asInstanceOf, isInstanceOf, <init>, bound, offset, RangeBoundOrdering, ==, ordering, copy, toString, !=, current, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala: Set(BoundOrdering, <init>, offset, ==, copy, !=, current, compare)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, AggregateHashMap, find, wait, $assertionsDisabled, equals, findOrInsert, notifyAll, <init>, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, v1Relation, notify, treeString$default$2, unapply, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, source, execute, executeCollectIterator, wait, stats, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, copy$default$5, ContinuousExecutionRelation, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, StreamingExecutionRelation, doPrepare, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, dataSource, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, computeStats, logName, notifyAll, conf, mapProductIterator, sourceName, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, StreamingRelationV2, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, StreamingRelationExec, constraints, sameResult, foreach, p, jsonFields, resolve, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, newInstance, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, extraOptions, executeBroadcast, ne, requiredChildOrdering, transform, StreamingRelation, withNewChildren, resolveQuoted, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, stats, map, asInstanceOf, StreamingExecutionRelation, expressions, conf, sourceName, isInstanceOf, isStreaming, StreamingRelationV2, <init>, outputOrdering, schema, apply, ==, StreamingRelationExec, p, sparkContext, outputPartitioning, copy, toString, output, ne, StreamingRelation, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(v1Relation, source, execute, map, asInstanceOf, StreamingExecutionRelation, expressions, dataSource, conf, sourceName, isInstanceOf, isStreaming, StreamingRelationV2, <init>, schema, apply, flatMap, ==, foreach, sparkContext, copy, toString, !=, collect, logWarning, output, transformAllExpressions, extraOptions, ne, transform, StreamingRelation, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala: Set(unapply, map, asInstanceOf, synchronized, StreamingExecutionRelation, expressions, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, sparkContext, copy, toString, collect, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(v1Relation, source, schemaString, asInstanceOf, conf, isInstanceOf, StreamingRelationV2, <init>, schema, apply, ==, sqlContext, newInstance, toString, extraOptions, StreamingRelation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(source, execute, ContinuousExecutionRelation, map, asInstanceOf, synchronized, expressions, dataSource, sourceName, isInstanceOf, StreamingRelationV2, <init>, schema, apply, ==, sparkContext, toString, !=, collect, output, transformAllExpressions, extraOptions, ne, transform, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, OneTimeTrigger, wait, $asInstanceOf, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, canEqual, productPrefix, notifyAll, isInstanceOf, ==, clone, $init$, toString, !=, getClass, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(OneTimeTrigger, asInstanceOf, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, fold, nonEmpty, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, toString, copyToArray, length, seq, !=, collect, getClass, hasDefiniteSize, patch, close, foldLeft, contains, isEmpty, ne, withPartial, reversed, hasNext, indexOf, reduceLeft, HadoopFileWholeTextReader, eq, sum, ##, scanLeft, finalize, hashCode, zipAll, product.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala: Set(size, map, <init>, foreach, length, !=, close, isEmpty, HadoopFileWholeTextReader)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, notifyAll, <init>, ClusteredDistribution, toString, getClass, clusteredColumns, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala: Set(<init>, ClusteredDistribution)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, SaveIntoDataSourceCommand, maxRowsPerPartition, verboseString, semanticHash, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, query, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, dataSource, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, options, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy$default$3, copy, mode, inputSet, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(find, SaveIntoDataSourceCommand, stats, map, asInstanceOf, run, expressions, dataSource, conf, isInstanceOf, options, <init>, schema, apply, flatMap, resolved, ==, p, mode, toString, !=, collect, getClass, logWarning, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, createFilter, !=, getClass, ParquetFilters, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, isInstanceOf, ==, toString, createFilter, getClass, ParquetFilters, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	onExecutorUnblacklisted, notify, post, getTimer, postToAll, wait, $asInstanceOf, onJobEnd, onApplicationEnd, listeners, equals, onTaskEnd, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, onOtherEvent, isTraceEnabled, initializeLogIfNecessary$default$2, onTaskStart, logName, notifyAll, onExecutorRemoved, onExecutorAdded, isInstanceOf, STREAM_EVENT_QUERY, removeListener, <init>, onBlockUpdated, ==, onBlockManagerRemoved, clone, onExecutorMetricsUpdate, onEnvironmentUpdate, onBlockManagerAdded, $init$, onNodeBlacklisted, findListenersByClass, onApplicationStart, onSpeculativeTaskSubmitted, toString, logError, !=, getClass, logWarning, onExecutorBlacklisted, doPostEvent, onUnpersistRDD, addListener, ne, onStageSubmitted, eq, onNodeUnblacklisted, log, StreamingQueryListenerBus, ##, finalize, hashCode, logDebug, logInfo, removeListenerOnError.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(post, wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, removeListener, <init>, ==, toString, !=, logWarning, addListener, ne, StreamingQueryListenerBus)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, sparkSchema, reader, equals, SpecificParquetRecordReaderBase$ValuesReaderIntIterator, SpecificParquetRecordReaderBase, getProgress, SpecificParquetRecordReaderBase$IntIterator, listDirectory, notifyAll, delegate, initialize, getCurrentValue, <init>, nextKeyValue, nextInt, SpecificParquetRecordReaderBase$NullIntIterator, createRLEIterator, toString, file, getCurrentKey, getClass, totalRowCount, close, fileSchema, SpecificParquetRecordReaderBase$RLEIntIterator, requestedSchema, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, metadataPath, parseVersion, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, logName, notifyAll, purge, isInstanceOf, VERSION, getLatest, <init>, isBatchFile, ==, clone, purgeAfter, batchIdToPath, OffsetSeqLog, $init$, fileManager, toString, logError, !=, get, getClass, logWarning, batchFilesFilter, ne, serialize, pathToBatchId, add, getOrderedBatchFiles, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(metadataPath, asInstanceOf, synchronized, purge, isInstanceOf, getLatest, <init>, ==, OffsetSeqLog, toString, !=, get, ne, add, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(metadataPath, asInstanceOf, isInstanceOf, <init>, ==, OffsetSeqLog, toString, logError, !=, get, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(metadataPath, asInstanceOf, purge, isInstanceOf, getLatest, <init>, ==, OffsetSeqLog, toString, !=, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, BatchEvalPythonExec, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, evaluate, udfs, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, BatchEvalPythonExec, asInstanceOf, transformExpressions, udfs, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, startQuery$default$8, wait, $asInstanceOf, notifyQueryTermination, equals, startQuery, asInstanceOf, initializeLogIfNecessary, StreamingQueryManager, startQuery$default$7, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, resetTerminated, logName, notifyAll, isInstanceOf, removeListener, <init>, awaitAnyTermination, stateStoreCoordinator, postListenerEvent, ==, clone, $init$, startQuery$default$10, toString, startQuery$default$9, logError, !=, get, getClass, logWarning, addListener, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo, active.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(StreamingQueryManager, <init>, clone, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(StreamingQueryManager, <init>, toString, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(asInstanceOf, StreamingQueryManager, isInstanceOf, <init>, stateStoreCoordinator, ==, toString, !=, get, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(notifyQueryTermination, asInstanceOf, StreamingQueryManager, isInstanceOf, <init>, postListenerEvent, ==, toString, logError, !=, get, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(StreamingQueryManager, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(asInstanceOf, StreamingQueryManager, isInstanceOf, <init>, stateStoreCoordinator, ==, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, StreamingQueryManager, synchronized, isInstanceOf, <init>, ==, clone, toString, get, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(startQuery, asInstanceOf, StreamingQueryManager, isInstanceOf, <init>, ==, toString, !=, get, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(asInstanceOf, StreamingQueryManager, isInstanceOf, <init>, stateStoreCoordinator, ==, toString, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(StreamingQueryManager, <init>, stateStoreCoordinator, get)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CodecStreams.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CodecStreams.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CodecStreams.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CodecStreams.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CodecStreams.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, createOutputStream, equals, asInstanceOf, synchronized, createInputStream, $isInstanceOf, notifyAll, isInstanceOf, createOutputStreamWriter$default$3, ==, clone, createOutputStreamWriter, toString, !=, getCompressionExtension, getClass, CodecStreams, createInputStreamWithCloseResource, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(toString, CodecStreams, createInputStreamWithCloseResource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, CodecStreams, createInputStreamWithCloseResource, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(isInstanceOf, ==, createOutputStreamWriter, !=, getCompressionExtension, getClass, CodecStreams, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(isInstanceOf, ==, createOutputStreamWriter, !=, getCompressionExtension, getClass, CodecStreams, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala: Set(createOutputStream, !=, getCompressionExtension, CodecStreams)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortPrefixUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortPrefixUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortPrefixUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortPrefixUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortPrefixUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, SortPrefixUtils, synchronized, $isInstanceOf, getPrefixComparator, notifyAll, createPrefixGenerator, isInstanceOf, ==, clone, toString, !=, getClass, canSortFullyWithPrefix, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala: Set(asInstanceOf, SortPrefixUtils, getPrefixComparator, isInstanceOf, ==, toString, canSortFullyWithPrefix, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF14.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF14.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF14.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF14.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF14.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF14.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF14)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF14.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	SessionConfigSupport, keyPrefix.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala: Set(SessionConfigSupport, keyPrefix)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, apply, ==, clone, planLater, $init$, toString, logError, !=, getClass, logWarning, ne, eq, FileSourceStrategy, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(asInstanceOf, isInstanceOf, apply, ==, FileSourceStrategy)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, FlatMapGroupsWithStateFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(call, FlatMapGroupsWithStateFunction)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, copy$default$2, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, scanTable, union, coalesce$default$3, zip, localCheckpoint, map, productArity, subtract, JDBCPartition, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, canEqual, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, productPrefix, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, resolveTable, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, copy, pipe$default$7, idx, toString, mapPartitionsInternal, preferredLocations, whereClause, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, copy$default$1, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, productIterator, withScope, log, compileFilter, ##, finalize, treeAggregate, index, productElement, hashCode, takeOrdered, logDebug, firstParent, logInfo, JDBCRDD, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(scanTable, JDBCPartition, asInstanceOf, conf, isInstanceOf, filter, <init>, ==, resolveTable, sparkContext, toString, whereClause, !=, logWarning, isEmpty, eq, compileFilter, JDBCRDD)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(name, mapPartitions, map, JDBCPartition, asInstanceOf, conf, isInstanceOf, <init>, ++, flatMap, take, ==, foreach, zipWithIndex, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	readSchema, pushFilters, createDataReaderFactories, SupportsPushDownFilters, pushedFilters.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala: Set(SupportsPushDownFilters, pushedFilters)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(readSchema, pushFilters, SupportsPushDownFilters)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, DataSourceV2Utils, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, extractSessionConfigs, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(DataSourceV2Utils, asInstanceOf, isInstanceOf, extractSessionConfigs, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(DataSourceV2Utils, asInstanceOf, isInstanceOf, extractSessionConfigs, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ShuffledRowRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, dependency, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, partitionStartIndices, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, CoalescedPartitioner, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, ShuffledRowRDD, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, getPartition, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, numPartitions, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala: Set(mapPartitions, map, asInstanceOf, isInstanceOf, <init>, ShuffledRowRDD, take, ==, toString, mapPartitionsInternal, !=, eq, takeOrdered)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala: Set(map, partitionStartIndices, asInstanceOf, synchronized, min, isInstanceOf, <init>, max, ShuffledRowRDD, ==, distinct, sparkContext, !=, partitions, isEmpty, numPartitions, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala: Set(partitioner, dependency, map, asInstanceOf, conf, isInstanceOf, <init>, mapPartitionsWithIndexInternal, ShuffledRowRDD, ==, foreach, sparkContext, toString, mapPartitionsInternal, partitions, getPartition, numPartitions, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, TypedSumLong, wait, $asInstanceOf, toColumnJava, TypedCount, equals, zero, asInstanceOf, f, synchronized, $isInstanceOf, finish, notifyAll, isInstanceOf, <init>, merge, toColumn, outputEncoder, ==, clone, reduce, toString, TypedSumDouble, !=, TypedAverage, getClass, bufferEncoder, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala: Set(TypedSumLong, TypedCount, f, <init>, toColumn, TypedSumDouble, TypedAverage)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	DataWriterFactory, createDataWriter.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala: Set(DataWriterFactory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala: Set(DataWriterFactory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala: Set(DataWriterFactory)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(DataWriterFactory, createDataWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(DataWriterFactory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala: Set(DataWriterFactory)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, batchTimestampMs, notify, setStoreMetrics, treeString$default$2, find, getTimeoutTimestamp, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, groupingAttributes, requiredChildDistribution, getStateRow, copy$default$2, $asInstanceOf, numberedTreeString, outputMode, copy$default$5, resetMetrics, printSchema, timeoutConf, map, productArity, verboseStringWithSuffix, equals, treeString, outputObjectType, schemaString, getStateInfo, copy$default$9, StateStoreUpdater, copy$default$12, argString, watermarkPredicateForData, subqueries, executeQuery, asInstanceOf, keyExpressions, transformExpressions, initializeLogIfNecessary, doExecute, watermarkPredicateForKeys, generateTreeString, prepare, dataAttributes, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, copy$default$8, getProgress, copy$default$13, logTrace, asCode, canEqual, expressions, canonicalized, updateStateForKeysWithData, stateInfo, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, stateEncoder, child, doCanonicalize, valueDeserializer, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, setTimeoutTimestamp, transformExpressionsDown, prettyJson, apply, flatMap, timeTakenMs, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, copy$default$7, subexpressionEliminationEnabled, sparkContext, copy$default$10, outputPartitioning, $init$, removeKeysOlderThanWatermark, copy$default$3, func, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, copy$default$1, getStateObj, transformDown, transformAllExpressions, outputObjAttr, mapExpressions, copy$default$6, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, copy$default$11, updateStateForTimedOutKeys, eventTimeWatermark, statePrefix, keyDeserializer, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, FlatMapGroupsWithStateExec, executeToIterator, ##, containsChild, newOrdering, finalize, watermarkExpression, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(batchTimestampMs, execute, outputMode, map, asInstanceOf, keyExpressions, expressions, stateInfo, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, withNewChildren, eventTimeWatermark, FlatMapGroupsWithStateExec, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, groupingAttributes, outputMode, map, asInstanceOf, dataAttributes, expressions, conf, isInstanceOf, stateEncoder, child, valueDeserializer, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, func, copy, toString, output, outputObjAttr, ne, keyDeserializer, eq, FlatMapGroupsWithStateExec)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF12.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF12.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF12.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF12.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF12.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF12.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF12)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF12.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, ReusedExchangeExec, notify, ReuseExchange, treeString$default$2, ruleName, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, Exchange, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala: Set(map, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, sqlContext, sparkContext, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(execute, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, withNewChildren, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala: Set(execute, map, asInstanceOf, expressions, isInstanceOf, child, <init>, outputOrdering, apply, ==, outputPartitioning, copy, toString, executeTake, !=, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(ReuseExchange, simpleString, execute, map, treeString, asInstanceOf, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala: Set(execute, map, asInstanceOf, Exchange, expressions, conf, isInstanceOf, child, <init>, apply, ==, foreach, sparkContext, copy, toString, metrics, longMetric, output, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala: Set(executeCollectIterator, map, asInstanceOf, Exchange, expressions, canonicalized, conf, isInstanceOf, child, <init>, apply, ==, sqlContext, sparkContext, toString, metrics, getClass, longMetric, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala: Set(ReusedExchangeExec, simpleString, children, map, subqueries, asInstanceOf, nodeName, Exchange, isInstanceOf, child, <init>, apply, ==, metrics, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, FilterAndProject, unapply, ruleName, wait, $asInstanceOf, canEvaluateWithinJoin, equals, canEvaluate, asInstanceOf, initializeLogIfNecessary, synchronized, splitDisjunctivePredicates, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, splitConjunctivePredicates, logName, notifyAll, isInstanceOf, apply, ==, clone, $init$, PushDownOperatorsToDataSource, toString, logError, !=, getClass, logWarning, replaceAlias, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala: Set(PushDownOperatorsToDataSource)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getInterval, notify, reserveDictionaryIds, putShort, arrayData, appendNulls, appendStruct, putShorts, getShort, putDecimal, wait, getUTF8String, reserve, $assertionsDisabled, reserveNewColumn, getBooleans, getArrayOffset, appendLong, appendShort, putNull, equals, putLongsLittleEndian, putLong, hasNull, appendShorts, isNullAt, putDoubles, allocateColumns, getDictId, putNulls, getDouble, putFloats, putBoolean, appendFloats, appendNotNulls, getStruct, appendFloat, getFloats, getBytesAsUTF8String, putInt, putDouble, putIntsLittleEndian, appendDouble, appendBooleans, reserveInternal, setDictionary, getMap, putBytes, getDictionaryIds, notifyAll, getByte, appendByteArray, appendBoolean, appendByte, putArray, getArray, putLongs, <init>, getInts, putNotNull, appendArray, getDoubles, appendBytes, getFloat, getDecimal, appendInt, appendNull, putBooleans, hasDictionary, getInt, getBytes, appendDoubles, getShorts, putNotNulls, appendInts, getLongs, reset, putByteArray, putFloat, toString, getBoolean, numNulls, getClass, setIsConstant, dataType, close, getArrayLength, appendLongs, getChild, valuesNativeAddress, getElementsAppended, getLong, hashCode, appendNotNull, OffHeapColumnVector, getBinary, putInts, putByte.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(<init>, toString, getBoolean, getClass, dataType, close, hashCode, OffHeapColumnVector)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(allocateColumns, <init>, toString, dataType, close, OffHeapColumnVector)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, zero, asInstanceOf, synchronized, $isInstanceOf, finish, notifyAll, isInstanceOf, <init>, Aggregator, merge, toColumn, outputEncoder, ==, clone, reduce, toString, !=, getClass, bufferEncoder, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala: Set(<init>, toColumn)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala: Set(zero, asInstanceOf, finish, isInstanceOf, <init>, Aggregator, merge, outputEncoder, ==, reduce, toString, !=, getClass, bufferEncoder, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(asInstanceOf, <init>, toColumn, outputEncoder, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala: Set(<init>, toColumn)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala: Set(asInstanceOf, <init>, Aggregator, toColumn, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala: Set(asInstanceOf, <init>, Aggregator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(asInstanceOf, <init>, toColumn, outputEncoder, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, PrunedScan, wait, $asInstanceOf, insert, sourceSchema, equals, buildScan, asInstanceOf, unhandledFilters, synchronized, $isInstanceOf, CreatableRelationProvider, StreamSinkProvider, notifyAll, StreamSourceProvider, isInstanceOf, BaseRelation, <init>, schema, createSink, ==, CatalystScan, sqlContext, clone, sizeInBytes, RelationProvider, SchemaRelationProvider, TableScan, toString, createRelation, !=, InsertableRelation, getClass, shortName, ne, PrunedFilteredScan, createSource, DataSourceRegister, eq, ##, finalize, hashCode, needConversion.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(sourceSchema, asInstanceOf, CreatableRelationProvider, StreamSourceProvider, isInstanceOf, BaseRelation, <init>, schema, createSink, ==, sqlContext, sizeInBytes, RelationProvider, SchemaRelationProvider, toString, createRelation, !=, getClass, shortName, ne, createSource, DataSourceRegister, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, <init>, schema, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(sourceSchema, asInstanceOf, CreatableRelationProvider, StreamSourceProvider, isInstanceOf, BaseRelation, <init>, schema, createSink, ==, sqlContext, sizeInBytes, RelationProvider, SchemaRelationProvider, toString, createRelation, !=, getClass, shortName, ne, createSource, DataSourceRegister, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, <init>, schema, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(BaseRelation, <init>, schema, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(sourceSchema, asInstanceOf, CreatableRelationProvider, StreamSourceProvider, isInstanceOf, BaseRelation, <init>, schema, createSink, ==, sqlContext, sizeInBytes, RelationProvider, SchemaRelationProvider, toString, createRelation, !=, getClass, shortName, ne, createSource, DataSourceRegister, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, <init>, schema, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(sourceSchema, asInstanceOf, CreatableRelationProvider, StreamSourceProvider, isInstanceOf, BaseRelation, <init>, schema, createSink, ==, sqlContext, sizeInBytes, RelationProvider, SchemaRelationProvider, toString, createRelation, !=, getClass, shortName, ne, createSource, DataSourceRegister, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(PrunedScan, buildScan, asInstanceOf, unhandledFilters, isInstanceOf, BaseRelation, <init>, schema, ==, CatalystScan, TableScan, toString, !=, ne, PrunedFilteredScan, eq, needConversion)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(insert, asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, toString, !=, InsertableRelation, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(sourceSchema, asInstanceOf, CreatableRelationProvider, StreamSourceProvider, isInstanceOf, BaseRelation, <init>, schema, createSink, ==, sqlContext, sizeInBytes, RelationProvider, SchemaRelationProvider, toString, createRelation, !=, getClass, shortName, ne, createSource, DataSourceRegister, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sizeInBytes, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(sourceSchema, asInstanceOf, CreatableRelationProvider, StreamSourceProvider, isInstanceOf, BaseRelation, <init>, schema, createSink, ==, sqlContext, sizeInBytes, RelationProvider, SchemaRelationProvider, toString, createRelation, !=, getClass, shortName, ne, createSource, DataSourceRegister, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(BaseRelation, <init>, schema, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, isInstanceOf, BaseRelation, <init>, schema, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(PrunedScan, buildScan, asInstanceOf, unhandledFilters, isInstanceOf, BaseRelation, <init>, schema, ==, CatalystScan, TableScan, toString, !=, ne, PrunedFilteredScan, eq, needConversion)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(BaseRelation, <init>, schema, sqlContext, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(StreamSourceProvider, <init>, schema, ==, sqlContext, toString, !=, shortName, DataSourceRegister)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(insert, asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, toString, !=, InsertableRelation, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala: Set(asInstanceOf, CreatableRelationProvider, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, toString, DataSourceRegister, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(asInstanceOf, StreamSourceProvider, isInstanceOf, <init>, schema, ==, sqlContext, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, ne, DataSourceRegister, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(isInstanceOf, <init>, ==, !=, getClass, shortName, DataSourceRegister, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, clone, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(isInstanceOf, <init>, ==, !=, getClass, DataSourceRegister, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala: Set(<init>, schema, !=, DataSourceRegister)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala: Set(asInstanceOf, CreatableRelationProvider, isInstanceOf, BaseRelation, <init>, ==, sqlContext, toString, createRelation, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(sourceSchema, asInstanceOf, CreatableRelationProvider, StreamSourceProvider, isInstanceOf, BaseRelation, <init>, schema, createSink, ==, sqlContext, sizeInBytes, RelationProvider, SchemaRelationProvider, toString, createRelation, !=, getClass, shortName, ne, createSource, DataSourceRegister, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, isInstanceOf, <init>, schema, ==, toString, getClass, ne, DataSourceRegister, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(asInstanceOf, isInstanceOf, <init>, schema, ==, !=, getClass, DataSourceRegister, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sizeInBytes, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, sizeInBytes, toString, shortName, DataSourceRegister, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, sizeInBytes, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala: Set(insert, asInstanceOf, isInstanceOf, BaseRelation, <init>, ==, toString, InsertableRelation, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala: Set(CreatableRelationProvider, BaseRelation, <init>, schema, ==, sqlContext, RelationProvider, createRelation, DataSourceRegister)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, sqlContext, toString, !=, InsertableRelation, PrunedFilteredScan, eq, needConversion)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala: Set(sourceSchema, asInstanceOf, synchronized, StreamSourceProvider, isInstanceOf, <init>, schema, ==, sqlContext, !=, ne, DataSourceRegister)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala: Set(CreatableRelationProvider, BaseRelation, <init>, schema, ==, sqlContext, RelationProvider, createRelation, DataSourceRegister)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(asInstanceOf, isInstanceOf, BaseRelation, <init>, schema, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecutionException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecutionException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecutionException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecutionException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecutionException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, toString, !=, getClass, ne, eq, QueryExecutionException, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, getMessage, ==, toString, !=, ne, eq, QueryExecutionException)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, produce, parent, simpleString, children, doProduce, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, doConsume, relation, evaluateVariables, copy$default$5, resetMetrics, printSchema, map, productArity, FileSourceScanExec, verboseStringWithSuffix, equals, DataSourceScanExec, treeString, schemaString, argString, dataFilters, subqueries, executeQuery, nodeNamePrefix, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, partitionFilters, usedInputs, logTrace, asCode, canEqual, expressions, canonicalized, RowDataSourceScanExec, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, tableIdentifier, fullOutput, logName, notifyAll, conf, mapProductIterator, needsUnsafeRowConversion, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, rdd, inputRDDs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, handledFilters, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, supportsBatch, fastEquals, sqlContext, origin, transformExpressionsUp, clone, filters, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, copy$default$7, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, metadata, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, requiredColumnsIndex, logError, !=, consume, innerChildren, collect, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, copy$default$6, executeBroadcast, ne, requiredSchema, vectorTypes, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, map, FileSourceScanExec, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, nodeName, usedInputs, expressions, conf, isInstanceOf, rdd, inputRDDs, references, <init>, outputOrdering, schema, apply, ==, supportsBatch, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala: Set(simpleString, children, map, FileSourceScanExec, subqueries, asInstanceOf, nodeName, isInstanceOf, <init>, apply, ==, metadata, metrics, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(find, simpleString, relation, map, FileSourceScanExec, dataFilters, asInstanceOf, expressions, isInstanceOf, references, <init>, apply, ==, filters, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(relation, map, asInstanceOf, expressions, RowDataSourceScanExec, conf, collectFirst, isInstanceOf, rdd, references, <init>, schema, handledFilters, apply, flatMap, ==, filters, foreach, p, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	PartitionOffset.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(PartitionOffset)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/PartitionOffset.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala: Set(PartitionOffset)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(PartitionOffset)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(PartitionOffset)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/interface.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	tableType, notify, database, nullable, name, wait, Function, Column, $asInstanceOf, isBucket, equals, description, asInstanceOf, synchronized, Database, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, Table, className, locationUri, isTemporary, toString, !=, getClass, dataType, ne, isPartition, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala: Set(Function, Column, Database, <init>, Table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(tableType, database, nullable, name, Function, Column, description, asInstanceOf, Database, <init>, ==, Table, locationUri, dataType, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, DataSourceV2Strategy, apply, ==, clone, planLater, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(asInstanceOf, isInstanceOf, DataSourceV2Strategy, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	LoadDataCommand, notify, treeString$default$2, find, isView, isLocal, simpleString, children, refresh, maxRowsPerPartition, verboseString, partitionSpec, semanticHash, wait, stats, copy$default$2, $asInstanceOf, location, path, numberedTreeString, copy$default$5, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, AlterTableRenameCommand, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, ShowCreateTableCommand, oldName, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, partition, allAttributes, nodeName, $isInstanceOf, DescribeColumnCommand, newName, ignoreIfExists, CreateTableLikeCommand, validConstraints, logTrace, ShowPartitionsCommand, asCode, canEqual, expressions, canonicalized, ShowTablesCommand, copy$default$4, ifNotExists, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, isOverwrite, resolveChildren, propertyKey, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, TruncateTableCommand, CreateTableCommand, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, DescribeTableCommand, tableIdentifierPattern, flatMap, resolved, tableName, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, spec, copy$default$3, copy, inputSet, colNameParts, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, databaseName, logWarning, output, copy$default$1, isExtended, transformDown, transformAllExpressions, mapExpressions, colsToAdd, targetTable, ne, AlterTableAddColumnsCommand, transform, sourceTable, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, table, productElement, hashCode, logDebug, logInfo, ShowTablePropertiesCommand, ShowColumnsCommand.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(path, map, expressions, ShowTablesCommand, conf, isStreaming, <init>, schema, apply, tableName, foreach, toString, databaseName, ne, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(simpleString, stats, map, treeString, asInstanceOf, run, ShowTablesCommand, conf, isInstanceOf, <init>, apply, DescribeTableCommand, ==, toString, !=, output, isExtended, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(map, asInstanceOf, expressions, ShowTablesCommand, conf, isInstanceOf, <init>, schema, apply, ==, foreach, toString, !=, collect, databaseName, logWarning, ne, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(LoadDataCommand, children, partitionSpec, location, path, map, AlterTableRenameCommand, asInstanceOf, ShowCreateTableCommand, DescribeColumnCommand, CreateTableLikeCommand, ShowPartitionsCommand, expressions, ShowTablesCommand, ifNotExists, conf, isInstanceOf, TruncateTableCommand, <init>, schema, apply, DescribeTableCommand, flatMap, ==, foreach, spec, !=, collect, logWarning, isExtended, targetTable, ne, AlterTableAddColumnsCommand, sourceTable, table, ShowTablePropertiesCommand, ShowColumnsCommand)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CoGroupedIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CoGroupedIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CoGroupedIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CoGroupedIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CoGroupedIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, CoGroupedIterator, find, span, toBuffer, count, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, fold, nonEmpty, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, toString, copyToArray, length, seq, !=, collect, getClass, hasDefiniteSize, patch, foldLeft, contains, isEmpty, ne, withPartial, reversed, hasNext, indexOf, reduceLeft, eq, sum, ##, scanLeft, finalize, hashCode, zipAll, product.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala: Set(CoGroupedIterator, map, asInstanceOf, isInstanceOf, GroupedIterator, <init>, ++, grouped, flatMap, ==, exists, toString, length, collect, contains, isEmpty, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	clearCache, notify, databaseExists, listTables, wait, $asInstanceOf, recoverPartitions, equals, createExternalTable, makeDataset, refreshByPath, getDatabase, asInstanceOf, synchronized, $isInstanceOf, CatalogImpl, tableExists, notifyAll, isInstanceOf, createTable, setCurrentDatabase, refreshTable, <init>, dropTempView, functionExists, uncacheTable, cacheTable, ==, currentDatabase, clone, dropGlobalTempView, getTable, isCached, toString, !=, getFunction, getClass, listFunctions, listColumns, ne, listDatabases, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, CatalogImpl, isInstanceOf, <init>, ==, clone, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, EXECUTION_ID_KEY, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, getQueryExecution, checkSQLExecutionId, withNewExecutionId, notifyAll, isInstanceOf, ==, clone, withExecutionId, SQLExecution, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala: Set(EXECUTION_ID_KEY, asInstanceOf, isInstanceOf, ==, SQLExecution, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(asInstanceOf, withNewExecutionId, isInstanceOf, ==, SQLExecution, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(asInstanceOf, withNewExecutionId, isInstanceOf, ==, SQLExecution, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(asInstanceOf, checkSQLExecutionId, isInstanceOf, ==, SQLExecution, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala: Set(asInstanceOf, synchronized, isInstanceOf, ==, SQLExecution, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(EXECUTION_ID_KEY, asInstanceOf, isInstanceOf, ==, SQLExecution, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala: Set(EXECUTION_ID_KEY, asInstanceOf, isInstanceOf, ==, SQLExecution, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(asInstanceOf, withNewExecutionId, isInstanceOf, ==, SQLExecution, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala: Set(EXECUTION_ID_KEY, asInstanceOf, isInstanceOf, ==, withExecutionId, SQLExecution, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(asInstanceOf, synchronized, withNewExecutionId, isInstanceOf, ==, SQLExecution, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala: Set(EXECUTION_ID_KEY, asInstanceOf, isInstanceOf, ==, withExecutionId, SQLExecution, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	addInt, notify, NoopUpdater, hasDictionarySupport, addLong, currentRecord, ParquetPrimitiveConverter, isPrimitive, wait, $asInstanceOf, equals, addBoolean, asInstanceOf, initializeLogIfNecessary, set, synchronized, binaryToUnscaledLong, $isInstanceOf, setBoolean, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, setDictionary, ParquetGroupConverter, setByte, logName, notifyAll, setInt, asPrimitiveConverter, isInstanceOf, <init>, HasParentContainerUpdater, addValueFromDictionary, ==, clone, addDouble, $init$, asGroupConverter, end, toString, ParentContainerUpdater, logError, !=, setShort, getClass, updater, logWarning, setLong, binaryToSQLTimestamp, start, ne, addBinary, ParquetRowConverter, eq, log, getConverter, ##, finalize, setFloat, hashCode, logDebug, addFloat, logInfo, setDouble.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala: Set(NoopUpdater, currentRecord, <init>, ParentContainerUpdater, ParquetRowConverter)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/PythonUDFRunner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, read, withPadding, find, span, toBuffer, count, reduceOption, resume, wait, isAlive, foldRight, threadLocalRandomSeed, takeWhile, $asInstanceOf, setName, minBy, size, zip, PythonUDFRunner, toSet, getContextClassLoader, join, corresponds, :\, handleEndOfDataSection, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, writeIteratorToStream, setPriority, handleException, writeUDFs, parkBlocker, toList, threadLocalRandomProbe, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, run, getPriority, shutdownOnTaskCompletion, reduceLeftOption, synchronized, sliding, partition, blockedOn, aggregate, $isInstanceOf, exception, forall, compute, mkString, checkAccess, min, newWriterThread, scanRight, envVars, fold, suspend, logTrace, nonEmpty, pythonVer, isTraceEnabled, initializeLogIfNecessary$default$2, getUncaughtExceptionHandler, getThreadGroup, stop, logName, notifyAll, /:, toIterator, getName, addString, to, collectFirst, isInterrupted, drop, isInstanceOf, getState, filter, getStackTrace, handlePythonException, GroupedIterator, <init>, toStream, destroy, pythonExec, max, buffered, ++, grouped, flatMap, take, WriterThread, reduceRight, ==, maxBy, indexWhere, accumulator, clone, setDaemon, slice, foreach, writeCommand, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, inheritableThreadLocals, reduce, padTo, $init$, toSeq, ReaderIterator, next, zipWithIndex, setContextClassLoader, threadLocals, toString, copyToArray, length, seq, isDaemon, logError, !=, collect, handleTimingData, getClass, logWarning, hasDefiniteSize, setUncaughtExceptionHandler, countStackFrames, patch, start, foldLeft, contains, isEmpty, MonitorThread, ne, withPartial, getId, reversed, hasNext, indexOf, threadLocalRandomSecondarySeed, reduceLeft, eq, interrupt, newReaderIterator, sum, log, ##, scanLeft, finalize, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala: Set(PythonUDFRunner, map, asInstanceOf, compute, isInstanceOf, GroupedIterator, <init>, grouped, flatMap, ==, exists, toArray, toString, length, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala: Set(read, PythonUDFRunner, handleEndOfDataSection, map, handleException, writeUDFs, exception, handlePythonException, <init>, pythonExec, WriterThread, ==, toArray, ReaderIterator, next, !=, handleTimingData, start, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	format.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, supportBatch, isSplitable, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, CsvOutputWriter, synchronized, $isInstanceOf, buildReaderWithPartitionValues, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, inferSchema, logName, notifyAll, isInstanceOf, CSVFileFormat, prepareWrite, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, shortName, close, ne, vectorTypes, buildReader, eq, write, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, inferSchema, isInstanceOf, CSVFileFormat, <init>, ==, toString, !=, getClass, logWarning, shortName, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, CSVFileFormat, <init>, ==, toString, !=, getClass, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	readSchema, createDataReaderFactories, pushCatalystFilters, SupportsPushDownCatalystFilters, pushedCatalystFilters.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala: Set(SupportsPushDownCatalystFilters, pushedCatalystFilters)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(readSchema, pushCatalystFilters, SupportsPushDownCatalystFilters)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownCatalystFilters.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, asWriterSettings, wait, multiLine, $asInstanceOf, maxColumns, CSVOptions, equals, isCommentSet, asInstanceOf, initializeLogIfNecessary, nullValue, nanValue, escapeQuotes, synchronized, $isInstanceOf, charToEscapeQuoteEscaping, inferSchemaFlag, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, quoteAll, logName, notifyAll, quote, isInstanceOf, compressionCodec, timestampFormat, <init>, escape, inputBufferSize, positiveInf, ==, clone, charset, comment, $init$, ignoreLeadingWhiteSpaceInRead, dateFormat, toString, columnNameOfCorruptRecord, parseMode, logError, !=, getClass, logWarning, ignoreLeadingWhiteSpaceFlagInWrite, ne, ignoreTrailingWhiteSpaceInRead, parameters, ignoreTrailingWhiteSpaceFlagInWrite, eq, maxCharsPerColumn, negativeInf, log, timeZone, ##, finalize, asParserSettings, hashCode, delimiter, logDebug, logInfo, headerFlag.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala: Set(CSVOptions, asInstanceOf, nullValue, nanValue, isInstanceOf, timestampFormat, <init>, positiveInf, ==, dateFormat, columnNameOfCorruptRecord, parseMode, !=, negativeInf, asParserSettings)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityGenerator.scala: Set(asWriterSettings, CSVOptions, asInstanceOf, nullValue, isInstanceOf, timestampFormat, <init>, ==, dateFormat, toString, ne, headerFlag)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala: Set(CSVOptions, asInstanceOf, nullValue, nanValue, inferSchemaFlag, isInstanceOf, timestampFormat, <init>, positiveInf, ==, ne, negativeInf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(multiLine, CSVOptions, asInstanceOf, nullValue, isInstanceOf, <init>, ==, charset, toString, !=, ne, parameters, asParserSettings, headerFlag)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(CSVOptions, isInstanceOf, compressionCodec, <init>, ==, columnNameOfCorruptRecord, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(CSVOptions, asInstanceOf, isInstanceOf, <init>, ==, toString, columnNameOfCorruptRecord, parseMode, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala: Set(CSVOptions, isCommentSet, asInstanceOf, isInstanceOf, <init>, ==, comment, toString, headerFlag)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, curried, generateRowIterator, name, wait, copy$default$2, $asInstanceOf, productArity, equals, generate, asInstanceOf, Buffer, bufferValues, synchronized, $isInstanceOf, genComputeHash, initializeAggregateHashMap, tupled, groupingKeySignature, canEqual, generateEquals, productPrefix, notifyAll, isInstanceOf, generateClose, <init>, apply, groupingKeys, buffVars, generateFindOrInsert, ==, clone, $init$, RowBasedHashMapGenerator, copy, toString, !=, getClass, copy$default$1, dataType, generateHashFunction, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(equals, generate, asInstanceOf, isInstanceOf, <init>, apply, ==, RowBasedHashMapGenerator, copy, toString, !=, dataType, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, fieldIndex, anyNull, getShort, wait, $asInstanceOf, size, inputSchema, equals, isNullAt, asInstanceOf, evaluate, getDouble, getString, synchronized, $isInstanceOf, getStruct, mkString, getAs, getTimestamp, getDate, getMap, notifyAll, getByte, initialize, isInstanceOf, <init>, merge, schema, apply, getFloat, getDecimal, ==, UserDefinedAggregateFunction, clone, distinct, getInt, getValuesMap, $init$, toSeq, getList, copy, toString, getBoolean, length, MutableAggregationBuffer, getSeq, !=, get, deterministic, getJavaMap, getClass, update, dataType, ne, bufferSchema, eq, ##, finalize, getLong, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, UserDefinedAggregateFunction, toSeq, toString, length, dataType, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala: Set(<init>, apply, UserDefinedAggregateFunction, MutableAggregationBuffer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala: Set(getShort, inputSchema, isNullAt, asInstanceOf, evaluate, getDouble, mkString, getByte, initialize, isInstanceOf, <init>, merge, schema, apply, getFloat, getDecimal, ==, UserDefinedAggregateFunction, getInt, toString, getBoolean, length, MutableAggregationBuffer, !=, get, deterministic, getClass, update, dataType, ne, bufferSchema, eq, getLong)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, UserDefinedAggregateFunction, toSeq, toString, length, dataType, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	partitionSchema, notify, refresh, partitionSpec, wait, $asInstanceOf, inferPartitioning, listFiles, equals, inputFiles, asInstanceOf, initializeLogIfNecessary, metadataOpsTimeNs, synchronized, $isInstanceOf, <init>$default$4, logTrace, hadoopConf, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, rootPaths, leafFiles, <init>, allFiles, ==, clone, PartitioningAwareFileIndex, sizeInBytes, $init$, leafDirToChildrenFiles, toString, logError, !=, getClass, logWarning, ne, BASE_PATH_PARAM, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, logTrace, hadoopConf, isInstanceOf, <init>, allFiles, ==, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(partitionSchema, partitionSpec, listFiles, inputFiles, asInstanceOf, metadataOpsTimeNs, hadoopConf, isInstanceOf, rootPaths, <init>, ==, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, hadoopConf, isInstanceOf, <init>, allFiles, ==, sizeInBytes, toString, !=, getClass, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(partitionSchema, inferPartitioning, asInstanceOf, logTrace, hadoopConf, isInstanceOf, rootPaths, leafFiles, <init>, allFiles, ==, PartitioningAwareFileIndex, toString, !=, getClass, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(inferPartitioning, leafFiles, <init>, allFiles, ==, PartitioningAwareFileIndex, leafDirToChildrenFiles, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, hadoopConf, isInstanceOf, <init>, allFiles, ==, sizeInBytes, toString, !=, getClass, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, logTrace, hadoopConf, isInstanceOf, <init>, allFiles, ==, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, hadoopConf, isInstanceOf, <init>, allFiles, ==, sizeInBytes, toString, !=, getClass, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, hadoopConf, isInstanceOf, <init>, allFiles, ==, sizeInBytes, toString, !=, getClass, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, asInstanceOf, isInstanceOf, <init>, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, logTrace, hadoopConf, isInstanceOf, <init>, allFiles, ==, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(partitionSchema, partitionSpec, listFiles, inputFiles, asInstanceOf, metadataOpsTimeNs, hadoopConf, isInstanceOf, rootPaths, <init>, ==, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, verifySchema, asInstanceOf, synchronized, $isInstanceOf, CSVUtils, notifyAll, isInstanceOf, dropHeaderLine, ==, clone, filterHeaderLine, toChar, toString, !=, getClass, ne, eq, ##, finalize, hashCode, filterCommentAndEmpty.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala: Set(asInstanceOf, CSVUtils, isInstanceOf, dropHeaderLine, ==, !=, filterCommentAndEmpty)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVOptions.scala: Set(asInstanceOf, CSVUtils, isInstanceOf, ==, toChar, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(asInstanceOf, CSVUtils, isInstanceOf, ==, filterHeaderLine, toString, !=, ne, filterCommentAndEmpty)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(verifySchema, CSVUtils, isInstanceOf, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(asInstanceOf, CSVUtils, isInstanceOf, ==, filterHeaderLine, toString, !=, ne, filterCommentAndEmpty)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, StateStoreCoordinatorRef, $asInstanceOf, reportActiveInstance, forDriver, equals, forExecutor, asInstanceOf, initializeLogIfNecessary, deactivateInstances, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, stop, logName, notifyAll, isInstanceOf, ==, clone, $init$, getLocation, toString, logError, !=, getClass, logWarning, ne, verifyIfInstanceActive, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, StateStoreCoordinatorRef, forDriver, asInstanceOf, deactivateInstances, synchronized, notifyAll, isInstanceOf, ==, toString, !=, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala: Set(StateStoreCoordinatorRef, getLocation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(StateStoreCoordinatorRef, asInstanceOf, isInstanceOf, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(StateStoreCoordinatorRef, asInstanceOf, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(StateStoreCoordinatorRef, asInstanceOf, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(StateStoreCoordinatorRef)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala: Set(StateStoreCoordinatorRef, reportActiveInstance, forExecutor, asInstanceOf, synchronized, stop, isInstanceOf, ==, toString, !=, logWarning, ne, verifyIfInstanceActive, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala: Set(StateStoreCoordinatorRef, asInstanceOf, isInstanceOf, ==, getLocation, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, planAggregateWithoutDistinct, isInstanceOf, planStreamingAggregation, ==, clone, toString, !=, getClass, ne, planAggregateWithOneDistinct, eq, ##, finalize, hashCode, AggUtils.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(asInstanceOf, planAggregateWithoutDistinct, isInstanceOf, planStreamingAggregation, ==, toString, ne, planAggregateWithOneDistinct, eq, AggUtils)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/package-info.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/package-info.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/package-info.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, produce, parent, simpleString, createDataReader, children, doProduce, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, doConsume, evaluateVariables, resetMetrics, printSchema, DataSourceV2ScanExec, map, productArity, reader, verboseStringWithSuffix, equals, treeString, schemaString, argString, rowReader, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, usedInputs, logTrace, asCode, canEqual, expressions, RowToUnsafeDataReader, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, needsUnsafeRowConversion, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, inputRDDs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, supportsBatch, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, next, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, preferredLocations, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, get, RowToUnsafeRowDataReaderFactory, consume, innerChildren, collect, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, close, executeBroadcast, ne, vectorTypes, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(createDataReader, map, reader, rowReader, asInstanceOf, expressions, RowToUnsafeDataReader, conf, isInstanceOf, <init>, apply, ==, sqlContext, foreach, next, copy, toString, preferredLocations, !=, get, getClass, close, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(DataSourceV2ScanExec, reader, asInstanceOf, isInstanceOf, <init>, output)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ParquetOutputWriter, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, toString, !=, getClass, close, ne, eq, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(ParquetOutputWriter, asInstanceOf, isInstanceOf, <init>, ==, toString, getClass, close, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, format, queryName, wait, $asInstanceOf, outputMode, partitionBy, equals, asInstanceOf, synchronized, option, $isInstanceOf, notifyAll, isInstanceOf, options, <init>, ==, clone, foreach, toString, !=, getClass, DataStreamWriter, start, ne, eq, ##, finalize, hashCode, trigger.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(format, asInstanceOf, isInstanceOf, <init>, ==, foreach, toString, !=, DataStreamWriter, start, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, supportBatch, isSplitable, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, buildReaderWithPartitionValues, inferSchema, notifyAll, isInstanceOf, prepareWrite, <init>, TextFileFormat, ==, clone, $init$, toString, !=, TextOutputWriter, getClass, shortName, close, ne, vectorTypes, buildReader, eq, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(isSplitable, <init>, TextFileFormat, toString, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(isSplitable, asInstanceOf, isInstanceOf, <init>, TextFileFormat, ==, toString, !=, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	partitionSchema, notify, refresh, wait, $asInstanceOf, listFiles, equals, inputFiles, asInstanceOf, filterPartitions, metadataOpsTimeNs, synchronized, $isInstanceOf, hadoopConf, notifyAll, isInstanceOf, rootPaths, <init>, ==, clone, sizeInBytes, $init$, toString, !=, getClass, ne, CatalogFileIndex, eq, ##, finalize, table, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, hadoopConf, isInstanceOf, <init>, ==, sizeInBytes, toString, !=, getClass, ne, CatalogFileIndex, eq, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, asInstanceOf, filterPartitions, isInstanceOf, <init>, sizeInBytes, CatalogFileIndex)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF16.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF16.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF16.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF16.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF16.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	UDF16, call.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF16.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(UDF16, call)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, ruleName, catalog, wait, $asInstanceOf, productArity, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, PartitionedRelation, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, isInstanceOf, <init>, apply, ==, clone, $init$, copy, toString, logError, !=, getClass, logWarning, copy$default$1, OptimizeMetadataOnlyQuery, ne, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala: Set(catalog, <init>, OptimizeMetadataOnlyQuery)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOptions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, ParquetOptions, asInstanceOf, synchronized, getParquetCompressionCodecName, $isInstanceOf, compressionCodecClassName, notifyAll, isInstanceOf, <init>, ==, clone, mergeSchema, toString, !=, MERGE_SCHEMA, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(ParquetOptions, asInstanceOf, compressionCodecClassName, isInstanceOf, <init>, ==, mergeSchema, toString, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, resume, wait, isAlive, threadLocalRandomSeed, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, epoch, getContextClassLoader, join, failureReason, localCheckpoint, map, ContinuousDataSourceRDD, productArity, subtract, equals, pipe$default$5, intersection, sortBy$default$3, setPriority, foreachPartition, countApprox$default$2, scope, parkBlocker, threadLocalRandomProbe, asInstanceOf, context, initializeLogIfNecessary, EpochPackedPartitionOffset, subtract$default$3, getPreferredLocations, glom, run, getPriority, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, EpochPollRunnable, repartition$default$2, blockedOn, aggregate, $isInstanceOf, compute, checkAccess, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, suspend, logTrace, DataReaderThread, canEqual, treeAggregate$default$4, isTraceEnabled, getBaseReader, initializeLogIfNecessary$default$2, zipWithUniqueId, getUncaughtExceptionHandler, productPrefix, iterator, getThreadGroup, coalesce$default$4, stop, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getName, getNarrowAncestors, cache, isInterrupted, getNumPartitions, isInstanceOf, getState, filter, getStackTrace, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, destroy, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, setDaemon, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, inheritableThreadLocals, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, setContextClassLoader, elementClassTag, sample, copy, pipe$default$7, threadLocals, toString, mapPartitionsInternal, preferredLocations, isDaemon, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, copy$default$1, setUncaughtExceptionHandler, pipe$default$4, cartesian, repartition, countStackFrames, start, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, getId, randomSplit, top, coalesce$default$2, getCreationSite, threadLocalRandomSecondarySeed, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, interrupt, isReliablyCheckpointed, productIterator, withScope, log, ##, finalize, treeAggregate, productElement, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(name, map, ContinuousDataSourceRDD, asInstanceOf, isInstanceOf, <init>, ==, sparkContext, toString, preferredLocations)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/r/MapPartitionsRWrapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/r/MapPartitionsRWrapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/r/MapPartitionsRWrapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/r/MapPartitionsRWrapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/r/MapPartitionsRWrapper.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, outputSchema, packageNames, wait, copy$default$2, $asInstanceOf, copy$default$5, inputSchema, compose, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, andThen, canEqual, copy$default$4, broadcastVars, productPrefix, notifyAll, isInstanceOf, <init>, apply, MapPartitionsRWrapper, ==, clone, $init$, copy$default$3, func, copy, toString, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(outputSchema, packageNames, inputSchema, asInstanceOf, broadcastVars, isInstanceOf, <init>, apply, MapPartitionsRWrapper, ==, func, copy, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF19.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF19.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF19.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF19.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF19.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF19.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF19.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF19)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, desc, name, wait, copy$default$2, $asInstanceOf, SparkPlanGraph, productArity, equals, toId, asInstanceOf, SparkPlanGraphEdge, synchronized, $isInstanceOf, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, fromId, id, apply, nodes, ==, clone, $init$, makeDotEdge, copy, toString, metrics, !=, SparkPlanGraphCluster, edges, getClass, makeDotFile, copy$default$1, makeDotNode, SparkPlanGraphNode, ne, allNodes, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala: Set(desc, SparkPlanGraph, <init>, id, ==, toString, metrics, !=, makeDotFile, SparkPlanGraphNode, ne, allNodes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala: Set(desc, name, SparkPlanGraph, asInstanceOf, SparkPlanGraphEdge, isInstanceOf, <init>, id, apply, nodes, ==, metrics, !=, SparkPlanGraphCluster, edges, getClass, SparkPlanGraphNode, ne, allNodes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala: Set(desc, name, SparkPlanGraph, asInstanceOf, SparkPlanGraphEdge, isInstanceOf, <init>, id, apply, nodes, ==, toString, metrics, !=, SparkPlanGraphCluster, edges, SparkPlanGraphNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF4.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF4.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF4.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF4.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF4.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF4.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(call, UDF4)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF4.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF4)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, endOffset, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, notifyAll, isInstanceOf, StreamingQueryException, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, cause, startOffset, message, toString, !=, time, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala: Set(StreamingQueryException)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, StreamingQueryException, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(asInstanceOf, getCause, isInstanceOf, StreamingQueryException, <init>, getMessage, ==, cause, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala: Set(StreamingQueryException, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(asInstanceOf, synchronized, isInstanceOf, StreamingQueryException, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unregisterDialect, canHandle, JdbcType, wait, copy$default$2, $asInstanceOf, registerDialect, getJDBCType, productArity, equals, asInstanceOf, JdbcDialect, synchronized, databaseTypeDefinition, $isInstanceOf, escapeSql, getCatalystType, canEqual, getTruncateQuery, productPrefix, notifyAll, isInstanceOf, <init>, quoteIdentifier, ==, clone, $init$, compileValue, copy, JdbcDialects, toString, getSchemaQuery, !=, get, getClass, copy$default$1, ne, beforeFetch, isCascadingTruncateTable, eq, productIterator, ##, finalize, jdbcNullType, productElement, hashCode, getTableExistsQuery.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala: Set(canHandle, JdbcType, registerDialect, asInstanceOf, JdbcDialect, databaseTypeDefinition, escapeSql, isInstanceOf, <init>, ==, compileValue, JdbcDialects, toString, eq, jdbcNullType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala: Set(canHandle, JdbcType, registerDialect, asInstanceOf, JdbcDialect, databaseTypeDefinition, escapeSql, isInstanceOf, <init>, ==, compileValue, JdbcDialects, toString, eq, jdbcNullType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala: Set(canHandle, JdbcType, registerDialect, asInstanceOf, JdbcDialect, databaseTypeDefinition, escapeSql, isInstanceOf, <init>, ==, compileValue, JdbcDialects, toString, eq, jdbcNullType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala: Set(JdbcType, JdbcDialect, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala: Set(equals, JdbcDialect, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala: Set(asInstanceOf, JdbcDialect, isInstanceOf, <init>, quoteIdentifier, ==, compileValue, JdbcDialects, toString, getSchemaQuery, !=, get, beforeFetch)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MsSqlServerDialect.scala: Set(JdbcType, JdbcDialect, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala: Set(JdbcType, asInstanceOf, JdbcDialect, escapeSql, isInstanceOf, <init>, ==, compileValue, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala: Set(JdbcType, asInstanceOf, JdbcDialect, isInstanceOf, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala: Set(JdbcType, getJDBCType, equals, asInstanceOf, JdbcDialect, databaseTypeDefinition, isInstanceOf, <init>, ==, !=, beforeFetch)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/AggregatedDialect.scala: Set(canHandle, JdbcType, getJDBCType, asInstanceOf, JdbcDialect, getCatalystType, getTruncateQuery, isInstanceOf, <init>, quoteIdentifier, ==, getSchemaQuery, isCascadingTruncateTable, getTableExistsQuery)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(JdbcType, getJDBCType, asInstanceOf, JdbcDialect, databaseTypeDefinition, getCatalystType, getTruncateQuery, isInstanceOf, <init>, quoteIdentifier, ==, JdbcDialects, toString, getSchemaQuery, !=, get, getClass, isCascadingTruncateTable, jdbcNullType, getTableExistsQuery)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala: Set(JdbcType, JdbcDialect, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(asInstanceOf, JdbcDialect, isInstanceOf, <init>, ==, JdbcDialects, toString, !=, get, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala: Set(canHandle, JdbcType, registerDialect, asInstanceOf, JdbcDialect, databaseTypeDefinition, escapeSql, isInstanceOf, <init>, ==, compileValue, JdbcDialects, toString, eq, jdbcNullType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala: Set(canHandle, JdbcType, registerDialect, asInstanceOf, JdbcDialect, databaseTypeDefinition, escapeSql, isInstanceOf, <init>, ==, compileValue, JdbcDialects, toString, eq, jdbcNullType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala: Set(canHandle, JdbcType, registerDialect, asInstanceOf, JdbcDialect, databaseTypeDefinition, escapeSql, isInstanceOf, <init>, ==, compileValue, JdbcDialects, toString, eq, jdbcNullType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala: Set(canHandle, JdbcType, registerDialect, asInstanceOf, JdbcDialect, databaseTypeDefinition, escapeSql, isInstanceOf, <init>, ==, compileValue, JdbcDialects, toString, eq, jdbcNullType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala: Set(canHandle, JdbcType, registerDialect, asInstanceOf, JdbcDialect, databaseTypeDefinition, escapeSql, isInstanceOf, <init>, ==, compileValue, JdbcDialects, toString, eq, jdbcNullType)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, commit, getBatch, equals, asInstanceOf, synchronized, $isInstanceOf, stop, notifyAll, isInstanceOf, schema, Source, ==, clone, $init$, toString, getOffset, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, isInstanceOf, schema, Source, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(asInstanceOf, isInstanceOf, schema, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(asInstanceOf, isInstanceOf, schema, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, isInstanceOf, schema, Source, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(getBatch, schema, Source, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(commit, getBatch, asInstanceOf, isInstanceOf, schema, Source, ==, toString, getOffset, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala: Set(getBatch, asInstanceOf, synchronized, isInstanceOf, schema, Source, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, isInstanceOf, schema, Source, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(asInstanceOf, isInstanceOf, schema, Source, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala: Set(Source)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala: Set(asInstanceOf, synchronized, isInstanceOf, schema, Source, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createWriterFactory, DataSourceWriter, abort, commit.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(DataSourceWriter, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(DataSourceWriter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala: Set(createWriterFactory, DataSourceWriter, abort, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(DataSourceWriter, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala: Set(DataSourceWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataSourceWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(DataSourceWriter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(createWriterFactory, DataSourceWriter, abort, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(DataSourceWriter, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(DataSourceWriter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala: Set(commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala: Set(createWriterFactory, DataSourceWriter, abort, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(DataSourceWriter, commit)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/streaming/StreamWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(createWriterFactory, DataSourceWriter, abort, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(DataSourceWriter, commit)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(DataSourceWriter)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(createWriterFactory, DataSourceWriter, abort, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala: Set(createWriterFactory, DataSourceWriter, abort, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(DataSourceWriter, commit)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FailureSafeParser.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FailureSafeParser.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FailureSafeParser.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FailureSafeParser.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FailureSafeParser.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, FailureSafeParser, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, parse, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala: Set(asInstanceOf, FailureSafeParser, isInstanceOf, <init>, parse, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(FailureSafeParser, <init>, parse, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(asInstanceOf, FailureSafeParser, isInstanceOf, <init>, parse, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF20.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF20.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF20.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF20.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF20.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF20.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF20.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF20)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	defaultFormats.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, StateStoreRDD, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(StateStoreRDD, <init>, sparkContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(name, map, asInstanceOf, iterator, StateStoreRDD, conf, isInstanceOf, filter, <init>, max, ++, flatMap, ==, foreach, toString, isEmpty, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, DataFrame, notifyAll, Strategy, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(package, DataFrame, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(package, asInstanceOf, synchronized, DataFrame, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala: Set(package, DataFrame)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(package, wait, asInstanceOf, synchronized, DataFrame, notifyAll, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(package, asInstanceOf, Strategy, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala: Set(package, DataFrame)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(package, asInstanceOf, Strategy, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(package, Strategy, clone, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(package, asInstanceOf, Strategy, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala: Set(package, DataFrame, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(package, asInstanceOf, Strategy, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(package, DataFrame, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala: Set(package, Strategy)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(package, asInstanceOf, DataFrame, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(package, DataFrame, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala: Set(package, asInstanceOf, DataFrame, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala: Set(package, asInstanceOf, synchronized, DataFrame, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(package, asInstanceOf, synchronized, DataFrame, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(package, asInstanceOf, synchronized, DataFrame, isInstanceOf, ==, clone, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(package, asInstanceOf, Strategy, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala: Set(package, DataFrame)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(package, asInstanceOf, Strategy, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala: Set(package, DataFrame)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ExperimentalMethods.scala: Set(package, Strategy)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala: Set(package, DataFrame, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala: Set(package, DataFrame)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala: Set(package, DataFrame)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(package, asInstanceOf, DataFrame, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala: Set(package, asInstanceOf, synchronized, DataFrame, isInstanceOf, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DerbyDialect.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createDataReaderFactories, readSchema, SupportsScanUnsafeRow, createUnsafeRowReaderFactories.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanUnsafeRow.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(createDataReaderFactories, readSchema, SupportsScanUnsafeRow, createUnsafeRowReaderFactories)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, fileCleanupDelayMs, metadataPath, parseVersion, wait, copy$default$2, $asInstanceOf, path, size, copy$default$5, productArity, equals, isDeletingExpiredLog, modificationTime, asInstanceOf, initializeLogIfNecessary, minBatchesToRetain, synchronized, $isInstanceOf, DELETE_ACTION, logTrace, toFileStatus, canEqual, copy$default$4, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, blockSize, productPrefix, logName, notifyAll, purge, isInstanceOf, VERSION, getLatest, <init>, apply, compactLogs, allFiles, defaultCompactInterval, isBatchFile, ==, SinkFileStatus, clone, purgeAfter, batchIdToPath, copy$default$7, compactInterval, $init$, fileManager, copy$default$3, copy, toString, logError, !=, get, getClass, logWarning, copy$default$1, batchFilesFilter, copy$default$6, ne, serialize, isDir, pathToBatchId, add, FileStreamSinkLog, getOrderedBatchFiles, blockReplication, action, ADD_ACTION, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala: Set(path, asInstanceOf, <init>, apply, SinkFileStatus, toString, !=, add, FileStreamSinkLog, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(path, toFileStatus, VERSION, <init>, apply, allFiles, ==, SinkFileStatus, toString, FileStreamSinkLog, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(unapply, path, asInstanceOf, isInstanceOf, VERSION, getLatest, <init>, apply, ==, SinkFileStatus, toString, !=, get, logWarning, FileStreamSinkLog, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	Distribution.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Distribution.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala: Set(Distribution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala: Set(Distribution)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/ClusteredDistribution.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/FileRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/FileRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/FileRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/FileRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/FileRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/FileRelation.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, inputFiles, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, FileRelation, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(inputFiles, asInstanceOf, isInstanceOf, ==, toString, !=, ne, FileRelation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala: Set(inputFiles, asInstanceOf, isInstanceOf, ==, toString, eq, FileRelation)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(asInstanceOf, isInstanceOf, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(asInstanceOf, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(asInstanceOf, isInstanceOf)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, setStoreMetrics, treeString$default$2, unapply, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, apply$default$4, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, outputMode, copy$default$5, resetMetrics, printSchema, operatorId, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, getStateInfo, argString, watermarkPredicateForData, subqueries, executeQuery, StateStoreReader, asInstanceOf, keyExpressions, transformExpressions, initializeLogIfNecessary, doExecute, watermarkPredicateForKeys, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, apply$default$3, doPrepare, <init>$default$4, StatefulOperatorStateInfo, getProgress, logTrace, asCode, canEqual, expressions, canonicalized, stateInfo, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, StreamingDeduplicateExec, StatefulOperator, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, <init>$default$3, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, StateStoreWriter, flatMap, WatermarkSupport, timeTakenMs, executeCollect, StateStoreRestoreExec, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, checkpointLocation, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, removeKeysOlderThanWatermark, queryRunId, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, StateStoreSaveExec, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, eventTimeWatermark, statePrefix, numPartitions, storeVersion, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, watermarkExpression, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, outputMode, map, asInstanceOf, keyExpressions, StatefulOperatorStateInfo, expressions, stateInfo, StreamingDeduplicateExec, conf, isInstanceOf, child, <init>, apply, StateStoreRestoreExec, ==, checkpointLocation, sparkContext, StateStoreSaveExec, output, transformAllExpressions, transform, withNewChildren, eventTimeWatermark, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, outputMode, map, asInstanceOf, StatefulOperatorStateInfo, expressions, StreamingDeduplicateExec, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, outputMode, map, asInstanceOf, StatefulOperatorStateInfo, expressions, StreamingDeduplicateExec, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala: Set(map, asInstanceOf, StatefulOperatorStateInfo, expressions, stateInfo, conf, isInstanceOf, <init>, apply, flatMap, ==, foreach, toString, metrics, !=, ne, storeVersion, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala: Set(children, map, asInstanceOf, StatefulOperatorStateInfo, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, StateStoreRestoreExec, ==, sqlContext, toString, StateStoreSaveExec, transformDown, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, outputMode, map, asInstanceOf, keyExpressions, StatefulOperatorStateInfo, expressions, stateInfo, StreamingDeduplicateExec, conf, isInstanceOf, child, <init>, apply, StateStoreRestoreExec, ==, checkpointLocation, sparkContext, StateStoreSaveExec, output, transformAllExpressions, transform, withNewChildren, eventTimeWatermark, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(find, execute, map, asInstanceOf, StatefulOperatorStateInfo, expressions, stateInfo, conf, isInstanceOf, <init>, apply, StateStoreWriter, flatMap, WatermarkSupport, timeTakenMs, ==, sqlContext, newPredicate, foreach, sparkContext, outputPartitioning, toString, metrics, !=, getClass, longMetric, output, ne, eventTimeWatermark, numPartitions, eq, watermarkExpression)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(setStoreMetrics, execute, outputMode, map, getStateInfo, watermarkPredicateForData, asInstanceOf, StatefulOperatorStateInfo, expressions, stateInfo, isInstanceOf, child, <init>, schema, apply, StateStoreWriter, flatMap, WatermarkSupport, ==, sqlContext, toString, metrics, !=, longMetric, output, ne, eventTimeWatermark, numPartitions, eq, watermarkExpression)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(map, asInstanceOf, synchronized, getProgress, conf, isInstanceOf, collectLeaves, <init>, apply, StateStoreWriter, flatMap, ==, sqlContext, copy, toString, metrics, collect, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(operatorId, StatefulOperatorStateInfo, stateInfo, <init>, apply, sqlContext, checkpointLocation, sparkContext, queryRunId, storeVersion)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala: Set(unapply, operatorId, map, asInstanceOf, synchronized, StatefulOperatorStateInfo, expressions, stateInfo, conf, isInstanceOf, <init>, flatMap, ==, foreach, checkpointLocation, queryRunId, toString, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala: Set(find, map, asInstanceOf, StatefulOperatorStateInfo, expressions, stateInfo, outputSet, collectFirst, isInstanceOf, references, <init>, apply, flatMap, WatermarkSupport, ==, sparkContext, toString, ne, eventTimeWatermark, eq, watermarkExpression)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, outputMode, map, asInstanceOf, keyExpressions, StatefulOperatorStateInfo, expressions, stateInfo, StreamingDeduplicateExec, conf, isInstanceOf, child, <init>, apply, StateStoreRestoreExec, ==, checkpointLocation, sparkContext, StateStoreSaveExec, output, transformAllExpressions, transform, withNewChildren, eventTimeWatermark, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, outputMode, map, asInstanceOf, StatefulOperatorStateInfo, expressions, StreamingDeduplicateExec, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala: Set(map, asInstanceOf, StatefulOperatorStateInfo, expressions, stateInfo, conf, isInstanceOf, <init>, apply, flatMap, ==, foreach, toString, metrics, !=, ne, storeVersion, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	recacheByPlan, clearCache, notify, wait, lookupCachedData, copy$default$2, cacheQuery, $asInstanceOf, cacheQuery$default$2, recacheByPath, CachedData, productArity, equals, uncacheQuery, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, cacheQuery$default$3, productPrefix, logName, notifyAll, CacheManager, isInstanceOf, <init>, ==, clone, cachedRepresentation, $init$, copy, useCachedData, toString, logError, !=, getClass, logWarning, copy$default$1, isEmpty, ne, eq, productIterator, log, uncacheQuery$default$2, plan, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala: Set(asInstanceOf, CacheManager, <init>, ==, !=, logWarning, isEmpty, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(lookupCachedData, cacheQuery, CachedData, uncacheQuery, asInstanceOf, CacheManager, isInstanceOf, <init>, ==, cachedRepresentation, copy, toString, !=, isEmpty, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(uncacheQuery, asInstanceOf, CacheManager, isInstanceOf, <init>, ==, toString, !=, getClass, isEmpty, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(clearCache, lookupCachedData, cacheQuery, recacheByPath, CachedData, uncacheQuery, asInstanceOf, CacheManager, <init>, ==, copy, isEmpty, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(asInstanceOf, CacheManager, isInstanceOf, <init>, ==, useCachedData, toString, !=, isEmpty, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(uncacheQuery, asInstanceOf, CacheManager, isInstanceOf, <init>, ==, toString, !=, getClass, logWarning, isEmpty, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala: Set(recacheByPlan, asInstanceOf, CacheManager, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, getBufferedMatches, notify, SortMergeJoinScanner, treeString$default$2, find, produce, parent, simpleString, children, doProduce, findNextOuterJoinRows, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, doConsume, evaluateVariables, copy$default$5, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, left, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, usedInputs, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, leftKeys, inputRDDs, rightKeys, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, condition, schema, SortMergeJoinExec, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, consume, innerChildren, collect, findNextInnerJoinRows, joinType, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, getStreamedRow, copy$default$6, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, executeToIterator, right, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, asInstanceOf, left, expressions, conf, isInstanceOf, leftKeys, rightKeys, <init>, outputOrdering, condition, schema, SortMergeJoinExec, apply, ==, p, sparkContext, outputPartitioning, copy, toString, joinType, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, left, expressions, transformUp, conf, isInstanceOf, leftKeys, rightKeys, <init>, outputOrdering, condition, SortMergeJoinExec, apply, flatMap, ==, foreach, outputPartitioning, toString, joinType, ne, requiredChildOrdering, withNewChildren, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, nodeName, usedInputs, expressions, conf, isInstanceOf, inputRDDs, references, <init>, outputOrdering, schema, SortMergeJoinExec, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF2.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF2.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF2.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF2.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF2.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF2.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF2.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(call, UDF2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF2)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, unapply, find, BroadcastExchangeExec, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, executionContext, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy, mode, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, BroadcastExchangeExec, children, requiredChildDistribution, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, mode, toString, ne, requiredChildOrdering, withNewChildren, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	partitionSchema, notify, refresh, partitionSpec, wait, $asInstanceOf, inferPartitioning, listFiles, equals, inputFiles, asInstanceOf, initializeLogIfNecessary, metadataOpsTimeNs, synchronized, $isInstanceOf, logTrace, hadoopConf, MetadataLogFileIndex, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, rootPaths, leafFiles, <init>, allFiles, ==, clone, sizeInBytes, $init$, leafDirToChildrenFiles, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, asInstanceOf, hadoopConf, MetadataLogFileIndex, isInstanceOf, <init>, allFiles, ==, sizeInBytes, toString, !=, getClass, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, logTrace, hadoopConf, MetadataLogFileIndex, isInstanceOf, <init>, allFiles, ==, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newTaskTempFile, notify, wait, $asInstanceOf, setupJob, equals, newTaskTempFileAbsPath, abortJob, abortTask, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, setupManifestOptions, isInstanceOf, commitJob, <init>, commitTask, ==, clone, $init$, deleteWithJob, toString, logError, !=, getClass, ManifestFileCommitProtocol, logWarning, setupTask, ne, eq, log, onTaskCommit, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, setupManifestOptions, isInstanceOf, <init>, ==, toString, !=, ManifestFileCommitProtocol, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, execute, executeCollectIterator, wait, stats, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, commit, createDataWriter, resetMetrics, printSchema, writer, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, InternalRowDataWriter, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, run, generateTreeString, prepare, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, query, InternalRowDataWriterFactory, validConstraints, WriteToDataSourceV2, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, WriteToDataSourceV2Exec, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, runContinuous, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, constraints, sameResult, foreach, p, jsonFields, resolve, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, DataWritingSparkTask, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, resolveQuoted, statePrefix, eq, waitForSubqueries, productIterator, write, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, abort, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(execute, commit, writer, map, asInstanceOf, run, synchronized, WriteToDataSourceV2, expressions, isInstanceOf, <init>, runContinuous, schema, apply, ==, sparkContext, toString, !=, collect, output, transformAllExpressions, ne, transform, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(writer, map, asInstanceOf, WriteToDataSourceV2, conf, isInstanceOf, <init>, schema, apply, ==, toString, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(execute, commit, writer, map, asInstanceOf, WriteToDataSourceV2, expressions, conf, isInstanceOf, isStreaming, <init>, schema, apply, flatMap, ==, foreach, sparkContext, copy, toString, !=, collect, logWarning, output, transformAllExpressions, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(writer, asInstanceOf, query, WriteToDataSourceV2, WriteToDataSourceV2Exec, isInstanceOf, <init>, output)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getInterval, notify, ArrowColumnVector, getShort, wait, getUTF8String, getBooleans, equals, hasNull, isNullAt, getDouble, getStruct, getFloats, getMap, notifyAll, getByte, getArray, <init>, getInts, getDoubles, getFloat, getDecimal, getInt, getBytes, getShorts, getLongs, toString, getBoolean, numNulls, getClass, dataType, close, getChild, getLong, hashCode, getBinary.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala: Set(ArrowColumnVector, <init>, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala: Set(ArrowColumnVector, <init>, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RuntimeConfig.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, getAll, equals, unset, asInstanceOf, set, synchronized, $isInstanceOf, getOption, notifyAll, RuntimeConfig, isInstanceOf, <init>, ==, clone, toString, !=, get, getClass, contains, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(getAll, set, RuntimeConfig, <init>, toString, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(asInstanceOf, set, RuntimeConfig, isInstanceOf, <init>, ==, toString, !=, get, contains, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala: Set(asInstanceOf, set, getOption, RuntimeConfig, isInstanceOf, <init>, ==, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(asInstanceOf, RuntimeConfig, isInstanceOf, <init>, ==, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(getAll, asInstanceOf, set, synchronized, RuntimeConfig, isInstanceOf, <init>, ==, clone, toString, get, contains, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala: Set(getAll, equals, asInstanceOf, set, getOption, RuntimeConfig, isInstanceOf, <init>, ==, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, RuntimeConfig, isInstanceOf, <init>, ==, toString, !=, get, getClass, contains, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(getAll, asInstanceOf, set, RuntimeConfig, isInstanceOf, <init>, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/VariableSubstitution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/VariableSubstitution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/VariableSubstitution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/VariableSubstitution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/VariableSubstitution.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, substitute, toString, !=, getClass, ne, VariableSubstitution, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, substitute, !=, ne, VariableSubstitution)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	closeCurrentPage, notify, getPeakMemoryUsedBytes, wait, equals, insertKV, cleanupResources, getKey, sortedIterator, access$200, getSpillSize, notifyAll, <init>, merge, access$100, this$0, next, toString, UnsafeKVExternalSorter$KVSorterIterator, getClass, getValue, UnsafeKVExternalSorter, close, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala: Set(insertKV, getKey, sortedIterator, <init>, next, getValue, UnsafeKVExternalSorter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(getPeakMemoryUsedBytes, equals, getKey, sortedIterator, getSpillSize, <init>, merge, next, toString, getValue, UnsafeKVExternalSorter, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala: Set(getPeakMemoryUsedBytes, equals, getKey, sortedIterator, <init>, merge, next, getValue, UnsafeKVExternalSorter, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeKVExternalSorter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala: Set(insertKV, getKey, <init>, next, getValue, UnsafeKVExternalSorter)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ConsoleRelation, wait, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, unhandledFilters, data, synchronized, $isInstanceOf, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, schema, ConsoleSinkProvider, ==, sqlContext, clone, sizeInBytes, $init$, copy, toString, createRelation, !=, getClass, copy$default$1, shortName, ne, createStreamWriter, eq, productIterator, ##, finalize, productElement, hashCode, needConversion.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, produce, parent, simpleString, children, doProduce, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, projections, copy$default$2, $asInstanceOf, numberedTreeString, doConsume, evaluateVariables, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, ExpandExec, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, usedInputs, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, inputRDDs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, consume, innerChildren, collect, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, projections, map, ExpandExec, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, AlterViewAsCommand, treeString$default$2, find, simpleString, children, refresh, name, maxRowsPerPartition, verboseString, semanticHash, wait, stats, replace, ViewHelper, copy$default$2, $asInstanceOf, numberedTreeString, copy$default$5, originalText, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, copy$default$9, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, query, copy$default$8, checkCyclicViewReference, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, LocalTempView, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, generateViewProperties, isStreaming, doCanonicalize, viewType, collectLeaves, references, ViewType, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, properties, apply, flatMap, userSpecifiedColumns, resolved, ==, producedAttributes, fastEquals, PersistedView, origin, transformExpressionsUp, clone, allowExisting, comment, constraints, sameResult, foreach, p, jsonFields, copy$default$7, resolve, $init$, CreateViewCommand, copy$default$3, copy, inputSet, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, GlobalTempView, mapExpressions, copy$default$6, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(find, simpleString, children, name, replace, map, treeString, asInstanceOf, expressions, outputSet, transformUp, conf, LocalTempView, isInstanceOf, isStreaming, viewType, ViewType, <init>, mapChildren, schema, apply, flatMap, ==, foreach, resolve, CreateViewCommand, copy, toString, !=, collect, output, GlobalTempView, ne, transform, resolveQuoted)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(AlterViewAsCommand, children, name, map, asInstanceOf, query, expressions, conf, LocalTempView, isInstanceOf, viewType, ViewType, <init>, schema, properties, apply, flatMap, userSpecifiedColumns, ==, PersistedView, comment, foreach, CreateViewCommand, !=, collect, logWarning, GlobalTempView, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, StateStoreConf, wait, $asInstanceOf, empty, equals, minVersionsToRetain, asInstanceOf, synchronized, $isInstanceOf, minDeltasForSnapshot, notifyAll, isInstanceOf, <init>, apply, ==, clone, toString, !=, getClass, providerClass, ne, eq, ##, finalize, hashCode, confs.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala: Set(StateStoreConf, empty, minVersionsToRetain, asInstanceOf, synchronized, minDeltasForSnapshot, isInstanceOf, <init>, apply, ==, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala: Set(StateStoreConf, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala: Set(StateStoreConf, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(StateStoreConf, empty, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala: Set(StateStoreConf, asInstanceOf, synchronized, isInstanceOf, <init>, ==, toString, !=, providerClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverWrapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverWrapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverWrapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverWrapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverWrapper.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, connect, equals, wrapped, asInstanceOf, synchronized, $isInstanceOf, notifyAll, DriverWrapper, isInstanceOf, jdbcCompliant, <init>, ==, clone, getParentLogger, getMinorVersion, toString, getMajorVersion, !=, getClass, acceptsURL, ne, eq, ##, finalize, hashCode, getPropertyInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(connect, wrapped, asInstanceOf, DriverWrapper, isInstanceOf, <init>, ==, toString, !=, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry.scala: Set(asInstanceOf, synchronized, DriverWrapper, <init>, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, register, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, UDFRegistration, isInstanceOf, <init>, registerJavaUDAF, ==, registerJava, clone, $init$, registerPython, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(UDFRegistration, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(UDFRegistration, <init>, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, UDFRegistration, isInstanceOf, <init>, ==, clone, toString, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(UDFRegistration, <init>, clone, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ruleName, wait, $asInstanceOf, canEvaluateWithinJoin, equals, ExtractPythonUDFs, canEvaluate, asInstanceOf, initializeLogIfNecessary, synchronized, splitDisjunctivePredicates, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, splitConjunctivePredicates, logName, notifyAll, isInstanceOf, ExtractPythonUDFFromAggregate, apply, ==, clone, $init$, toString, logError, !=, getClass, logWarning, replaceAlias, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(ExtractPythonUDFs, asInstanceOf, isInstanceOf, apply, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala: Set(ExtractPythonUDFFromAggregate)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, supportBatch, isSplitable, wait, $asInstanceOf, JsonFileFormat, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, buildReaderWithPartitionValues, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, inferSchema, logName, notifyAll, isInstanceOf, prepareWrite, <init>, JsonOutputWriter, ==, clone, $init$, toString, logError, !=, getClass, logWarning, shortName, close, ne, vectorTypes, buildReader, eq, write, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(JsonFileFormat, asInstanceOf, inferSchema, isInstanceOf, <init>, ==, toString, !=, getClass, logWarning, shortName, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(JsonFileFormat, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, ACTIVE, awaitInitialization, curried, runUninterruptibly, name, resume, wait, isAlive, INITIALIZING, threadLocalRandomSeed, currentBatchId, copy$default$2, $asInstanceOf, setName, outputMode, explain, getContextClassLoader, join, state, TERMINATED, productArity, equals, inputRows, setPriority, reportTimeTaken, parkBlocker, threadLocalRandomProbe, isInterruptionException, asInstanceOf, initializeLogIfNecessary, run, committedOffsets, getPriority, finishTrigger, streamDeathCause, synchronized, blockedOn, queryExecutionThread, $isInstanceOf, exception, streamMetadata, streamMetrics, checkAccess, tupled, awaitProgressLockCondition, RECONFIGURING, updateStatusMessage, suspend, logTrace, canEqual, ExecutionStats, lastExecution, isTraceEnabled, initializeLogIfNecessary$default$2, triggerClock, offsetSeqMetadata, getUncaughtExceptionHandler, productPrefix, getThreadGroup, stop, logName, notifyAll, getName, pollingDelayMs, postEvent, isInterrupted, isInstanceOf, startTrigger, awaitOffset, getState, getStackTrace, sink, StreamExecution, runActivatedStream, <init>, offsetLog, resolvedCheckpointRoot, id, destroy, eventTimeStats, apply, lastProgress, noNewData, processAllAvailable, stateOperators, ==, clone, status, setDaemon, minLogEntriesToMaintain, sparkSession, inheritableThreadLocals, prettyIdString, $init$, getBatchDescriptionString, isActive, recentProgress, setContextClassLoader, availableOffsets, copy$default$3, copy, threadLocals, toString, awaitTermination, isDaemon, logError, QueryExecutionThread, !=, commitLog, getClass, logWarning, stopSources, copy$default$1, setUncaughtExceptionHandler, State, countStackFrames, start, uniqueSources, checkpointFile, ne, sources, getId, threadLocalRandomSecondarySeed, newData, runId, watermarkMsMap, eq, interrupt, QUERY_ID_KEY, productIterator, log, logicalPlan, ##, finalize, productElement, hashCode, logDebug, logInfo, explainInternal, currentStatus, trigger, awaitProgressLock.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, outputMode, state, asInstanceOf, synchronized, exception, triggerClock, notifyAll, isInstanceOf, sink, StreamExecution, <init>, id, apply, ==, sparkSession, toString, !=, logWarning, start, ne, sources, runId, trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, outputMode, state, asInstanceOf, synchronized, exception, triggerClock, notifyAll, isInstanceOf, sink, StreamExecution, <init>, id, apply, ==, sparkSession, toString, !=, logWarning, start, ne, sources, runId, trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(name, currentBatchId, outputMode, reportTimeTaken, asInstanceOf, committedOffsets, finishTrigger, queryExecutionThread, awaitProgressLockCondition, updateStatusMessage, lastExecution, triggerClock, offsetSeqMetadata, pollingDelayMs, isInstanceOf, startTrigger, sink, StreamExecution, <init>, offsetLog, resolvedCheckpointRoot, eventTimeStats, apply, ==, minLogEntriesToMaintain, sparkSession, getBatchDescriptionString, isActive, availableOffsets, copy, toString, QueryExecutionThread, !=, commitLog, logWarning, start, uniqueSources, checkpointFile, ne, sources, newData, runId, watermarkMsMap, eq, logicalPlan, logDebug, logInfo, currentStatus, trigger, awaitProgressLock)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala: Set(name, explain, exception, stop, StreamExecution, <init>, id, lastProgress, processAllAvailable, ==, status, sparkSession, isActive, recentProgress, awaitTermination, runId, explainInternal)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala: Set(name, synchronized, StreamExecution, <init>, apply, lastProgress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(unapply, isInterruptionException, asInstanceOf, run, isInterrupted, isInstanceOf, StreamExecution, <init>, apply, ==, toString, logError, ne, sources, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(ACTIVE, name, isAlive, currentBatchId, outputMode, join, state, reportTimeTaken, asInstanceOf, run, committedOffsets, streamDeathCause, synchronized, queryExecutionThread, awaitProgressLockCondition, RECONFIGURING, lastExecution, triggerClock, offsetSeqMetadata, stop, isInstanceOf, startTrigger, sink, StreamExecution, <init>, offsetLog, resolvedCheckpointRoot, apply, ==, setDaemon, minLogEntriesToMaintain, sparkSession, prettyIdString, isActive, toString, QueryExecutionThread, !=, commitLog, stopSources, State, start, checkpointFile, ne, sources, runId, interrupt, logicalPlan, logDebug, logInfo, trigger, awaitProgressLock)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, outputMode, state, asInstanceOf, synchronized, exception, triggerClock, notifyAll, isInstanceOf, sink, StreamExecution, <init>, id, apply, ==, sparkSession, toString, !=, logWarning, start, ne, sources, runId, trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, id, apply, ==, toString, ne, sources, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(unapply, isInterruptionException, asInstanceOf, run, isInterrupted, isInstanceOf, StreamExecution, <init>, apply, ==, toString, logError, ne, sources, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(asInstanceOf, run, isInterrupted, isInstanceOf, <init>, apply, ==, setDaemon, copy, toString, !=, getClass, start, ne, sources, eq, interrupt, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, sources)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, MetricsReporter, asInstanceOf, initializeLogIfNecessary, metricRegistry, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, sourceName, isInstanceOf, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(MetricsReporter, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, supportBatch, isSplitable, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, buildReaderWithPartitionValues, TextBasedFileFormat, inferSchema, notifyAll, isInstanceOf, prepareWrite, <init>, FileFormat, ==, clone, $init$, toString, !=, getClass, ne, vectorTypes, buildReader, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(asInstanceOf, isInstanceOf, <init>, FileFormat, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, isInstanceOf, <init>, FileFormat, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(asInstanceOf, isInstanceOf, prepareWrite, <init>, FileFormat, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(supportBatch, isSplitable, asInstanceOf, buildReaderWithPartitionValues, isInstanceOf, <init>, FileFormat, ==, toString, getClass, ne, vectorTypes, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(isSplitable, TextBasedFileFormat, inferSchema, isInstanceOf, <init>, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(isSplitable, TextBasedFileFormat, inferSchema, isInstanceOf, <init>, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala: Set(isSplitable, TextBasedFileFormat, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, FileFormat, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, inferSchema, isInstanceOf, <init>, FileFormat, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(supportBatch, asInstanceOf, isInstanceOf, <init>, FileFormat, ==, toString, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(supportBatch, asInstanceOf, isInstanceOf, <init>, FileFormat, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala: Set(asInstanceOf, isInstanceOf, <init>, FileFormat, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(asInstanceOf, isInstanceOf, <init>, FileFormat)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, inferSchema, isInstanceOf, <init>, FileFormat, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, inferSchema, isInstanceOf, <init>, FileFormat, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(isSplitable, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(isSplitable, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, inferSchema, isInstanceOf, <init>, FileFormat, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(supportBatch, isSplitable, asInstanceOf, buildReaderWithPartitionValues, isInstanceOf, <init>, FileFormat, ==, toString, getClass, ne, vectorTypes, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, inferSchema, isInstanceOf, <init>, FileFormat, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, numFiles, getFinalStats, wait, copy$default$2, $asInstanceOf, productArity, equals, newPartition, asInstanceOf, initializeLogIfNecessary, synchronized, BasicWriteTaskStatsTracker, $isInstanceOf, newFile, logTrace, canEqual, newRow, copy$default$4, isTraceEnabled, newBucket, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, isInstanceOf, <init>, newTaskInstance, BasicWriteJobStatsTracker, ==, clone, $init$, copy$default$3, copy, toString, metrics, logError, !=, getClass, logWarning, copy$default$1, numBytes, ne, BasicWriteTaskStats, numPartitions, eq, productIterator, processStats, log, numRows, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala: Set(<init>, BasicWriteJobStatsTracker, ==, metrics, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, BasicWriteJobStatsTracker, ==, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, isInstanceOf, <init>, BasicWriteJobStatsTracker, ==, toString, metrics, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, JdbcRelationProvider, notifyAll, isInstanceOf, <init>, ==, clone, toString, createRelation, !=, getClass, shortName, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, JdbcRelationProvider, isInstanceOf, <init>, ==, toString, createRelation, !=, getClass, shortName, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, canEvaluateWithinJoin, verboseStringWithSuffix, equals, treeString, schemaString, canEvaluate, StreamingRelationStrategy, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, splitDisjunctivePredicates, generateTreeString$default$6, FlatMapGroupsWithStateStrategy, allAttributes, nodeName, $isInstanceOf, doPrepare, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, splitConjunctivePredicates, productPrefix, JoinSelection, logName, notifyAll, prunePlans, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, StatefulAggregationStrategy, stringArgs, StreamingDeduplicationStrategy, doCanonicalize, SpecialLimits, collectLeaves, SparkStrategies, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, PlanLater, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, Aggregation, newPredicate, sameResult, foreach, p, jsonFields, planLater, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, singleRowRdd, strategies, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, SparkStrategy, longMetric, logWarning, output, copy$default$1, replaceAlias, BasicOperators, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, InMemoryScans, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, plan, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, StreamingJoinStrategy, logInfo, collectPlaceholders.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(find, execute, map, asInstanceOf, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, flatMap, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, getClass, longMetric, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, expressions, JoinSelection, conf, isInstanceOf, SpecialLimits, SparkStrategies, references, <init>, apply, flatMap, ==, PlanLater, Aggregation, sparkContext, collect, SparkStrategy, BasicOperators, InMemoryScans, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(map, conf, <init>, apply, clone, foreach, sparkContext, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(map, asInstanceOf, expressions, outputSet, transformUp, splitConjunctivePredicates, isInstanceOf, references, <init>, mapChildren, apply, flatMap, ==, p, output, ne, transform, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, SparkStrategy, output, ne, transform, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(asInstanceOf, isInstanceOf, <init>, planLater, SparkStrategy, output, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(execute, map, StreamingRelationStrategy, asInstanceOf, FlatMapGroupsWithStateStrategy, expressions, conf, isInstanceOf, StatefulAggregationStrategy, StreamingDeduplicationStrategy, <init>, apply, ==, sparkContext, strategies, SparkStrategy, output, transformAllExpressions, transform, withNewChildren, plan, StreamingJoinStrategy, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, expressions, JoinSelection, conf, isInstanceOf, SpecialLimits, SparkStrategies, references, <init>, apply, flatMap, ==, PlanLater, Aggregation, sparkContext, collect, SparkStrategy, BasicOperators, InMemoryScans, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(find, simpleString, map, asInstanceOf, expressions, isInstanceOf, references, <init>, apply, ==, SparkStrategy, output, transform, plan, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala: Set(<init>, SparkStrategy)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(conf, <init>, apply, ==, foreach, sparkContext, !=, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(map, schemaString, asInstanceOf, synchronized, expressions, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, clone, foreach, sparkContext, copy, toString, logWarning, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, expressions, JoinSelection, conf, isInstanceOf, SpecialLimits, SparkStrategies, references, <init>, apply, flatMap, ==, PlanLater, Aggregation, sparkContext, collect, SparkStrategy, BasicOperators, InMemoryScans, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(execute, map, asInstanceOf, synchronized, expressions, isInstanceOf, <init>, schema, apply, ==, sparkContext, toString, !=, collect, output, transformAllExpressions, ne, transform, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, apply, executeCollect, ==, foreach, sparkContext, copy, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(simpleString, map, asInstanceOf, nodeName, expressions, isInstanceOf, <init>, schema, apply, ==, sqlContext, sparkContext, toString, metrics, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(execute, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, sparkContext, copy, toString, !=, collect, logWarning, output, transformAllExpressions, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(map, conf, <init>, schema, apply, flatMap, foreach, sparkContext, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, toString, !=, collect, getClass, output, ne, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(map, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, p, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala: Set(<init>, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(find, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, ==, foreach, toString, executeTake, !=, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, map, asInstanceOf, synchronized, notifyAll, conf, isInstanceOf, <init>, apply, ==, foreach, sparkContext, toString, !=, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, SparkStrategy, output, ne, transform, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala: Set(conf, <init>, apply, flatMap)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala: Set(simpleString, map, asInstanceOf, conf, <init>, schema, apply, flatMap, ==, toString, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(find, simpleString, children, execute, resetMetrics, map, treeString, asInstanceOf, expressions, outputSet, transformUp, conf, isInstanceOf, <init>, outputOrdering, mapChildren, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, outputPartitioning, copy, toString, !=, collect, output, ne, transform, executeToIterator, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(simpleString, map, asInstanceOf, nodeName, expressions, isInstanceOf, <init>, schema, apply, ==, sqlContext, sparkContext, toString, metrics, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(map, conf, <init>, apply, clone, foreach, sparkContext, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(execute, map, StreamingRelationStrategy, asInstanceOf, FlatMapGroupsWithStateStrategy, expressions, conf, isInstanceOf, StatefulAggregationStrategy, StreamingDeduplicationStrategy, <init>, apply, ==, sparkContext, strategies, SparkStrategy, output, transformAllExpressions, transform, withNewChildren, plan, StreamingJoinStrategy, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(map, expressions, conf, <init>, schema, apply, sqlContext, foreach, sparkContext, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala: Set(simpleString, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, toString, collect, getClass, output, ne, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(simpleString, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, getClass, output, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(find, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, toString, metrics, !=, logWarning, output, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(simpleString, map, asInstanceOf, expressions, <init>, schema, apply, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(map, asInstanceOf, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, ==, sparkContext, outputPartitioning, toString, metrics, longMetric, output, transform, eq, productElement)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(map, asInstanceOf, conf, <init>, schema, apply, ==, copy, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(find, execute, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, sqlContext, newPredicate, foreach, sparkContext, outputPartitioning, toString, metrics, !=, getClass, longMetric, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, apply, executeCollect, ==, foreach, sparkContext, copy, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(find, simpleString, children, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, <init>, schema, apply, ==, transformExpressionsUp, foreach, toString, !=, collect, output, ne, transform, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala: Set(map, logTrace, conf, <init>, apply, ==, foreach, toString, !=, logWarning, output, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, sparkContext, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(find, execute, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, outputOrdering, apply, flatMap, ==, foreach, sparkContext, copy, toString, logError, !=, output, ne, eq, plan, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(find, execute, map, asInstanceOf, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, flatMap, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, getClass, longMetric, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, apply, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(schemaString, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, p, toString, collect, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, ==, sqlContext, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala: Set(map, asInstanceOf, logTrace, expressions, conf, isInstanceOf, references, <init>, apply, flatMap, ==, toString, ne, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala: Set(map, expressions, conf, isInstanceOf, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, ==, p)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(execute, map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, toString, metrics, !=, longMetric, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(map, asInstanceOf, synchronized, conf, isInstanceOf, collectLeaves, <init>, apply, flatMap, ==, sqlContext, copy, toString, metrics, collect, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(conf, isInstanceOf, <init>, apply, ==, foreach, sparkContext, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(map, schemaString, asInstanceOf, synchronized, expressions, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, clone, foreach, sparkContext, copy, toString, logWarning, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(conf, isInstanceOf, <init>, apply, ==, foreach, sparkContext, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(map, conf, <init>, apply, ==, foreach, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala: Set(map, conf, <init>, apply, flatMap, p, toString, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, p, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(find, map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, sparkContext, toString, !=, getClass, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, foreach, p, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, toString, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala: Set(map, equals, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, toString, !=, logWarning, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(find, simpleString, map, asInstanceOf, expressions, isInstanceOf, references, <init>, apply, ==, SparkStrategy, output, transform, plan, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(simpleString, execute, map, treeString, asInstanceOf, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(find, execute, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, sqlContext, foreach, outputPartitioning, toString, metrics, longMetric, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(find, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, p, toString, !=, collect, getClass, logWarning, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(map, treeString, schemaString, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, sparkContext, toString, collect, getClass, logWarning, ne, eq, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(map, asInstanceOf, logTrace, conf, isInstanceOf, <init>, apply, flatMap, ==, foreach, sparkContext, toString, metrics, !=, collect, getClass, logWarning, output, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, foreach, sparkContext, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(map, schemaString, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(<init>, apply, sqlContext, sparkContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(find, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, ==, sameResult, foreach, toString, logWarning, output, transformDown, transformAllExpressions, ne, eq, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, foreach, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, flatMap, ==, sparkContext, toString, collect, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala: Set(map, conf, <init>, apply, ==, foreach, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, foreach, sparkContext, toString, !=, collect, getClass, logWarning, output, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(find, simpleString, map, schemaString, executeQuery, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, <init>, schema, apply, ==, foreach, p, toString, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(find, map, asInstanceOf, expressions, isInstanceOf, references, <init>, apply, output, transformDown, transform, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, apply, ==, foreach, sparkContext, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala: Set(find, map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, foreach, sparkContext, toString, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, sparkContext, toString, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, ==, toString, !=, collect, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, StreamingRelationStrategy, asInstanceOf, FlatMapGroupsWithStateStrategy, expressions, JoinSelection, conf, isInstanceOf, StatefulAggregationStrategy, StreamingDeduplicationStrategy, SpecialLimits, SparkStrategies, <init>, outputOrdering, schema, apply, ==, PlanLater, Aggregation, p, planLater, sparkContext, outputPartitioning, copy, toString, singleRowRdd, SparkStrategy, output, BasicOperators, ne, InMemoryScans, eq, plan, StreamingJoinStrategy)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(map, conf, <init>, apply, clone, foreach, sparkContext, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(execute, map, StreamingRelationStrategy, asInstanceOf, FlatMapGroupsWithStateStrategy, expressions, conf, isInstanceOf, StatefulAggregationStrategy, StreamingDeduplicationStrategy, <init>, apply, ==, sparkContext, strategies, SparkStrategy, output, transformAllExpressions, transform, withNewChildren, plan, StreamingJoinStrategy, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(conf, <init>, apply, ==, foreach, sparkContext, !=, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(simpleString, execute, map, treeString, asInstanceOf, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, expressions, JoinSelection, conf, isInstanceOf, SpecialLimits, SparkStrategies, references, <init>, apply, flatMap, ==, PlanLater, Aggregation, sparkContext, collect, SparkStrategy, BasicOperators, InMemoryScans, plan)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	to_utc_timestamp, notify, sha1, var_samp, encode, from_utc_timestamp, lead, desc, coalesce, window, log2, count, hex, var_pop, substring_index, ltrim, factorial, stddev_samp, soundex, from_json, wait, sumDistinct, months_between, crc32, $asInstanceOf, rtrim, covar_samp, sinh, size, array_contains, base64, toDegrees, desc_nulls_last, asc_nulls_last, levenshtein, input_file_name, variance, map, degrees, typedLit, expm1, collect_list, unix_timestamp, equals, asin, unhex, date_sub, initcap, collect_set, covar_pop, from_unixtime, regexp_replace, monotonicallyIncreasingId, minute, unboundedFollowing, asInstanceOf, grouping, sort_array, to_timestamp, kurtosis, lag, sha2, rank, synchronized, log10, shiftRight, avg, callUDF, concat, format_string, $isInstanceOf, mean, current_timestamp, min, instr, year, get_json_object, posexplode_outer, lit, expr, radians, lower, ntile, stddev_pop, date_trunc, cos, decode, unboundedPreceding, quarter, upper, approxCountDistinct, last, exp, add_months, asc, notifyAll, hypot, last_day, cbrt, hour, atan, json_tuple, pow, ceil, least, ascii, isInstanceOf, rint, reverse, dayofmonth, array, shiftRightUnsigned, percent_rank, hash, second, row_number, log1p, nanvl, md5, max, randn, currentRow, atan2, sqrt, explode, broadcast, greatest, pmod, map_values, ==, col, stddev, floor, shiftLeft, date_add, negate, desc_nulls_first, dayofyear, split, countDistinct, cume_dist, clone, dayofweek, bitwiseNOT, format_number, map_keys, udf, not, conv, rand, translate, toRadians, next_day, tanh, first, struct, lpad, column, toString, weekofyear, round, length, bin, spark_partition_id, cosh, !=, dense_rank, functions, skewness, concat_ws, rpad, getClass, posexplode, isnan, substring, repeat, bround, date_format, regexp_extract, trim, tan, abs, grouping_id, sin, ne, current_date, to_json, monotonically_increasing_id, asc_nulls_first, when, approx_count_distinct, datediff, eq, sum, log, trunc, isnull, explode_outer, locate, signum, ##, acos, finalize, hashCode, month, unbase64, to_date, corr.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala: Set(window, map, equals, asInstanceOf, lit, expr, isInstanceOf, ==, col, struct, toString, length, functions, substring, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(count, map, asInstanceOf, ==, toString, length, !=, functions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala: Set(count, size, map, asInstanceOf, expr, isInstanceOf, sqrt, ==, toString, length, functions, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala: Set(map, asInstanceOf, isInstanceOf, ==, col, udf, rand, functions, corr)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala: Set(asInstanceOf, isInstanceOf, ==, toString, length, functions, trim)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala: Set(coalesce, size, map, asInstanceOf, lit, expr, isInstanceOf, nanvl, ==, col, functions, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	DataSourceV2.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceV2.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Utils.scala: Set(DataSourceV2)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/SessionConfigSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ExperimentalMethods.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ExperimentalMethods.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ExperimentalMethods.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ExperimentalMethods.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/ExperimentalMethods.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, extraOptimizations, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, extraStrategies, <init>, ==, clone, toString, !=, getClass, ExperimentalMethods, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(<init>, clone, ExperimentalMethods, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ExperimentalMethods)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(<init>, toString, ExperimentalMethods, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(<init>, ==, !=, ExperimentalMethods, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, ExperimentalMethods, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(asInstanceOf, isInstanceOf, extraStrategies, <init>, ==, ExperimentalMethods)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala: Set(extraOptimizations, <init>, ExperimentalMethods)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createDataReaderFactories, readSchema, SupportsReportStatistics, getStatistics.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala: Set(readSchema, SupportsReportStatistics, getStatistics)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, columnStats, synchronized, isWorthCompressing, $isInstanceOf, build, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, initialize$default$3, logName, notifyAll, initialize, isInstanceOf, ==, clone, compressionEncoders, $init$, appendFrom, unaligned, toString, initialize$default$2, logError, !=, getClass, logWarning, CompressibleColumnBuilder, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(isWorthCompressing, ==, compressionEncoders, unaligned, CompressibleColumnBuilder, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(asInstanceOf, columnStats, initialize, isInstanceOf, ==, CompressibleColumnBuilder)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala: Set(asInstanceOf, columnStats, build, isInstanceOf, ==, appendFrom, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(isWorthCompressing, ==, compressionEncoders, unaligned, CompressibleColumnBuilder, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala: Set(columnStats)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(asInstanceOf, columnStats, initialize, isInstanceOf, ==, CompressibleColumnBuilder)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, readerFactory, wait, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, zip, localCheckpoint, map, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, treeAggregate$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, persist, checkpointData, <init>, isCheckpointed, id, DataSourceRDDPartition, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, DataSourceRDD, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, elementClassTag, sample, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, pipe$default$4, cartesian, repartition, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, withScope, log, ##, finalize, treeAggregate, index, hashCode, takeOrdered, logDebug, firstParent, logInfo, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(readerFactory, map, asInstanceOf, context, conf, isInstanceOf, <init>, DataSourceRDDPartition, ==, foreach, zipWithIndex, toString, preferredLocations, !=, getClass, ne, eq, index, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(name, map, asInstanceOf, isInstanceOf, <init>, ==, sparkContext, DataSourceRDD, toString, preferredLocations)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF10.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF10.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF10.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF10.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF10.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF10.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF10.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(call, UDF10)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF10)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, toDS, DatasetHolder, wait, $asInstanceOf, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, toDF, ==, clone, $init$, copy, toString, !=, getClass, copy$default$1, ne, ds$1, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala: Set(DatasetHolder, asInstanceOf, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, sideEffectResult, verboseString, semanticHash, DataWritingCommandExec, execute, executeCollectIterator, wait, ExplainCommand, stats, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, extended, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, run, generateTreeString, prepare, childrenResolved, RunnableCommand, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, codegen, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, cmd, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, ExecutedCommandExec, child, isStreaming, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, StreamingExplainCommand, cost, flatMap, resolved, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, constraints, sameResult, foreach, p, jsonFields, resolve, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, queryExecution, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, resolveQuoted, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, logicalPlan, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(find, simpleString, children, execute, ExplainCommand, extended, resetMetrics, map, treeString, asInstanceOf, codegen, expressions, outputSet, transformUp, conf, isInstanceOf, isStreaming, <init>, outputOrdering, mapChildren, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, resolve, outputPartitioning, copy, toString, !=, collect, output, queryExecution, ne, transform, resolveQuoted, executeToIterator, logicalPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala: Set(stats, asInstanceOf, RunnableCommand, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala: Set(children, map, asInstanceOf, RunnableCommand, expressions, conf, isInstanceOf, child, <init>, schema, apply, flatMap, resolved, ==, foreach, toString, !=, collect, getClass, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(find, stats, map, asInstanceOf, RunnableCommand, expressions, conf, isInstanceOf, <init>, apply, ==, foreach, toString, executeTake, !=, output, ne, eq, logicalPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala: Set(asInstanceOf, RunnableCommand, isInstanceOf, <init>, ==, foreach, toString, eq, logicalPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, DataWritingCommandExec, stats, map, asInstanceOf, RunnableCommand, expressions, conf, isInstanceOf, ExecutedCommandExec, child, isStreaming, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(find, simpleString, children, execute, ExplainCommand, extended, resetMetrics, map, treeString, asInstanceOf, codegen, expressions, outputSet, transformUp, conf, isInstanceOf, isStreaming, <init>, outputOrdering, mapChildren, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, resolve, outputPartitioning, copy, toString, !=, collect, output, queryExecution, ne, transform, resolveQuoted, executeToIterator, logicalPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(simpleString, stats, map, asInstanceOf, RunnableCommand, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, p, resolve, toString, !=, getClass, output, queryExecution, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(map, asInstanceOf, RunnableCommand, isInstanceOf, <init>, ==, toString, logError, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(extended, map, asInstanceOf, conf, isInstanceOf, <init>, apply, StreamingExplainCommand, executeCollect, ==, foreach, sparkContext, copy, toString, logError, !=, logWarning, ne, eq, logicalPlan, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala: Set(map, asInstanceOf, RunnableCommand, expressions, isInstanceOf, <init>, apply, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(stats, map, asInstanceOf, run, RunnableCommand, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, p, toString, collect, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(map, asInstanceOf, run, RunnableCommand, expressions, conf, isInstanceOf, child, <init>, schema, ==, sqlContext, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala: Set(asInstanceOf, RunnableCommand, isInstanceOf, <init>, apply, ==, sqlContext, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala: Set(map, equals, asInstanceOf, RunnableCommand, expressions, conf, isInstanceOf, <init>, schema, apply, ==, toString, !=, logWarning, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(simpleString, execute, stats, map, treeString, asInstanceOf, run, RunnableCommand, conf, cmd, isInstanceOf, ExecutedCommandExec, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala: Set(map, asInstanceOf, RunnableCommand, expressions, isInstanceOf, <init>, schema, apply, flatMap, ==, sparkContext, toString, collect, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(stats, extended, map, asInstanceOf, RunnableCommand, expressions, conf, cmd, collectFirst, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, foreach, sparkContext, toString, !=, collect, getClass, logWarning, output, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala: Set(asInstanceOf, RunnableCommand, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(map, asInstanceOf, RunnableCommand, expressions, isInstanceOf, <init>, apply, resolved, ==, toString, eq, logicalPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala: Set(map, asInstanceOf, RunnableCommand, expressions, isInstanceOf, <init>, schema, apply, ==, toString, !=, collect, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(map, expressions, conf, isStreaming, <init>, schema, apply, sqlContext, foreach, sparkContext, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(simpleString, execute, stats, map, treeString, asInstanceOf, run, RunnableCommand, conf, cmd, isInstanceOf, ExecutedCommandExec, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, foreach, sparkContext, toString, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(find, stats, map, asInstanceOf, run, expressions, conf, cmd, isInstanceOf, <init>, schema, apply, flatMap, resolved, ==, sqlContext, p, toString, !=, collect, getClass, logWarning, output, queryExecution, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(simpleString, stats, map, asInstanceOf, RunnableCommand, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, p, resolve, toString, !=, getClass, output, queryExecution, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(refresh, map, asInstanceOf, conf, <init>, schema, apply, ==, copy, queryExecution, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(find, simpleString, children, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, <init>, schema, apply, resolved, ==, transformExpressionsUp, foreach, resolve, toString, !=, collect, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(schemaString, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(map, asInstanceOf, run, RunnableCommand, expressions, conf, isInstanceOf, child, <init>, schema, ==, sqlContext, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(find, map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, sparkContext, toString, !=, getClass, output, ne, logicalPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(refresh, map, asInstanceOf, run, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, foreach, p, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, toString, collect, queryExecution, ne, logicalPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(map, schemaString, asInstanceOf, conf, isInstanceOf, isStreaming, <init>, schema, apply, flatMap, ==, foreach, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(map, asInstanceOf, RunnableCommand, expressions, isInstanceOf, <init>, apply, resolved, ==, toString, eq, logicalPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(refresh, map, asInstanceOf, conf, <init>, schema, apply, ==, copy, queryExecution, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(find, simpleString, children, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, <init>, schema, apply, resolved, ==, transformExpressionsUp, foreach, resolve, toString, !=, collect, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, toString, collect, queryExecution, ne, logicalPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, ExplainCommand, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	next, get, DataReader, close.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(next, get, DataReader, close)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(next, get, DataReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala: Set(next, get, DataReader, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(next, get, DataReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/DataReaderFactory.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(get, DataReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(next, get, DataReader, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(next, get, DataReader, close)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, unapply, find, produce, parent, simpleString, children, finishAggregate, doProduce, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, doConsume, evaluateVariables, copy$default$5, resetMetrics, printSchema, map, productArity, resultExpressions, verboseStringWithSuffix, equals, getTaskMemoryManager, treeString, schemaString, argString, subqueries, executeQuery, requiredChildDistributionExpressions, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, usedInputs, HashAggregateExec, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, getEmptyAggregationBuffer, child, inputRDDs, doCanonicalize, supportsAggregate, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, groupingExpressions, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, initialInputBufferOffset, p, jsonFields, copy$default$7, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, createUnsafeJoiner, aggregateExpressions, consume, innerChildren, aggregateAttributes, collect, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, copy$default$6, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, createHashMap, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala: Set(children, map, resultExpressions, requiredChildDistributionExpressions, asInstanceOf, HashAggregateExec, expressions, conf, isInstanceOf, child, supportsAggregate, <init>, groupingExpressions, apply, flatMap, ==, sqlContext, initialInputBufferOffset, toString, aggregateExpressions, aggregateAttributes, transformDown, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, nodeName, usedInputs, HashAggregateExec, expressions, conf, isInstanceOf, child, inputRDDs, references, <init>, outputOrdering, schema, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, wait, stats, replace, functionName, copy$default$2, $asInstanceOf, numberedTreeString, copy$default$5, pattern, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, DropFunctionCommand, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, db, allAttributes, nodeName, $isInstanceOf, ignoreIfExists, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, showUserFunctions, references, <init>, DescribeFunctionCommand, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, className, p, jsonFields, copy$default$7, resolve, ifExists, $init$, ShowFunctionsCommand, resources, copy$default$3, copy, inputSet, showSystemFunctions, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, databaseName, logWarning, output, copy$default$1, isExtended, transformDown, transformAllExpressions, mapExpressions, isTemp, copy$default$6, ne, transform, withNewChildren, resolveQuoted, statePrefix, CreateFunctionCommand, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, functionName, pattern, map, DropFunctionCommand, asInstanceOf, db, expressions, conf, isInstanceOf, <init>, DescribeFunctionCommand, schema, apply, flatMap, ==, foreach, className, ShowFunctionsCommand, resources, !=, collect, logWarning, isExtended, ne, CreateFunctionCommand)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newByteEncoder, newScalaDecimalEncoder, newBoxedDoubleEncoder, newBoxedFloatEncoder, newByteArrayEncoder, newIntArrayEncoder, newBooleanArrayEncoder, newFloatEncoder, newMapEncoder, newProductEncoder, newDoubleArrayEncoder, newBoxedLongEncoder, newBoxedShortEncoder, newSequenceEncoder, newIntEncoder, newShortArrayEncoder, newJavaDecimalEncoder, newStringEncoder, newBooleanEncoder, StringToColumn, newLongArrayEncoder, newShortEncoder, newFloatArrayEncoder, newBoxedIntEncoder, localSeqToDatasetHolder, newBoxedBooleanEncoder, newDoubleEncoder, newTimeStampEncoder, symbolToColumn, newLongEncoder, newDateEncoder, newProductArrayEncoder, rddToDatasetHolder, newBoxedByteEncoder, newSetEncoder, newStringArrayEncoder.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSessionExtensions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriteStatsTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriteStatsTracker.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriteStatsTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriteStatsTracker.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriteStatsTracker.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/WriteStatsTracker.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getFinalStats, wait, $asInstanceOf, equals, newPartition, asInstanceOf, synchronized, $isInstanceOf, newFile, newRow, newBucket, WriteTaskStats, WriteTaskStatsTracker, notifyAll, isInstanceOf, newTaskInstance, ==, clone, WriteJobStatsTracker, toString, !=, getClass, ne, eq, processStats, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala: Set(asInstanceOf, WriteTaskStats, WriteTaskStatsTracker, isInstanceOf, ==, WriteJobStatsTracker, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(getFinalStats, newPartition, asInstanceOf, newFile, newRow, newBucket, WriteTaskStats, WriteTaskStatsTracker, isInstanceOf, newTaskInstance, ==, WriteJobStatsTracker, toString, !=, ne, eq, processStats)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(asInstanceOf, isInstanceOf, ==, WriteJobStatsTracker, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, isInstanceOf, ==, WriteJobStatsTracker, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala: Set(==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(asInstanceOf, isInstanceOf, ==, WriteJobStatsTracker, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, isInstanceOf, ==, WriteJobStatsTracker, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/SaveMode.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/SaveMode.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/SaveMode.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/SaveMode.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/SaveMode.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, Overwrite, name, wait, valueOf, Append, equals, getDeclaringClass, Ignore, notifyAll, compareTo, ErrorIfExists, ordinal, values, toString, SaveMode, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(name, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/SaveMode.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(name, values, toString, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(toString, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(name, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(name, toString, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala: Set(toString, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(toString, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(name, toString, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala: Set(toString, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(name, toString, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(name, Append, toString, SaveMode, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala: Set(SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala: Set(SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(toString, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(toString, SaveMode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getInterval, notify, getShort, wait, getUTF8String, getBooleans, equals, hasNull, isNullAt, getDouble, ColumnVector, getStruct, getFloats, getMap, notifyAll, getByte, getArray, <init>, getInts, getDoubles, getFloat, getDecimal, getInt, getBytes, getShorts, getLongs, toString, getBoolean, numNulls, getClass, dataType, type, close, getChild, getLong, hashCode, getBinary.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala: Set(ColumnVector, <init>, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala: Set(ColumnVector, <init>, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala: Set(ColumnVector, <init>, toString, dataType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala: Set(ColumnVector, <init>, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala: Set(ColumnVector, <init>, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(ColumnVector, <init>, toString, dataType, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala: Set(ColumnVector, <init>, dataType)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(<init>, toString, getBoolean, getClass, dataType, close, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(ColumnVector, <init>, toString, dataType, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala: Set(<init>, dataType)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala: Set(getShort, isNullAt, getByte, <init>, getInt, toString, dataType, getLong)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala: Set(getInt)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(ColumnVector, <init>, toString, dataType, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala: Set(<init>, getInt)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala: Set(<init>, dataType)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(<init>, toString, getBoolean, getClass, dataType, close, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(ColumnVector, <init>, toString, dataType, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/HiveSerDe.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/HiveSerDe.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/HiveSerDe.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/HiveSerDe.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/HiveSerDe.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	serdeMap, notify, unapply, wait, copy$default$2, $asInstanceOf, <init>$default$1, productArity, equals, serde, asInstanceOf, getDefaultStorage, synchronized, $isInstanceOf, apply$default$3, canEqual, productPrefix, notifyAll, HiveSerDe, isInstanceOf, <init>$default$3, <init>, apply$default$2, apply, ==, outputFormat, clone, $init$, copy$default$3, copy, apply$default$1, toString, !=, inputFormat, getClass, copy$default$1, sourceToSerDe, ne, <init>$default$2, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(unapply, serde, asInstanceOf, HiveSerDe, isInstanceOf, <init>, apply, ==, toString, !=, getClass, sourceToSerDe, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(serde, asInstanceOf, getDefaultStorage, HiveSerDe, isInstanceOf, <init>, apply, ==, outputFormat, !=, inputFormat, sourceToSerDe, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF13.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF13.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF13.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF13.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF13.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	UDF13, call.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(UDF13, call)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF13.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, getCurrentRecord, asInstanceOf, synchronized, $isInstanceOf, ParquetRecordMaterializer, notifyAll, isInstanceOf, <init>, skipCurrentRecord, ==, getRootConverter, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala: Set(asInstanceOf, ParquetRecordMaterializer, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, outputSchema, packageNames, treeString$default$2, unapply, find, produce, parent, wrapObjectToRow, simpleString, children, doProduce, verboseString, semanticHash, execute, executeCollectIterator, wait, MapGroupsExec, groupingAttributes, requiredChildDistribution, CoGroupExec, SerializeFromObjectExec, copy$default$2, $asInstanceOf, unwrapObjectFromRow, numberedTreeString, doConsume, evaluateVariables, copy$default$5, inputSchema, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, outputObjectType, schemaString, copy$default$9, argString, ObjectOperator, FlatMapGroupsInRExec, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, rightAttr, dataAttributes, metricTerm, synchronized, left, deserializeRowToObject, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, leftGroup, doPrepare, copy$default$8, AppendColumnsWithObjectExec, usedInputs, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, broadcastVars, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, MapPartitionsExec, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, ObjectProducerExec, stringArgs, AppendColumnsExec, child, inputRDDs, doCanonicalize, MapElementsExec, valueDeserializer, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, deserializer, schema, serializer, transformExpressionsDown, serializeObjectToRow, prettyJson, inputSerializer, rightGroup, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, leftAttr, jsonFields, copy$default$7, subexpressionEliminationEnabled, sparkContext, copy$default$10, outputPartitioning, $init$, ObjectConsumerExec, copy$default$3, func, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, newColumnsSerializer, inputObjectType, consume, innerChildren, collect, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, outputObjAttr, mapExpressions, copy$default$6, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, DeserializeToObjectExec, copy$default$11, statePrefix, keyDeserializer, eq, waitForSubqueries, leftDeserializer, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, executeToIterator, right, ##, containsChild, newOrdering, finalize, rightDeserializer, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(wrapObjectToRow, execute, groupingAttributes, map, ObjectOperator, asInstanceOf, dataAttributes, deserializeRowToObject, expressions, isInstanceOf, ObjectProducerExec, child, valueDeserializer, <init>, deserializer, schema, serializeObjectToRow, apply, flatMap, ==, sqlContext, func, toString, metrics, !=, longMetric, output, outputObjAttr, ne, keyDeserializer, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(outputSchema, packageNames, unapply, children, MapGroupsExec, groupingAttributes, CoGroupExec, SerializeFromObjectExec, inputSchema, map, FlatMapGroupsInRExec, asInstanceOf, rightAttr, dataAttributes, left, leftGroup, AppendColumnsWithObjectExec, expressions, broadcastVars, MapPartitionsExec, conf, isInstanceOf, AppendColumnsExec, child, MapElementsExec, valueDeserializer, <init>, outputOrdering, deserializer, schema, serializer, rightGroup, apply, ==, p, leftAttr, sparkContext, outputPartitioning, func, copy, toString, newColumnsSerializer, output, outputObjAttr, ne, DeserializeToObjectExec, keyDeserializer, eq, leftDeserializer, right, rightDeserializer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(wrapObjectToRow, map, ObjectOperator, asInstanceOf, nodeName, expressions, conf, isInstanceOf, ObjectProducerExec, <init>, outputOrdering, schema, apply, ==, sparkContext, outputPartitioning, toString, metrics, longMetric, output, outputObjAttr, transform, eq, productElement)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, left, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, withNewChildren, right, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(outputSchema, packageNames, unapply, children, MapGroupsExec, groupingAttributes, CoGroupExec, SerializeFromObjectExec, inputSchema, map, FlatMapGroupsInRExec, asInstanceOf, rightAttr, dataAttributes, left, leftGroup, AppendColumnsWithObjectExec, expressions, broadcastVars, MapPartitionsExec, conf, isInstanceOf, AppendColumnsExec, child, MapElementsExec, valueDeserializer, <init>, outputOrdering, deserializer, schema, serializer, rightGroup, apply, ==, p, leftAttr, sparkContext, outputPartitioning, func, copy, toString, newColumnsSerializer, output, outputObjAttr, ne, DeserializeToObjectExec, keyDeserializer, eq, leftDeserializer, right, rightDeserializer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(outputSchema, packageNames, unapply, children, MapGroupsExec, groupingAttributes, CoGroupExec, SerializeFromObjectExec, inputSchema, map, FlatMapGroupsInRExec, asInstanceOf, rightAttr, dataAttributes, left, leftGroup, AppendColumnsWithObjectExec, expressions, broadcastVars, MapPartitionsExec, conf, isInstanceOf, AppendColumnsExec, child, MapElementsExec, valueDeserializer, <init>, outputOrdering, deserializer, schema, serializer, rightGroup, apply, ==, p, leftAttr, sparkContext, outputPartitioning, func, copy, toString, newColumnsSerializer, output, outputObjAttr, ne, DeserializeToObjectExec, keyDeserializer, eq, leftDeserializer, right, rightDeserializer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(packageNames, unapply, find, simpleString, children, execute, resetMetrics, map, treeString, asInstanceOf, left, expressions, broadcastVars, outputSet, transformUp, conf, isInstanceOf, <init>, outputOrdering, mapChildren, deserializer, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, outputPartitioning, func, copy, toString, !=, collect, output, ne, transform, executeToIterator, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(unapply, map, schemaString, asInstanceOf, synchronized, expressions, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, clone, foreach, sparkContext, copy, toString, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, nodeName, usedInputs, expressions, conf, isInstanceOf, child, inputRDDs, references, <init>, outputOrdering, schema, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $assertionsDisabled, equals, OrcColumnarBatchReader, getProgress, initBatch, notifyAll, initialize, getCurrentValue, <init>, nextKeyValue, toString, getCurrentKey, getClass, close, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(OrcColumnarBatchReader, initBatch, initialize, <init>, getClass, close, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, ShowDatabasesCommand, children, refresh, maxRowsPerPartition, verboseString, semanticHash, wait, stats, $asInstanceOf, numberedTreeString, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy, inputSet, toString, SetDatabaseCommand, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, databaseName, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, databasePattern, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(ShowDatabasesCommand, children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, SetDatabaseCommand, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	noLiveData, onExecutorUnblacklisted, notify, wait, $asInstanceOf, onJobEnd, onApplicationEnd, equals, onTaskEnd, onStageCompleted, onTaskGettingResult, asInstanceOf, onJobStart, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, onOtherEvent, isTraceEnabled, initializeLogIfNecessary$default$2, onTaskStart, logName, notifyAll, onExecutorRemoved, onExecutorAdded, isInstanceOf, <init>, onBlockUpdated, ==, onBlockManagerRemoved, liveExecutionMetrics, clone, onExecutorMetricsUpdate, onEnvironmentUpdate, onBlockManagerAdded, $init$, onNodeBlacklisted, onApplicationStart, onSpeculativeTaskSubmitted, toString, SQLAppStatusListener, logError, !=, getClass, logWarning, onExecutorBlacklisted, onUnpersistRDD, ne, onStageSubmitted, eq, onNodeUnblacklisted, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala: Set(<init>, SQLAppStatusListener)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, liveExecutionMetrics, toString, SQLAppStatusListener, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala: Set(asInstanceOf, <init>, ==, SQLAppStatusListener, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	supportedCustomMetrics, notify, MAINTENANCE_INTERVAL_CONFIG, unapply, numKeys, desc, name, UnsafeRowPair, wait, apply$default$4, copy$default$2, $asInstanceOf, DEFAULT_STORE_NAME, commit, StateStoreMetrics, operatorId, getRange, StateStoreProviderId, productArity, equals, storeCheckpointLocation, isMaintenanceRunning, StateStoreCustomTimingMetric, stateStoreId, asInstanceOf, initializeLogIfNecessary, doMaintenance, synchronized, StateStoreId, storeId, $isInstanceOf, StateStoreCustomSizeMetric, create, <init>$default$4, StateStoreProvider, logTrace, canEqual, copy$default$4, isTraceEnabled, StateStoreCustomMetric, initializeLogIfNecessary$default$2, productPrefix, iterator, stop, logName, notifyAll, unload, partitionId, key, isInstanceOf, MAINTENANCE_INTERVAL_DEFAULT_SECS, version, <init>, MaintenanceTask, createAndInit, id, remove, apply, hasCommitted, ==, checkpointRootLocation, clone, $init$, withRows, combine, isRunning, queryRunId, copy$default$3, copy, put, toString, storeName, metrics, getStore, logError, StateStore, !=, get, getClass, logWarning, customMetrics, copy$default$1, close, ne, memoryUsedBytes, init, value, eq, productIterator, log, ##, finalize, productElement, hashCode, abort, logDebug, isLoaded, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala: Set(unapply, UnsafeRowPair, StateStoreMetrics, operatorId, storeCheckpointLocation, stateStoreId, asInstanceOf, synchronized, StateStoreId, create, StateStoreProvider, StateStoreCustomMetric, iterator, partitionId, key, isInstanceOf, version, <init>, id, remove, apply, ==, withRows, copy, put, toString, StateStore, !=, get, logWarning, close, value, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala: Set(operatorId, StateStoreProviderId, StateStoreId, iterator, <init>, apply, queryRunId, StateStore, get, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala: Set(numKeys, desc, UnsafeRowPair, commit, StateStoreMetrics, getRange, StateStoreProviderId, StateStoreCustomTimingMetric, asInstanceOf, StateStoreId, StateStoreCustomSizeMetric, create, StateStoreCustomMetric, iterator, key, isInstanceOf, <init>, id, remove, apply, hasCommitted, ==, withRows, put, toString, metrics, StateStore, !=, get, customMetrics, ne, memoryUsedBytes, value, eq, abort, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(numKeys, name, UnsafeRowPair, commit, StateStoreMetrics, asInstanceOf, create, StateStoreCustomMetric, key, isInstanceOf, <init>, apply, ==, combine, toString, metrics, !=, get, getClass, customMetrics, ne, memoryUsedBytes, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(UnsafeRowPair, commit, getRange, asInstanceOf, key, isInstanceOf, <init>, remove, apply, ==, put, toString, metrics, StateStore, !=, get, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(supportedCustomMetrics, numKeys, desc, name, UnsafeRowPair, commit, StateStoreMetrics, operatorId, getRange, StateStoreCustomTimingMetric, asInstanceOf, StateStoreCustomSizeMetric, create, StateStoreProvider, StateStoreCustomMetric, iterator, key, isInstanceOf, <init>, remove, apply, ==, queryRunId, put, toString, metrics, StateStore, get, customMetrics, ne, memoryUsedBytes, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(operatorId, <init>, apply, hasCommitted, queryRunId, StateStore, get, abort)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala: Set(desc, StateStoreProviderId, asInstanceOf, StateStoreProvider, isInstanceOf, <init>, apply, ==, toString, storeName, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator.scala: Set(StateStoreProviderId, asInstanceOf, synchronized, storeId, stop, isInstanceOf, <init>, id, apply, ==, queryRunId, put, toString, get, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, isSplitable, wait, TextInputCSVDataSource, $asInstanceOf, infer, equals, asInstanceOf, synchronized, $isInstanceOf, inferSchema, notifyAll, readFile, MultiLineCSVDataSource, isInstanceOf, <init>, apply, ==, clone, toString, !=, inferFromDataset, getClass, CSVDataSource, ne, makeSafeHeader, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(isSplitable, inferSchema, readFile, isInstanceOf, <init>, apply, ==, !=, getClass, CSVDataSource, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(TextInputCSVDataSource, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, inferFromDataset, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, format, wait, $asInstanceOf, equals, json, asInstanceOf, initializeLogIfNecessary, orc, parquet, csv, textFile, synchronized, option, $isInstanceOf, load, text, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, options, <init>, schema, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, DataStreamReader, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(format, json, parquet, option, load, options, <init>, schema, toString, ne, DataStreamReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, isInstanceOf, options, <init>, schema, ==, clone, toString, logWarning, ne, DataStreamReader)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/TeradataDialect.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, register, register$default$2, name, wait, $asInstanceOf, SQLMetrics, countFailedValues, equals, createTimingMetric, isZero, metricType, +=, asInstanceOf, register$default$3, set, synchronized, $isInstanceOf, createAverageMetric, SQLMetric, copyAndReset, toInfo, notifyAll, postDriverMetricUpdates, isInstanceOf, <init>, merge, id, setDoubleForAverageMetrics, stringValue, ==, clone, createMetric, isAtDriverSide, writeReplace, copy, reset, metadata, toString, createSizeMetric, !=, isRegistered, getClass, ne, add, value, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala: Set(SQLMetrics, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, createMetric, copy, toString, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala: Set(+=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, copy, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala: Set(SQLMetrics, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, createMetric, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala: Set(SQLMetrics, asInstanceOf, createAverageMetric, SQLMetric, isInstanceOf, <init>, ==, createMetric, toString, getClass, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala: Set(SQLMetrics, createTimingMetric, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, createMetric, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala: Set(SQLMetrics, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, createMetric, toString, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala: Set(name, +=, asInstanceOf, synchronized, SQLMetric, isInstanceOf, <init>, ==, reset, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala: Set(SQLMetrics, createTimingMetric, +=, asInstanceOf, createAverageMetric, SQLMetric, isInstanceOf, <init>, ==, createMetric, toString, createSizeMetric, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/BasicWriteStatsTracker.scala: Set(SQLMetrics, asInstanceOf, SQLMetric, postDriverMetricUpdates, isInstanceOf, <init>, ==, createMetric, toString, !=, add, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(SQLMetrics, equals, createTimingMetric, +=, asInstanceOf, set, createAverageMetric, SQLMetric, isInstanceOf, <init>, merge, ==, createMetric, copy, toString, createSizeMetric, !=, ne, add, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala: Set(SQLMetrics, createTimingMetric, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, toString, createSizeMetric, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala: Set(name, metricType, asInstanceOf, SQLMetric, isInstanceOf, <init>, id, ==, metadata, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(name, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(SQLMetrics, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, createMetric, toString, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(name, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, metadata, toString, !=, getClass, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala: Set(asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, copy, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(name, SQLMetrics, createTimingMetric, +=, asInstanceOf, SQLMetric, postDriverMetricUpdates, isInstanceOf, <init>, ==, createMetric, metadata, toString, getClass, ne, add, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala: Set(equals, +=, asInstanceOf, set, SQLMetric, isInstanceOf, <init>, merge, ==, copy, !=, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala: Set(SQLMetrics, +=, asInstanceOf, set, SQLMetric, isInstanceOf, <init>, ==, createMetric, copy, toString, !=, getClass, ne, add, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala: Set(SQLMetrics, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, copy, toString, createSizeMetric, value, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(+=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, metadata, toString, !=, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(name, +=, asInstanceOf, synchronized, SQLMetric, isInstanceOf, <init>, id, ==, copy, toString, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala: Set(SQLMetrics, +=, asInstanceOf, set, SQLMetric, isInstanceOf, <init>, ==, createMetric, copy, toString, !=, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala: Set(name, SQLMetrics, metricType, asInstanceOf, toInfo, isInstanceOf, <init>, id, stringValue, ==, !=, getClass, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(name, SQLMetrics, createTimingMetric, +=, asInstanceOf, set, SQLMetric, isInstanceOf, <init>, id, ==, toString, !=, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator.scala: Set(+=, SQLMetric, <init>, ==, copy)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(name, SQLMetrics, createTimingMetric, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, createMetric, metadata, toString, createSizeMetric, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala: Set(name, SQLMetrics, +=, asInstanceOf, SQLMetric, postDriverMetricUpdates, isInstanceOf, <init>, id, ==, createMetric, toString, !=, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/UnsafeRowSerializer.scala: Set(asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, !=, add, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(name, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, toString, ne, add, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala: Set(+=, asInstanceOf, set, SQLMetric, isInstanceOf, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala: Set(name, SQLMetrics, createTimingMetric, SQLMetric, <init>, createMetric, ne, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala: Set(SQLMetric, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala: Set(SQLMetrics, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, createMetric, toString, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala: Set(name, SQLMetrics, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, createMetric, toString, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala: Set(SQLMetrics, +=, asInstanceOf, SQLMetric, postDriverMetricUpdates, isInstanceOf, <init>, ==, createMetric, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(name, +=, asInstanceOf, SQLMetric, isInstanceOf, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	decodeToBinary, notify, wait, equals, decodeToDouble, decodeToFloat, ParquetDictionary, notifyAll, <init>, toString, getClass, decodeToLong, decodeToInt, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetDictionary.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	decodeToBinary, notify, wait, equals, decodeToDouble, decodeToFloat, notifyAll, ColumnDictionary, <init>, toString, getClass, decodeToLong, decodeToInt, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala: Set(ColumnDictionary, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/columnar/ColumnDictionary.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, unapply, ruleName, find, produce, parent, simpleString, children, doProduce, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, $asInstanceOf, numberedTreeString, doConsume, evaluateVariables, InputAdapter, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, generatedClassName, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, CodegenSupport, resetPerQuery, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, usedInputs, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, codegenStageId, inputRDDs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, getNextStageId, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, CollapseCodegenStages, flatMap, PIPELINE_DURATION_METRIC, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, WholeStageCodegenExec, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, WholeStageCodegenId, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, consume, innerChildren, collect, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, isTooManyFields, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo, doCodeGen.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, InputAdapter, map, treeString, executeQuery, asInstanceOf, generatedClassName, doExecute, generateTreeString, CodegenSupport, resetPerQuery, nodeName, usedInputs, expressions, conf, isInstanceOf, child, codegenStageId, inputRDDs, references, getNextStageId, <init>, outputOrdering, schema, apply, CollapseCodegenStages, PIPELINE_DURATION_METRIC, ==, sqlContext, WholeStageCodegenExec, foreach, p, sparkContext, outputPartitioning, WholeStageCodegenId, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, isTooManyFields, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo, doCodeGen)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, toString, transformDown, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, InputAdapter, map, treeString, executeQuery, asInstanceOf, generatedClassName, doExecute, generateTreeString, CodegenSupport, resetPerQuery, nodeName, usedInputs, expressions, conf, isInstanceOf, child, codegenStageId, inputRDDs, references, getNextStageId, <init>, outputOrdering, schema, apply, CollapseCodegenStages, PIPELINE_DURATION_METRIC, ==, sqlContext, WholeStageCodegenExec, foreach, p, sparkContext, outputPartitioning, WholeStageCodegenId, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, isTooManyFields, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo, doCodeGen)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(find, execute, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, sparkContext, copy, toString, logError, !=, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, find, simpleString, children, execute, resetMetrics, map, treeString, asInstanceOf, expressions, outputSet, transformUp, conf, isInstanceOf, <init>, outputOrdering, mapChildren, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, outputPartitioning, copy, toString, !=, collect, output, ne, transform, executeToIterator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(unapply, map, schemaString, asInstanceOf, synchronized, expressions, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, clone, foreach, sparkContext, copy, toString, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, InputAdapter, map, treeString, executeQuery, asInstanceOf, generatedClassName, doExecute, generateTreeString, CodegenSupport, resetPerQuery, nodeName, usedInputs, expressions, conf, isInstanceOf, child, codegenStageId, inputRDDs, references, getNextStageId, <init>, outputOrdering, schema, apply, CollapseCodegenStages, PIPELINE_DURATION_METRIC, ==, sqlContext, WholeStageCodegenExec, foreach, p, sparkContext, outputPartitioning, WholeStageCodegenId, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, isTooManyFields, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo, doCodeGen)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, InputAdapter, map, treeString, executeQuery, asInstanceOf, generatedClassName, doExecute, generateTreeString, CodegenSupport, resetPerQuery, nodeName, usedInputs, expressions, conf, isInstanceOf, child, codegenStageId, inputRDDs, references, getNextStageId, <init>, outputOrdering, schema, apply, CollapseCodegenStages, PIPELINE_DURATION_METRIC, ==, sqlContext, WholeStageCodegenExec, foreach, p, sparkContext, outputPartitioning, WholeStageCodegenId, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, isTooManyFields, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo, doCodeGen)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala: Set(simpleString, children, map, subqueries, asInstanceOf, nodeName, isInstanceOf, child, <init>, apply, ==, metrics, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, find, simpleString, map, asInstanceOf, CodegenSupport, expressions, isInstanceOf, references, <init>, apply, ==, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, InputAdapter, map, treeString, executeQuery, asInstanceOf, generatedClassName, doExecute, generateTreeString, CodegenSupport, resetPerQuery, nodeName, usedInputs, expressions, conf, isInstanceOf, child, codegenStageId, inputRDDs, references, getNextStageId, <init>, outputOrdering, schema, apply, CollapseCodegenStages, PIPELINE_DURATION_METRIC, ==, sqlContext, WholeStageCodegenExec, foreach, p, sparkContext, outputPartitioning, WholeStageCodegenId, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, isTooManyFields, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo, doCodeGen)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, withNewChildren, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala: Set(parent, simpleString, children, map, asInstanceOf, nodeName, isInstanceOf, <init>, apply, flatMap, PIPELINE_DURATION_METRIC, ==, WholeStageCodegenExec, foreach, toString, metrics, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala: Set(produce, execute, map, asInstanceOf, metricTerm, CodegenSupport, expressions, isInstanceOf, inputRDDs, references, <init>, apply, ==, sparkContext, toString, evaluateRequiredVariables, metrics, consume, getClass, longMetric, output, needCopyResult, executeBroadcast, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(produce, execute, evaluateVariables, map, equals, asInstanceOf, metricTerm, CodegenSupport, allAttributes, expressions, conf, isInstanceOf, child, inputRDDs, <init>, apply, flatMap, ==, sqlContext, newMutableProjection, subexpressionEliminationEnabled, sparkContext, outputPartitioning, copy, inputSet, toString, metrics, !=, consume, longMetric, logWarning, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala: Set(produce, execute, asInstanceOf, metricTerm, CodegenSupport, expressions, conf, isInstanceOf, child, inputRDDs, <init>, schema, apply, ==, sqlContext, sparkContext, outputPartitioning, toString, metrics, consume, longMetric, output, eq, newOrdering)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala: Set(produce, execute, map, asInstanceOf, CodegenSupport, expressions, isInstanceOf, child, inputRDDs, <init>, outputOrdering, apply, ==, outputPartitioning, copy, toString, executeTake, !=, consume, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(find, execute, map, asInstanceOf, metricTerm, CodegenSupport, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, flatMap, ==, sqlContext, WholeStageCodegenExec, foreach, p, sparkContext, outputPartitioning, toString, metrics, consume, getClass, longMetric, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala: Set(newNaturalAscendingOrdering, unapply, execute, evaluateVariables, map, asInstanceOf, metricTerm, CodegenSupport, expressions, conf, isInstanceOf, child, references, <init>, outputOrdering, apply, ==, sqlContext, newPredicate, sparkContext, outputPartitioning, copy, toString, metrics, !=, consume, getClass, longMetric, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, find, simpleString, map, asInstanceOf, CodegenSupport, expressions, isInstanceOf, references, <init>, apply, ==, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, simpleString, execute, map, treeString, asInstanceOf, conf, isInstanceOf, <init>, apply, CollapseCodegenStages, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala: Set(produce, parent, children, execute, map, asInstanceOf, metricTerm, CodegenSupport, expressions, canonicalized, outputSet, isInstanceOf, child, inputRDDs, references, <init>, outputOrdering, apply, flatMap, executeCollect, ==, sqlContext, newPredicate, subexpressionEliminationEnabled, sparkContext, outputPartitioning, toString, evaluateRequiredVariables, metrics, !=, consume, collect, longMetric, output, needStopCheck, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(unapply, execute, map, asInstanceOf, expressions, canonicalized, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, flatMap, ==, sqlContext, WholeStageCodegenExec, newPredicate, foreach, p, sparkContext, outputPartitioning, toString, innerChildren, longMetric, output, isTooManyFields, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala: Set(parent, simpleString, map, metricTerm, CodegenSupport, expressions, <init>, apply, sparkContext, consume, output, needStopCheck, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala: Set(produce, execute, evaluateVariables, map, asInstanceOf, metricTerm, CodegenSupport, expressions, isInstanceOf, child, inputRDDs, references, <init>, apply, flatMap, ==, foreach, p, sparkContext, toString, metrics, consume, longMetric, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala: Set(produce, execute, map, asInstanceOf, metricTerm, CodegenSupport, expressions, outputSet, isInstanceOf, child, inputRDDs, <init>, apply, flatMap, ==, sparkContext, outputPartitioning, toString, metrics, consume, longMetric, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala: Set(produce, execute, map, asInstanceOf, CodegenSupport, expressions, outputSet, isInstanceOf, child, inputRDDs, <init>, outputOrdering, schema, apply, flatMap, ==, outputPartitioning, toString, consume, collect, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(execute, map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, ==, sqlContext, WholeStageCodegenExec, sparkContext, outputPartitioning, toString, longMetric, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala: Set(produce, simpleString, execute, map, asInstanceOf, CodegenSupport, expressions, isInstanceOf, child, inputRDDs, <init>, apply, ==, WholeStageCodegenExec, foreach, sparkContext, outputPartitioning, toString, !=, consume, getClass, output, ne, transform, eq, doCodeGen)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala: Set(children, execute, map, executeQuery, asInstanceOf, doExecute, prepare, synchronized, nodeName, doPrepare, expressions, outputSet, makeCopy, conf, isInstanceOf, child, <init>, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, p, subexpressionEliminationEnabled, sparkContext, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, !=, collect, logWarning, ne, waitForSubqueries, log, doExecuteBroadcast, newOrdering)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, references, <init>, apply, flatMap, ==, sparkContext, collect)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, find, simpleString, map, asInstanceOf, CodegenSupport, expressions, isInstanceOf, references, <init>, apply, ==, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala: Set(find, simpleString, map, subqueries, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, schema, apply, executeCollect, ==, sameResult, toString, transformAllExpressions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(unapply, execute, map, asInstanceOf, expressions, canonicalized, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, flatMap, ==, sqlContext, WholeStageCodegenExec, newPredicate, foreach, p, sparkContext, outputPartitioning, toString, innerChildren, longMetric, output, isTooManyFields, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(find, execute, map, asInstanceOf, metricTerm, CodegenSupport, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, flatMap, ==, sqlContext, WholeStageCodegenExec, foreach, p, sparkContext, outputPartitioning, toString, metrics, consume, getClass, longMetric, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(execute, map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, ==, sqlContext, WholeStageCodegenExec, sparkContext, outputPartitioning, toString, longMetric, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(execute, map, asInstanceOf, expressions, isInstanceOf, child, <init>, schema, apply, flatMap, ==, sqlContext, toString, metrics, !=, longMetric, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(map, asInstanceOf, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, ==, sparkContext, outputPartitioning, toString, metrics, longMetric, output, transform, eq, productElement)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, ==, sqlContext, foreach, copy, toString, !=, getClass, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(asInstanceOf, isInstanceOf, <init>, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(simpleString, map, asInstanceOf, nodeName, expressions, isInstanceOf, child, <init>, schema, apply, ==, sqlContext, sparkContext, toString, metrics, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, simpleString, execute, map, treeString, asInstanceOf, conf, isInstanceOf, <init>, apply, CollapseCodegenStages, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF22.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF22.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF22.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF22.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF22.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	UDF22, call.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF22.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(UDF22, call)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, ArrowRowIterator, withPadding, find, span, toBuffer, count, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, fromPayloadIterator, asPythonSerializable, toList, isTraversableAgain, asInstanceOf, sameElements, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, ArrowConverters, forall, mkString, min, scanRight, fold, nonEmpty, byteArrayToBatch, loadBatch, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, GroupedIterator, <init>, toStream, schema, max, buffered, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, toString, copyToArray, length, seq, !=, collect, getClass, hasDefiniteSize, ArrowPayload, patch, foldLeft, toPayloadIterator, contains, isEmpty, toDataFrame, ne, withPartial, reversed, hasNext, indexOf, reduceLeft, eq, sum, ##, scanLeft, finalize, hashCode, zipAll, product.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(withFilter, find, count, size, zip, toSet, map, toMap, asPythonSerializable, asInstanceOf, reduceLeftOption, sliding, ArrowConverters, forall, mkString, min, nonEmpty, addString, drop, isInstanceOf, filter, <init>, schema, max, ++, flatMap, take, ==, indexWhere, foreach, exists, toArray, reduce, toSeq, next, zipWithIndex, toString, length, !=, collect, ArrowPayload, foldLeft, toPayloadIterator, contains, isEmpty, ne, hasNext, sum, scanLeft)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala: Set(ArrowConverters, <init>, flatMap, toArray, toDataFrame)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, json, notifyAll, <init>, Offset, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(json, <init>, Offset, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(json, <init>, Offset, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala: Set(<init>, Offset)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(<init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(<init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Offset.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala: Set(<init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala: Set(json, <init>, Offset)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala: Set(json, <init>, Offset, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala: Set(Offset)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala: Set(<init>, Offset)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala: Set(<init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala: Set(<init>, Offset)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala: Set(json, <init>, Offset, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala: Set(json, <init>, Offset)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(<init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, unapply, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, copy$default$5, batchSize, printSchema, apply$default$8, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, apply$default$7, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, <init>$default$7, $isInstanceOf, partitionStatistics, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, computeStats, logName, notifyAll, conf, mapProductIterator, collectFirst, cachedColumnBuffers, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, isStreaming, doCanonicalize, collectLeaves, <init>$default$8, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, tableName, ==, producedAttributes, _cachedColumnBuffers, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, useCompression, jsonFields, resolve, CachedBatch, $init$, buffers, newInstance, copy$default$3, copy, inputSet, toString, isCanonicalizedPlan, logError, !=, statsCache, maxRows, InMemoryRelation, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, sizeInBytesStats, copy$default$6, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, withOutput, storageLevel, log, numRows, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, stats, map, asInstanceOf, expressions, conf, isInstanceOf, child, isStreaming, <init>, schema, apply, ==, p, copy, toString, InMemoryRelation, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, find, simpleString, children, map, treeString, asInstanceOf, expressions, outputSet, transformUp, conf, isInstanceOf, isStreaming, <init>, mapChildren, schema, apply, flatMap, tableName, ==, foreach, resolve, copy, toString, !=, InMemoryRelation, collect, output, ne, transform, resolveQuoted, storageLevel, numRows)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/GenerateColumnAccessor.scala: Set(unapply, map, asInstanceOf, expressions, isInstanceOf, <init>, ==, p, CachedBatch, getClass, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(unapply, stats, map, asInstanceOf, partitionStatistics, expressions, canonicalized, conf, cachedColumnBuffers, isInstanceOf, child, <init>, schema, apply, flatMap, resolved, ==, foreach, p, CachedBatch, buffers, toString, InMemoryRelation, innerChildren, output, ne, transform, eq, numRows, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(find, refresh, stats, batchSize, map, asInstanceOf, expressions, conf, cachedColumnBuffers, isInstanceOf, <init>, apply, tableName, ==, sameResult, foreach, useCompression, CachedBatch, toString, InMemoryRelation, logWarning, output, transformDown, transformAllExpressions, ne, eq, withOutput, storageLevel)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF8.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF8.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF8.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF8.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF8.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF8.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(call, UDF8)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF8.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF8)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, PythonSQLUtils, equals, arrowPayloadToDataFrame, parseDataType, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, listBuiltinFunctionInfos, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getInterval, notify, anyNull, copyValue, getShort, wait, getUTF8String, $assertionsDisabled, empty, equals, isNullAt, getDouble, numFields, getString, getStruct, setBoolean, getMap, setByte, notifyAll, getByte, setInt, getArray, <init>, fromSeq, ColumnarRow, apply, getFloat, getDecimal, getInt, toSeq, copy, toString, getBoolean, setDecimal, get, setShort, getClass, setNullAt, update, setLong, getLong, setFloat, hashCode, getBinary, setDouble.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, curried, name, wait, currentBatchId, copy$default$2, $asInstanceOf, productArity, equals, inputRows, reportTimeTaken, asInstanceOf, initializeLogIfNecessary, committedOffsets, finishTrigger, synchronized, $isInstanceOf, tupled, updateStatusMessage, logTrace, canEqual, ExecutionStats, lastExecution, isTraceEnabled, initializeLogIfNecessary$default$2, triggerClock, offsetSeqMetadata, productPrefix, logName, notifyAll, postEvent, isInstanceOf, startTrigger, sink, <init>, id, eventTimeStats, apply, lastProgress, ProgressReporter, stateOperators, ==, clone, status, sparkSession, $init$, recentProgress, availableOffsets, copy$default$3, copy, toString, logError, !=, getClass, logWarning, copy$default$1, ne, sources, newData, runId, eq, productIterator, log, logicalPlan, ##, finalize, productElement, hashCode, logDebug, logInfo, currentStatus.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, asInstanceOf, synchronized, triggerClock, notifyAll, isInstanceOf, sink, <init>, id, apply, ==, sparkSession, toString, !=, logWarning, ne, sources, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(unapply, name, currentBatchId, asInstanceOf, committedOffsets, updateStatusMessage, lastExecution, triggerClock, offsetSeqMetadata, postEvent, isInstanceOf, sink, <init>, id, apply, ProgressReporter, ==, status, sparkSession, availableOffsets, copy, toString, logError, !=, logWarning, ne, sources, newData, runId, eq, logicalPlan, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(name, currentBatchId, reportTimeTaken, asInstanceOf, committedOffsets, finishTrigger, updateStatusMessage, lastExecution, triggerClock, offsetSeqMetadata, isInstanceOf, startTrigger, sink, <init>, eventTimeStats, apply, ==, sparkSession, availableOffsets, copy, toString, !=, logWarning, ne, sources, newData, runId, eq, logicalPlan, logDebug, logInfo, currentStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala: Set(name, <init>, id, lastProgress, ==, status, sparkSession, recentProgress, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala: Set(name, synchronized, <init>, apply, lastProgress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(name, currentBatchId, reportTimeTaken, asInstanceOf, committedOffsets, synchronized, lastExecution, triggerClock, offsetSeqMetadata, isInstanceOf, startTrigger, sink, <init>, apply, ==, sparkSession, toString, !=, ne, sources, runId, logicalPlan, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, asInstanceOf, synchronized, triggerClock, notifyAll, isInstanceOf, sink, <init>, id, apply, ==, sparkSession, toString, !=, logWarning, ne, sources, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(name, currentBatchId, reportTimeTaken, asInstanceOf, committedOffsets, finishTrigger, updateStatusMessage, lastExecution, triggerClock, offsetSeqMetadata, isInstanceOf, startTrigger, sink, <init>, eventTimeStats, apply, ==, sparkSession, availableOffsets, copy, toString, !=, logWarning, ne, sources, newData, runId, eq, logicalPlan, logDebug, logInfo, currentStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala: Set(name, <init>, id, lastProgress, ==, status, sparkSession, recentProgress, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala: Set(name, synchronized, <init>, apply, lastProgress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(unapply, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, logError, ne, sources, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(name, currentBatchId, reportTimeTaken, asInstanceOf, committedOffsets, synchronized, lastExecution, triggerClock, offsetSeqMetadata, isInstanceOf, startTrigger, sink, <init>, apply, ==, sparkSession, toString, !=, ne, sources, runId, logicalPlan, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, asInstanceOf, synchronized, triggerClock, notifyAll, isInstanceOf, sink, <init>, id, apply, ==, sparkSession, toString, !=, logWarning, ne, sources, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, id, apply, ==, toString, ne, sources, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(unapply, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, logError, ne, sources, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, copy, toString, !=, getClass, ne, sources, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, sources)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, count, wait, equals, avg, notifyAll, typed, <init>, sumLong, toString, getClass, sum, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOptions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	OrcOptions, notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, compressionCodec, <init>, ==, clone, toString, !=, getORCCompressionCodecName, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(OrcOptions, asInstanceOf, isInstanceOf, compressionCodec, <init>, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getInterval, notify, toBooleanArray, getShort, wait, getUTF8String, equals, isNullAt, ColumnarArray, getDouble, getStruct, toFloatArray, setBoolean, getMap, setByte, notifyAll, getByte, setInt, array, getArray, <init>, getFloat, getDecimal, numElements, toLongArray, foreach, getInt, toArray, toShortArray, toArrayData, copy, toString, getBoolean, get, setShort, getClass, toDoubleArray, setNullAt, update, toByteArray, setLong, toIntArray, getLong, setFloat, hashCode, getBinary, setDouble, toObjectArray.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, supportBatch, isSplitable, wait, $asInstanceOf, checkFieldNames, equals, asInstanceOf, synchronized, $isInstanceOf, buildReaderWithPartitionValues, inferSchema, notifyAll, isInstanceOf, prepareWrite, <init>, ==, clone, $init$, toString, !=, getClass, OrcFileFormat, shortName, ne, vectorTypes, buildReader, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, inferSchema, isInstanceOf, <init>, ==, toString, !=, getClass, OrcFileFormat, shortName, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(checkFieldNames, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, OrcFileFormat, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLog.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, MetadataLog, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, purge, isInstanceOf, getLatest, ==, clone, toString, !=, get, getClass, ne, add, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala: Set(MetadataLog, getLatest, ==, toString, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala: Set(asInstanceOf, toString, !=, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, isInstanceOf, getLatest, ==, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, get, getClass, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, purge, isInstanceOf, getLatest, ==, toString, get, ne, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, isInstanceOf, getLatest, ==, toString, !=, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(==, toString, !=, get, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(asInstanceOf, purge, isInstanceOf, getLatest, ==, toString, !=, get, ne, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala: Set(asInstanceOf, isInstanceOf, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala: Set(add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(getLatest, ==, get, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(asInstanceOf, synchronized, purge, isInstanceOf, getLatest, ==, toString, !=, get, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala: Set(getLatest, ==, toString, get, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(asInstanceOf, synchronized, purge, isInstanceOf, getLatest, ==, toString, !=, get, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(asInstanceOf, purge, isInstanceOf, getLatest, ==, toString, !=, get, ne, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(asInstanceOf, synchronized, purge, isInstanceOf, getLatest, ==, toString, !=, get, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(asInstanceOf, purge, isInstanceOf, getLatest, ==, toString, !=, get, ne, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, purge, isInstanceOf, getLatest, ==, toString, get, ne, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, purge, isInstanceOf, getLatest, ==, toString, get, ne, add, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala: Set(asInstanceOf, isInstanceOf, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(getLatest, ==, get, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala: Set(asInstanceOf, toString, !=, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityGenerator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, UnivocityGenerator, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, flush, toString, !=, getClass, close, ne, eq, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(UnivocityGenerator, isInstanceOf, <init>, ==, !=, getClass, close, write, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	satisfy, numPartitions, Partitioning.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/partitioning/Partitioning.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(Partitioning)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala: Set(satisfy, numPartitions, Partitioning)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	batchTimestampMs, notify, unapply, wait, copy$default$2, OffsetSeqMetadata, $asInstanceOf, <init>$default$1, productArity, equals, json, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, apply$default$3, OffsetSeq, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, offsets, productPrefix, batchWatermarkMs, logName, notifyAll, conf, isInstanceOf, toStreamProgress, <init>$default$3, <init>, apply$default$2, apply, ==, fill, clone, $init$, copy$default$3, copy, apply$default$1, metadata, toString, logError, !=, getClass, logWarning, copy$default$1, setSessionConf, ne, <init>$default$2, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala: Set(OffsetSeqMetadata, OffsetSeq, <init>, metadata, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(OffsetSeqMetadata, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(batchTimestampMs, unapply, OffsetSeqMetadata, asInstanceOf, batchWatermarkMs, conf, isInstanceOf, <init>, apply, ==, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(unapply, OffsetSeqMetadata, asInstanceOf, OffsetSeq, conf, isInstanceOf, <init>, apply, ==, copy, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(batchTimestampMs, OffsetSeqMetadata, json, asInstanceOf, OffsetSeq, batchWatermarkMs, conf, isInstanceOf, toStreamProgress, <init>, apply, ==, copy, metadata, toString, !=, logWarning, setSessionConf, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala: Set(OffsetSeqMetadata, json, asInstanceOf, OffsetSeq, offsets, isInstanceOf, <init>, apply, ==, fill, metadata, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(OffsetSeqMetadata, json, asInstanceOf, synchronized, batchWatermarkMs, conf, isInstanceOf, <init>, apply, ==, copy, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(OffsetSeqMetadata, json, asInstanceOf, synchronized, OffsetSeq, offsets, isInstanceOf, toStreamProgress, <init>, apply, ==, fill, metadata, toString, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, commit, equals, asInstanceOf, synchronized, createInternalRowWriterFactory, $isInstanceOf, createWriterFactory, notifyAll, isInstanceOf, <init>, ==, clone, MicroBatchWriter, toString, !=, getClass, InternalRowMicroBatchWriter, ne, eq, ##, finalize, hashCode, abort.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(commit, asInstanceOf, isInstanceOf, <init>, ==, MicroBatchWriter, toString, !=, InternalRowMicroBatchWriter, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $assertionsDisabled, equals, setNumRows, numCols, notifyAll, <init>, getRow, column, toString, ColumnarBatch, getClass, close, rowIterator, numRows, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala: Set(<init>, ColumnarBatch)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala: Set(numCols, <init>, column, toString, ColumnarBatch, rowIterator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala: Set(setNumRows, <init>, ColumnarBatch, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala: Set(setNumRows, <init>, ColumnarBatch, close, rowIterator)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(setNumRows, <init>, column, toString, ColumnarBatch, close, numRows)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(<init>, ColumnarBatch, getClass, close, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala: Set(<init>, ColumnarBatch, numRows)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala: Set(<init>, toString, ColumnarBatch, rowIterator)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala: Set(<init>, toString, ColumnarBatch, close, numRows)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(<init>, toString, ColumnarBatch, close)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryStatus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryStatus.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryStatus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryStatus.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryStatus.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, copy$default$2, $asInstanceOf, equals, json, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isDataAvailable, isInstanceOf, <init>, prettyJson, ==, clone, copy$default$3, copy, message, toString, !=, isTriggerActive, StreamingQueryStatus, getClass, copy$default$1, ne, jsonValue, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala: Set(StreamingQueryStatus)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, copy, toString, !=, StreamingQueryStatus, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(json, asInstanceOf, isInstanceOf, <init>, ==, copy, toString, !=, StreamingQueryStatus, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(json, asInstanceOf, synchronized, isInstanceOf, <init>, ==, copy, message, toString, StreamingQueryStatus, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala: Set(<init>, ==, StreamingQueryStatus)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, locations, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, unpersist, sortBy$default$2, parent, isLocallyCheckpointed, getOrCompute, distinct$default$2, partitioner, coalesce, name, count, wait, copy$default$2, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, union, coalesce$default$3, copy$default$5, zip, localCheckpoint, map, productArity, subtract, equals, pipe$default$5, intersection, sortBy$default$3, foreachPartition, countApprox$default$2, scope, asInstanceOf, context, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, glom, sortBy, pipe$default$6, doCheckpoint, synchronized, pipe$default$2, repartition$default$2, aggregate, $isInstanceOf, compute, mapPartitions$default$2, min, getCheckpointFile, fold, getOutputDeterministicLevel, logTrace, canEqual, treeAggregate$default$4, copy$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, zipWithUniqueId, productPrefix, iterator, coalesce$default$4, countApprox, logName, files, notifyAll, countApproxDistinct$default$1, conf, getNarrowAncestors, cache, getNumPartitions, isInstanceOf, filter, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, FileScanRDD, persist, checkpointData, <init>, isCheckpointed, id, mapPartitionsWithIndexInternal, countApproxDistinct, max, outputDeterministicLevel, randomSampleWithRange, toDebugString, ++, flatMap, take, countByValue$default$1, groupBy, treeReduce$default$2, ==, randomSplit$default$2, groupBy$default$4, clone, distinct, retag, foreach, treeReduce, toLocalIterator, filePath, sparkContext, reduce, saveAsTextFile, $init$, takeSample$default$3, zipWithIndex, filePartitions, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, PartitionedFile, elementClassTag, copy$default$3, sample, copy, pipe$default$7, toString, mapPartitionsInternal, preferredLocations, length, logError, !=, partitions, collect, getClass, pipe, logWarning, getPartitions, copy$default$1, pipe$default$4, cartesian, partitionValues, repartition, start, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, isEmpty, sample$default$3, ne, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, randomSplit, top, coalesce$default$2, getCreationSite, computeOrReadCheckpoint, dependencies, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, toJavaRDD, eq, isReliablyCheckpointed, productIterator, withScope, log, ##, finalize, treeAggregate, index, productElement, hashCode, takeOrdered, logDebug, firstParent, logInfo, FilePartition, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(name, setName, map, conf, <init>, flatMap, foreach, filePath, sparkContext, PartitionedFile, sample, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala: Set(iterator, conf, <init>, filePath, PartitionedFile, length, start)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(name, mapPartitions, setName, map, asInstanceOf, conf, isInstanceOf, filter, <init>, flatMap, take, ==, distinct, foreach, filePath, sparkContext, zipWithIndex, PartitionedFile, toString, !=, start, isEmpty, ne, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(name, map, asInstanceOf, sortBy, min, files, conf, isInstanceOf, filter, FileScanRDD, <init>, mapPartitionsWithIndexInternal, max, flatMap, groupBy, ==, foreach, filePath, sparkContext, zipWithIndex, filePartitions, PartitionedFile, toString, length, partitions, getClass, isEmpty, ne, eq, index, logInfo, FilePartition)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala: Set(map, conf, isInstanceOf, <init>, ++, ==, PartitionedFile, partitionValues)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(name, context, files, conf, isInstanceOf, <init>, ==, foreach, sparkContext, PartitionedFile, length, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(name, context, files, conf, isInstanceOf, <init>, ==, foreach, sparkContext, PartitionedFile, length, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala: Set(map, context, conf, <init>, foreach, sparkContext, PartitionedFile, length, !=, isEmpty)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(mapPartitions, map, asInstanceOf, context, sortBy, min, iterator, files, conf, isInstanceOf, filter, <init>, max, ++, flatMap, ==, foreach, filePath, sparkContext, PartitionedFile, toString, length, collect, getClass, logWarning, partitionValues, start, isEmpty, ne, eq, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(name, map, asInstanceOf, context, files, conf, isInstanceOf, filter, <init>, ++, ==, foreach, filePath, sparkContext, PartitionedFile, length, !=, getClass, partitionValues, start, isEmpty, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala: Set(iterator, conf, <init>, filePath, PartitionedFile, length, start)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GroupedIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GroupedIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GroupedIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GroupedIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GroupedIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, currentIterator, toBuffer, count, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, fold, nonEmpty, notifyAll, /:, toIterator, keyOrdering, addString, to, collectFirst, drop, isInstanceOf, filter, GroupedIterator, <init>, toStream, max, buffered, currentRow, apply, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, sortOrder, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, toString, copyToArray, length, seq, !=, collect, getClass, hasDefiniteSize, currentGroup, patch, foldLeft, contains, isEmpty, ne, withPartial, reversed, hasNext, indexOf, reduceLeft, eq, sum, ##, scanLeft, finalize, hashCode, zipAll, product, keyProjection.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(map, asInstanceOf, nonEmpty, isInstanceOf, filter, GroupedIterator, <init>, apply, ++, flatMap, ==, exists, toString, !=, contains, ne, indexOf, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala: Set(map, asInstanceOf, drop, isInstanceOf, GroupedIterator, <init>, apply, grouped, flatMap, ==, toArray, toString, length, isEmpty, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala: Set(map, asInstanceOf, isInstanceOf, GroupedIterator, <init>, apply, ++, grouped, flatMap, ==, exists, toString, length, collect, contains, isEmpty, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, partitionBy, equals, rowsBetween, unboundedFollowing, asInstanceOf, synchronized, $isInstanceOf, rangeBetween, unboundedPreceding, notifyAll, isInstanceOf, currentRow, ==, clone, orderBy, spec, toString, !=, getClass, ne, Window, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala: Set(equals, asInstanceOf, isInstanceOf, ==, spec, toString, ne, Window, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcDeserializer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, set, OrcDeserializer, synchronized, RowUpdater, $isInstanceOf, setBoolean, ArrayDataUpdater, deserialize, setByte, notifyAll, setInt, isInstanceOf, <init>, ==, clone, $init$, CatalystDataUpdater, toString, !=, setShort, getClass, setNullAt, setLong, ne, eq, ##, finalize, setFloat, hashCode, setDouble.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(asInstanceOf, set, OrcDeserializer, deserialize, isInstanceOf, <init>, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	minBytesForPrecision, notify, wait, $asInstanceOf, checkFieldNames, equals, asInstanceOf, synchronized, EMPTY_MESSAGE, $isInstanceOf, checkFieldName, notifyAll, isInstanceOf, maxPrecisionForBytes, ParquetSchemaConverter, convertField, <init>, checkConversionRequirement, ==, SPARK_PARQUET_SCHEMA_NAME, clone, ParquetToSparkSchemaConverter, toString, !=, getClass, ne, eq, convert, SparkToParquetSchemaConverter, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala: Set(asInstanceOf, isInstanceOf, convertField, <init>, ==, ParquetToSparkSchemaConverter, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRecordMaterializer.scala: Set(<init>, ParquetToSparkSchemaConverter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport.scala: Set(asInstanceOf, EMPTY_MESSAGE, isInstanceOf, ParquetSchemaConverter, convertField, <init>, ==, SPARK_PARQUET_SCHEMA_NAME, ParquetToSparkSchemaConverter, toString, !=, SparkToParquetSchemaConverter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, ParquetToSparkSchemaConverter, toString, getClass, ne, eq, convert, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala: Set(minBytesForPrecision, asInstanceOf, checkFieldName, isInstanceOf, ParquetSchemaConverter, <init>, ==, toString, !=, ne, convert, SparkToParquetSchemaConverter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(checkFieldNames, asInstanceOf, isInstanceOf, ParquetSchemaConverter, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, getPeakMemoryUsedBytes, wait, equals, getAggregationBuffer, access$200, iterator, notifyAll, <init>, access$100, getAverageProbesPerLookup, access$000, supportsAggregationBufferSchema, UnsafeFixedWidthAggregationMap, toString, getClass, destructAndCreateExternalSorter, getAggregationBufferFromUnsafeRow, free, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(getPeakMemoryUsedBytes, equals, iterator, <init>, getAverageProbesPerLookup, supportsAggregationBufferSchema, UnsafeFixedWidthAggregationMap, toString, destructAndCreateExternalSorter, free)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala: Set(getPeakMemoryUsedBytes, equals, iterator, <init>, getAverageProbesPerLookup, UnsafeFixedWidthAggregationMap, destructAndCreateExternalSorter, getAggregationBufferFromUnsafeRow, free)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, replace, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, drop, isInstanceOf, <init>, ==, fill, clone, toString, !=, getClass, ne, DataFrameNaFunctions, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(replace, asInstanceOf, drop, isInstanceOf, <init>, ==, fill, toString, !=, ne, DataFrameNaFunctions)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala: Set(replace, asInstanceOf, isInstanceOf, <init>, ==, fill, toString, getClass, ne, DataFrameNaFunctions)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, unapply, find, simpleString, MemoryPlan, children, toDS, refresh, maxRowsPerPartition, batches, verboseString, semanticHash, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, commit, printSchema, getBatch, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, memoryStreamId, subqueries, clear, asInstanceOf, transformExpressions, initializeLogIfNecessary, generateTreeString, addData, childrenResolved, latestBatchData, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, latestBatchId, productPrefix, stop, resolveChildren, computeStats, logName, notifyAll, allData, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, sink, collectLeaves, references, <init>, generateTreeString$default$5, id, foreachUp, mapChildren, schema, encoder, transformExpressionsDown, prettyJson, toDebugString, apply, flatMap, resolved, lastOffsetCommitted, toDF, ==, producedAttributes, currentOffset, fastEquals, sqlContext, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, MemoryStream, $init$, copy, inputSet, reset, toString, isCanonicalizedPlan, getOffset, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, MemorySink, resolveQuoted, statePrefix, addBatch, eq, productIterator, toJSON, log, logicalPlan, ##, containsChild, finalize, currentBlockId, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(find, MemoryPlan, map, asInstanceOf, conf, isInstanceOf, sink, <init>, schema, apply, toDF, ==, sqlContext, toString, !=, getClass, output, ne, MemorySink, logicalPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, MemoryPlan, children, stats, map, asInstanceOf, expressions, allData, conf, isInstanceOf, isStreaming, sink, <init>, schema, encoder, apply, ==, p, copy, toString, output, ne, MemorySink, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	RichAttribute.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TypedAggregateExpression.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, VectorizedColumnReader, equals, notifyAll, <init>, access$100, access$000, readBatch, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/SchemaColumnConvertNotSupportedException.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/SchemaColumnConvertNotSupportedException.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/SchemaColumnConvertNotSupportedException.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/SchemaColumnConvertNotSupportedException.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/SchemaColumnConvertNotSupportedException.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, equals, fillInStackTrace, initCause, getLogicalType, getPhysicalType, SchemaColumnConvertNotSupportedException, getCause, notifyAll, getStackTrace, getColumn, <init>, getMessage, setStackTrace, getSuppressed, addSuppressed, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala: Set(getLogicalType, getPhysicalType, SchemaColumnConvertNotSupportedException, getColumn, <init>, getMessage, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/SchemaColumnConvertNotSupportedException.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	agg, notify, unapply, curried, flatMapGroupsInR, GroupType, count, wait, copy$default$2, $asInstanceOf, productArity, RelationalGroupedDataset, equals, pivot, RollupType, asInstanceOf, synchronized, avg, $isInstanceOf, mean, tupled, min, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, max, apply, flatMapGroupsInPandas, ==, clone, $init$, PivotType, copy, pivotCol, values, toString, !=, CubeType, getClass, copy$default$1, GroupByType, ne, eq, productIterator, sum, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(agg, unapply, GroupType, count, RelationalGroupedDataset, RollupType, asInstanceOf, min, isInstanceOf, <init>, max, apply, ==, copy, values, toString, !=, CubeType, GroupByType, ne, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(count, RelationalGroupedDataset, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(flatMapGroupsInR, RelationalGroupedDataset, asInstanceOf, isInstanceOf, <init>, apply, ==, values, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala: Set(agg, count, RelationalGroupedDataset, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, HashJoin, $asInstanceOf, buildSideKeyGenerator, join, equals, buildSide, streamedPlan, asInstanceOf, synchronized, left, $isInstanceOf, buildPlan, createResultProjection, notifyAll, isInstanceOf, leftKeys, rightKeys, condition, ==, clone, buildKeys, outputPartitioning, $init$, streamSideKeyGenerator, toString, !=, joinType, getClass, streamedKeys, output, ne, eq, right, ##, finalize, hashCode, rewriteKeyExpr.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala: Set(HashJoin, join, buildSide, streamedPlan, asInstanceOf, left, buildPlan, isInstanceOf, leftKeys, rightKeys, condition, ==, buildKeys, toString, joinType, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala: Set(HashJoin, join, buildSide, streamedPlan, asInstanceOf, left, buildPlan, isInstanceOf, leftKeys, rightKeys, condition, ==, buildKeys, toString, joinType, getClass, streamedKeys, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(buildSide, asInstanceOf, left, isInstanceOf, leftKeys, rightKeys, condition, ==, outputPartitioning, toString, joinType, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(buildSide, asInstanceOf, left, isInstanceOf, leftKeys, rightKeys, condition, ==, outputPartitioning, toString, joinType, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(asInstanceOf, isInstanceOf, ==, outputPartitioning, toString, !=, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(buildSide, asInstanceOf, left, isInstanceOf, leftKeys, rightKeys, condition, ==, outputPartitioning, toString, joinType, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(buildSide, asInstanceOf, left, isInstanceOf, leftKeys, rightKeys, condition, ==, outputPartitioning, toString, joinType, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	readBooleans, notify, readLong, readFloats, wait, readBoolean, equals, readValueDictionaryId, readLongs, readFloat, readDoubles, readBinarys, skip, readInteger, notifyAll, initFromPage, readByte, readShorts, <init>, readBytes, VectorizedRleValuesReader, readDouble, toString, readBinary, getClass, initFromBuffer, getNextOffset, readIntegers, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	catalogTable, buildStorageFormatFromOptions, notify, unapply, curried, providingClass, resolveRelation, name, resolveRelation$default$1, wait, apply$default$4, <init>$default$5, copy$default$2, $asInstanceOf, <init>$default$6, copy$default$5, apply$default$8, productArity, equals, writeAndRead, userSpecifiedSchema, asInstanceOf, initializeLogIfNecessary, apply$default$7, synchronized, <init>$default$7, $isInstanceOf, apply$default$3, <init>$default$4, copy$default$8, tupled, logTrace, canEqual, copy$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, paths, productPrefix, logName, notifyAll, SourceInfo, isInstanceOf, planForWriting, options, apply$default$6, <init>$default$3, <init>$default$8, <init>, partitionColumns, schema, createSink, apply, ==, bucketSpec, clone, sourceInfo, className, copy$default$7, sparkSession, $init$, lookupDataSource, copy$default$3, copy, toString, logError, !=, apply$default$5, getClass, logWarning, copy$default$1, copy$default$6, ne, createSource, eq, productIterator, log, ##, DataSource, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(resolveRelation, name, paths, options, <init>, schema, apply, sparkSession, toString, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(buildStorageFormatFromOptions, name, asInstanceOf, isInstanceOf, options, <init>, partitionColumns, schema, apply, ==, bucketSpec, className, !=, logWarning, ne, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(resolveRelation, asInstanceOf, synchronized, logTrace, isInstanceOf, options, <init>, partitionColumns, schema, apply, ==, sparkSession, toString, logWarning, ne, eq, DataSource, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(catalogTable, unapply, resolveRelation, name, asInstanceOf, isInstanceOf, options, <init>, schema, apply, ==, bucketSpec, sparkSession, toString, !=, ne, eq, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(catalogTable, unapply, name, asInstanceOf, isInstanceOf, <init>, schema, apply, ==, bucketSpec, sparkSession, lookupDataSource, toString, !=, getClass, ne, eq, log, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(providingClass, asInstanceOf, isInstanceOf, planForWriting, <init>, ==, sparkSession, toString, logError, eq, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(buildStorageFormatFromOptions, unapply, name, asInstanceOf, options, <init>, schema, apply, ==, bucketSpec, sparkSession, copy, ne, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(catalogTable, providingClass, resolveRelation, name, asInstanceOf, isInstanceOf, <init>, schema, apply, ==, bucketSpec, className, sparkSession, lookupDataSource, toString, !=, ne, eq, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(name, asInstanceOf, isInstanceOf, options, <init>, schema, apply, ==, sparkSession, copy, toString, !=, logWarning, ne, createSource, eq, DataSource, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(resolveRelation, name, asInstanceOf, paths, isInstanceOf, options, <init>, schema, apply, ==, sparkSession, toString, !=, ne, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(userSpecifiedSchema, asInstanceOf, isInstanceOf, options, <init>, schema, apply, ==, sparkSession, lookupDataSource, toString, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(resolveRelation, writeAndRead, asInstanceOf, isInstanceOf, <init>, schema, ==, bucketSpec, sparkSession, toString, logError, !=, logWarning, eq, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(name, asInstanceOf, isInstanceOf, options, <init>, schema, createSink, apply, ==, sparkSession, lookupDataSource, toString, !=, getClass, ne, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(buildStorageFormatFromOptions, name, asInstanceOf, isInstanceOf, planForWriting, options, <init>, schema, apply, ==, sparkSession, lookupDataSource, toString, ne, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(name, asInstanceOf, SourceInfo, isInstanceOf, <init>, schema, apply, ==, sourceInfo, sparkSession, toString, eq, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(resolveRelation, name, userSpecifiedSchema, asInstanceOf, paths, isInstanceOf, options, <init>, schema, apply, ==, sparkSession, lookupDataSource, toString, !=, ne, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(resolveRelation, userSpecifiedSchema, asInstanceOf, isInstanceOf, options, <init>, apply, ==, sparkSession, toString, eq, DataSource)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, MapGroupsWithStateFunction.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(call, MapGroupsWithStateFunction)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	creationSite, newNaturalAscendingOrdering, zipPartitions, markCheckpointed, notify, mapPartitionsWithIndex$default$2, treeString$default$2, unapply, unpersist, sortBy$default$2, find, produce, parent, isLocallyCheckpointed, simpleString, getOrCompute, children, distinct$default$2, partitioner, doProduce, coalesce, name, count, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, step, copy$default$2, $asInstanceOf, isCheckpointedAndMaterialized, mapPartitions, setName, numberedTreeString, doConsume, union, evaluateVariables, coalesce$default$3, copy$default$5, seed, zip, lowerBound, compose, resetMetrics, printSchema, localCheckpoint, EmptyPartition, map, productArity, subtract, canEvaluateWithinJoin, verboseStringWithSuffix, equals, pipe$default$5, intersection, treeString, schemaString, sortBy$default$3, canEvaluate, argString, foreachPartition, countApprox$default$2, subqueries, scope, executeQuery, asInstanceOf, context, transformExpressions, initializeLogIfNecessary, subtract$default$3, getPreferredLocations, doExecute, glom, ProjectExec, generateTreeString, prepare, sortBy, pipe$default$6, metricTerm, doCheckpoint, synchronized, pipe$default$2, splitDisjunctivePredicates, repartition$default$2, withReplacement, generateTreeString$default$6, allAttributes, nodeName, aggregate, $isInstanceOf, compute, andThen, doPrepare, mapPartitions$default$2, min, getCheckpointFile, usedInputs, fold, FilterExec, getOutputDeterministicLevel, logTrace, asCode, canEqual, expressions, treeAggregate$default$4, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, UnionExec, zipWithUniqueId, splitConjunctivePredicates, productPrefix, iterator, coalesce$default$4, countApprox, logName, notifyAll, countApproxDistinct$default$1, conf, mapProductIterator, getNarrowAncestors, collectFirst, cache, getNumPartitions, otherCopyArgs, missingInput, isInstanceOf, stringArgs, filter, child, upperBound, inputRDDs, pipe$default$3, countByValueApprox$default$3, unpersist$default$1, doCanonicalize, executionContext, collectLeaves, references, persist, newMutableProjection$default$3, checkpointData, <init>, isCheckpointed, outputOrdering, generateTreeString$default$5, id, foreachUp, mapChildren, mapPartitionsWithIndexInternal, condition, countApproxDistinct, schema, max, outputDeterministicLevel, randomSampleWithRange, transformExpressionsDown, prettyJson, toDebugString, apply, ++, flatMap, take, CoalesceExec, numElements, countByValue$default$1, executeCollect, groupBy, treeReduce$default$2, ==, producedAttributes, fastEquals, sqlContext, range, randomSplit$default$2, origin, transformExpressionsUp, groupBy$default$4, clone, newMutableProjection, newPredicate, distinct, retag, sameResult, foreach, treeReduce, toLocalIterator, p, jsonFields, subexpressionEliminationEnabled, sparkContext, reduce, RangeExec, saveAsTextFile, outputPartitioning, $init$, takeSample$default$3, zipWithIndex, getStorageLevel, checkpoint, first, countByValue, countByValueApprox$default$2, EmptyRDDWithPartitions, elementClassTag, copy$default$3, sample, copy, executeCollectPublic, inputSet, pipe$default$7, SubqueryExec, prepareSubqueries, end, toString, mapPartitionsInternal, preferredLocations, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, SampleExec, logError, !=, consume, innerChildren, partitions, collect, getClass, supportCodegen, longMetric, pipe, logWarning, getPartitions, output, copy$default$1, needStopCheck, replaceAlias, pipe$default$4, needCopyResult, transformDown, transformAllExpressions, cartesian, mapExpressions, repartition, start, mapPartitionsWithIndexInternal$default$2, collectPartitions, mapPartitionsInternal$default$2, clearDependencies, projectList, isEmpty, sample$default$3, executeBroadcast, ne, requiredChildOrdering, countByValueApprox, getDependencies, mapPartitionsWithIndex, intersection$default$3, keyBy, transform, randomSplit, top, coalesce$default$2, withNewChildren, getCreationSite, computeOrReadCheckpoint, dependencies, statePrefix, saveAsObjectFile, mapPartitionsWithIndexInternal$default$3, numPartitions, toJavaRDD, eq, waitForSubqueries, isReliablyCheckpointed, productIterator, toJSON, withScope, log, doExecuteBroadcast, consume$default$3, executeToIterator, ##, containsChild, newOrdering, finalize, treeAggregate, index, productElement, hashCode, takeOrdered, logDebug, firstParent, logInfo, numSlices, takeSample.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, name, map, asInstanceOf, ProjectExec, FilterExec, expressions, conf, collectFirst, isInstanceOf, filter, child, references, <init>, schema, apply, ++, flatMap, take, ==, foreach, p, toString, !=, output, projectList, isEmpty, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala: Set(children, name, count, execute, map, executeQuery, asInstanceOf, doExecute, prepare, synchronized, nodeName, doPrepare, min, expressions, outputSet, makeCopy, iterator, conf, isInstanceOf, child, <init>, schema, max, apply, flatMap, take, executeCollect, ==, sqlContext, foreach, toLocalIterator, p, subexpressionEliminationEnabled, sparkContext, zipWithIndex, SubqueryExec, prepareSubqueries, toString, mapPartitionsInternal, isCanonicalizedPlan, metrics, executeTake, !=, partitions, collect, logWarning, isEmpty, ne, waitForSubqueries, withScope, log, doExecuteBroadcast, newOrdering, index)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, seed, lowerBound, map, asInstanceOf, ProjectExec, withReplacement, aggregate, FilterExec, expressions, UnionExec, conf, isInstanceOf, child, upperBound, <init>, outputOrdering, condition, schema, apply, CoalesceExec, ==, distinct, p, sparkContext, RangeExec, outputPartitioning, copy, toString, SampleExec, output, projectList, isEmpty, ne, numPartitions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, zip, map, asInstanceOf, transformExpressions, ProjectExec, aggregate, FilterExec, expressions, outputSet, transformUp, splitConjunctivePredicates, isInstanceOf, filter, child, references, <init>, condition, apply, ++, flatMap, ==, foreach, zipWithIndex, inputSet, !=, output, transformDown, isEmpty, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, ProjectExec, FilterExec, expressions, conf, isInstanceOf, references, <init>, apply, ++, flatMap, ==, sparkContext, collect, projectList)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, find, simpleString, name, map, asInstanceOf, ProjectExec, FilterExec, expressions, isInstanceOf, filter, references, <init>, apply, ++, ==, output, isEmpty, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala: Set(find, simpleString, name, map, subqueries, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, id, schema, apply, executeCollect, ==, sameResult, SubqueryExec, toString, transformAllExpressions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, toIterable, withFilter, toTraversable, notify, withPadding, treeString$default$2, find, produce, span, parent, simpleString, outer, children, toBuffer, doProduce, count, generator, reduceOption, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, foldRight, copy$default$2, takeWhile, $asInstanceOf, minBy, numberedTreeString, size, requiredChildOutput, doConsume, evaluateVariables, copy$default$5, zip, toSet, resetMetrics, printSchema, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, productArity, toMap, filterNot, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, toList, executeQuery, isTraversableAgain, asInstanceOf, sameElements, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, reduceLeftOption, metricTerm, synchronized, sliding, generateTreeString$default$6, partition, allAttributes, nodeName, aggregate, $isInstanceOf, forall, doPrepare, mkString, min, scanRight, usedInputs, fold, logTrace, asCode, nonEmpty, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, /:, conf, mapProductIterator, toIterator, addString, to, collectFirst, drop, otherCopyArgs, missingInput, isInstanceOf, stringArgs, filter, child, inputRDDs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, GroupedIterator, <init>, outputOrdering, generateTreeString$default$5, toStream, foreachUp, mapChildren, results, schema, max, buffered, boundGenerator, transformExpressionsDown, prettyJson, apply, ++, grouped, flatMap, take, executeCollect, reduceRight, ==, producedAttributes, maxBy, indexWhere, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, slice, sameResult, foreach, exists, p, reduceRightOption, toVector, toIndexedSeq, jsonFields, copyToBuffer, subexpressionEliminationEnabled, sparkContext, toArray, reduce, padTo, outputPartitioning, $init$, toSeq, next, zipWithIndex, copy$default$3, func, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, copyToArray, length, evaluateRequiredVariables, isCanonicalizedPlan, seq, metrics, executeTake, logError, !=, consume, innerChildren, collect, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, hasDefiniteSize, needCopyResult, transformDown, transformAllExpressions, mapExpressions, GenerateExec, patch, foldLeft, contains, isEmpty, executeBroadcast, ne, generatorOutput, requiredChildOrdering, withPartial, LazyIterator, transform, reversed, hasNext, withNewChildren, indexOf, reduceLeft, statePrefix, eq, waitForSubqueries, productIterator, toJSON, sum, log, doExecuteBroadcast, consume$default$3, executeToIterator, ##, scanLeft, containsChild, newOrdering, finalize, productElement, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(outer, children, generator, requiredChildOutput, toSet, map, asInstanceOf, partition, aggregate, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, func, copy, toString, length, output, GenerateExec, isEmpty, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	appSparkVersion, notify, parent, name, wait, $asInstanceOf, equals, appName, prefix, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, pages, attachPage, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, basePath, logName, notifyAll, isInstanceOf, sqlStore, <init>, ==, clone, $init$, toString, logError, !=, getClass, logWarning, headerTabs, ne, SQLTab, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala: Set(parent, prefix, basePath, sqlStore, <init>, ==, toString, !=, ne, SQLTab)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala: Set(parent, basePath, sqlStore, <init>, ==, toString, !=, ne, SQLTab)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala: Set(<init>, SQLTab)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala: Set(parent, asInstanceOf, <init>, ==, !=, logWarning, SQLTab, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	StreamWriteSupport, createStreamWriter.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(StreamWriteSupport)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(StreamWriteSupport)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(StreamWriteSupport, createStreamWriter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala: Set(StreamWriteSupport)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(StreamWriteSupport)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(StreamWriteSupport, createStreamWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(StreamWriteSupport)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, MemoryWriter, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, outputMode, commit, createDataWriter, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, clear, asInstanceOf, transformExpressions, initializeLogIfNecessary, generateTreeString, data, childrenResolved, MemoryStreamWriter, latestBatchData, synchronized, generateTreeString$default$6, partition, allAttributes, nodeName, $isInstanceOf, validConstraints, createWriterFactory, logTrace, MemoryDataWriter, asCode, MemoryPlanV2, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, latestBatchId, productPrefix, resolveChildren, computeStats, logName, notifyAll, allData, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, sink, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, toDebugString, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy, inputSet, toString, isCanonicalizedPlan, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, MemoryWriterCommitMessage, statePrefix, createStreamWriter, eq, productIterator, write, toJSON, log, ##, containsChild, finalize, productElement, hashCode, MemorySinkV2, abort, logDebug, logInfo, MemoryWriterFactory.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(find, outputMode, map, asInstanceOf, MemoryPlanV2, conf, isInstanceOf, sink, <init>, schema, apply, ==, toString, !=, getClass, output, ne, MemorySinkV2)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, stats, outputMode, map, asInstanceOf, data, partition, MemoryPlanV2, expressions, allData, conf, isInstanceOf, isStreaming, sink, <init>, schema, apply, ==, p, copy, toString, output, ne, eq, MemorySinkV2)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, wait, stats, outputColumnNames, $asInstanceOf, numberedTreeString, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, basicWriteJobStatsTracker, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, outputColumns, childrenResolved, synchronized, logicalPlanOutputWithNames, DataWritingCommand, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, query, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, inputSet, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(find, stats, outputColumnNames, map, asInstanceOf, run, outputColumns, logicalPlanOutputWithNames, DataWritingCommand, expressions, conf, isInstanceOf, schema, apply, flatMap, resolved, ==, p, toString, !=, collect, getClass, logWarning, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, query, expressions, conf, collectFirst, isInstanceOf, references, schema, apply, flatMap, resolved, ==, foreach, p, resolve, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, query, expressions, conf, collectFirst, isInstanceOf, references, schema, apply, flatMap, resolved, ==, foreach, p, resolve, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, stats, map, asInstanceOf, DataWritingCommand, query, expressions, conf, isInstanceOf, isStreaming, schema, apply, ==, p, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(simpleString, map, asInstanceOf, run, DataWritingCommand, nodeName, expressions, isInstanceOf, isStreaming, schema, apply, ==, toString, metrics, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(outputColumnNames, map, asInstanceOf, run, DataWritingCommand, query, expressions, conf, isInstanceOf, schema, ==, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(refresh, outputColumnNames, map, basicWriteJobStatsTracker, asInstanceOf, run, outputColumns, DataWritingCommand, query, expressions, conf, isInstanceOf, apply, flatMap, ==, foreach, p, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(find, stats, outputColumnNames, map, asInstanceOf, run, outputColumns, logicalPlanOutputWithNames, DataWritingCommand, expressions, conf, isInstanceOf, schema, apply, flatMap, resolved, ==, p, toString, !=, collect, getClass, logWarning, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, DefaultKeySet, find, span, flatten, toBuffer, count, reduceOption, sliceWithKnownDelta, wait, foldRight, takeWhile, stringPrefix, $asInstanceOf, empty, splitAt, minBy, size, union, intersect, inits, zip, compose, toSet, subsetOf, :\, map, takeRight, dropWhile, toMap, filterNot, equals, par, unzip3, repr, toList, isTraversableAgain, FilteredKeys, head, asInstanceOf, sameElements, filterKeys, unzip, reduceLeftOption, &, synchronized, sliding, withDefault, Self, partition, aggregate, $isInstanceOf, forall, newBuilder, valuesIterator, andThen, mkString, min, scanRight, fold, |, keysIterator, scan, nonEmpty, canEqual, tail, lastOption, dropRight, iterator, last, orElse, notifyAll, /:, toIterator, addString, to, keySet, collectFirst, drop, -, isInstanceOf, filter, isDefinedAt, ++:, <init>, toStream, withDefaultValue, companion, max, tails, updated, apply, ++, grouped, diff, flatMap, getOrElse, take, parCombiner, reduceRight, groupBy, ==, maxBy, sliceWithKnownBound, clone, ImmutableDefaultKeySet, slice, default, foreach, &~, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, subsets, $init$, toSeq, zipWithIndex, baseMap, DefaultValuesIterable, values, toString, genericBuilder, copyToArray, runWith, seq, +, !=, MappedValues, get, transpose, collect, headOption, getClass, WithFilter, hasDefiniteSize, mapValues, --, foldLeft, toOffsetSeq, contains, toCollection, isEmpty, StreamProgress, ne, transform, init, reversed, reduceLeft, lift, eq, sum, thisCollection, ##, scanLeft, finalize, keys, hashCode, zipAll, product, view, applyOrElse.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(empty, map, asInstanceOf, mkString, isInstanceOf, <init>, apply, getOrElse, ==, foreach, toString, +, !=, get, toOffsetSeq, contains, isEmpty, StreamProgress, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala: Set(empty, size, zip, map, toMap, asInstanceOf, mkString, isInstanceOf, <init>, apply, ++, getOrElse, ==, foreach, toString, +, !=, get, collect, StreamProgress, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(count, empty, minBy, size, zip, map, toMap, asInstanceOf, nonEmpty, to, -, isInstanceOf, filter, isDefinedAt, <init>, max, apply, ++, flatMap, getOrElse, ==, foreach, exists, zipWithIndex, toString, +, !=, get, collect, mapValues, toOffsetSeq, isEmpty, StreamProgress, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(count, empty, size, zip, map, toMap, asInstanceOf, synchronized, mkString, min, nonEmpty, lastOption, -, isInstanceOf, <init>, max, apply, ++, flatMap, getOrElse, groupBy, ==, toArray, toString, seq, +, get, collect, headOption, mapValues, StreamProgress, ne, eq, sum)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(empty, size, zip, map, head, asInstanceOf, synchronized, -, isInstanceOf, <init>, apply, ++, getOrElse, ==, toArray, toString, +, !=, get, collect, contains, StreamProgress, ne, transform)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, needConversionInPython, asInstanceOf, synchronized, $isInstanceOf, makeFromJava, EvaluatePython, notifyAll, isInstanceOf, toJava, registerPicklers, ==, clone, javaToPython, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(asInstanceOf, EvaluatePython, isInstanceOf, toJava, registerPicklers, ==, javaToPython, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala: Set(needConversionInPython, asInstanceOf, makeFromJava, EvaluatePython, isInstanceOf, toJava, registerPicklers, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, makeFromJava, EvaluatePython, isInstanceOf, ==, clone, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	readSchema, MicroBatchReader, getEndOffset, commit, createDataReaderFactories, stop, getStartOffset, deserializeOffset, setOffsetRange.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(readSchema, MicroBatchReader, getEndOffset, commit, deserializeOffset, setOffsetRange)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(readSchema, MicroBatchReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(MicroBatchReader)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, ruleName, wait, $asInstanceOf, productArity, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, conf, translateFilter, isInstanceOf, convertStaticPartitions, <init>, DataSourceStrategy, cast, selectFilters, apply, ==, clone, planLater, $init$, copy, toString, resolver, logError, !=, getClass, logWarning, copy$default$1, getBucketId, ne, eq, productIterator, log, FindDataSourceTable, DataSourceAnalysis, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(asInstanceOf, conf, translateFilter, isInstanceOf, <init>, DataSourceStrategy, apply, ==, toString, getClass, getBucketId, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, DataSourceStrategy, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(conf, <init>, apply, clone, ne, FindDataSourceTable, DataSourceAnalysis)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(unapply, asInstanceOf, translateFilter, isInstanceOf, <init>, DataSourceStrategy, apply, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	classTag, exprEnc.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonUtils.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, unapply, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, subqueryExecutionContext, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, left, BinaryExecNode, generateTreeString$default$6, SparkPlan, allAttributes, nodeName, $isInstanceOf, doPrepare, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, LeafExecNode, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, UnaryExecNode, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, right, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala: Set(map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, schema, apply, flatMap, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, apply, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, SparkPlan, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, find, simpleString, children, execute, resetMetrics, map, treeString, asInstanceOf, left, SparkPlan, expressions, outputSet, transformUp, conf, isInstanceOf, <init>, outputOrdering, mapChildren, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, outputPartitioning, toString, !=, collect, output, ne, transform, executeToIterator, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala: Set(children, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, toString, transformDown, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, SparkPlan, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, left, SparkPlan, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, simpleString, children, execute, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, SparkPlan, nodeName, expressions, conf, isInstanceOf, child, references, <init>, outputOrdering, schema, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, !=, longMetric, logWarning, output, ne, UnaryExecNode, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(find, execute, map, asInstanceOf, SparkPlan, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, flatMap, LeafExecNode, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, getClass, longMetric, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, references, <init>, apply, flatMap, ==, sparkContext, collect)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(map, conf, <init>, apply, clone, foreach, sparkContext, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(unapply, map, asInstanceOf, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, mapChildren, apply, flatMap, ==, p, output, ne, transform)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala: Set(children, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, toString, transformDown, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala: Set(map, asInstanceOf, SparkPlan, expressions, isInstanceOf, <init>, apply, LeafExecNode, ==, sqlContext, sparkContext, toString, metrics, longMetric, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala: Set(find, children, execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, apply, ==, newMutableProjection, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(find, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, apply, ==, foreach, toString, executeTake, !=, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/SortAggregateExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, apply, flatMap, ==, newMutableProjection, subexpressionEliminationEnabled, sparkContext, outputPartitioning, toString, metrics, longMetric, output, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, apply, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala: Set(execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, isInstanceOf, references, <init>, apply, ==, sparkContext, toString, metrics, getClass, longMetric, output, executeBroadcast, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala: Set(execute, map, asInstanceOf, SparkPlan, allAttributes, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, newMutableProjection, subexpressionEliminationEnabled, sparkContext, outputPartitioning, toString, metrics, longMetric, output, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/CartesianProductExec.scala: Set(execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, newPredicate, foreach, sparkContext, toString, metrics, longMetric, output, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala: Set(execute, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, isInstanceOf, <init>, apply, ==, sparkContext, toString, metrics, longMetric, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala: Set(map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, schema, apply, flatMap, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, find, simpleString, children, execute, resetMetrics, map, treeString, asInstanceOf, left, SparkPlan, expressions, outputSet, transformUp, conf, isInstanceOf, <init>, outputOrdering, mapChildren, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, outputPartitioning, toString, !=, collect, output, ne, transform, executeToIterator, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(simpleString, map, asInstanceOf, SparkPlan, nodeName, expressions, isInstanceOf, child, <init>, schema, apply, LeafExecNode, ==, sqlContext, sparkContext, toString, metrics, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(execute, map, equals, asInstanceOf, SparkPlan, allAttributes, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, newMutableProjection, subexpressionEliminationEnabled, sparkContext, outputPartitioning, inputSet, toString, metrics, !=, longMetric, logWarning, output, ne, UnaryExecNode, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala: Set(unapply, execute, map, asInstanceOf, prepare, SparkPlan, expressions, conf, isInstanceOf, child, references, <init>, outputOrdering, schema, apply, flatMap, ==, sqlContext, newMutableProjection, foreach, subexpressionEliminationEnabled, outputPartitioning, toString, !=, collect, logWarning, output, ne, transform, UnaryExecNode, eq, newOrdering)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala: Set(map, asInstanceOf, synchronized, SparkPlan, isInstanceOf, <init>, apply, ==, sqlContext, sparkContext, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala: Set(children, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, toString, transformDown, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(asInstanceOf, SparkPlan, isInstanceOf, <init>, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, UnaryExecNode, withNewChildren, right, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala: Set(execute, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, apply, ==, sqlContext, sparkContext, outputPartitioning, toString, metrics, longMetric, output, UnaryExecNode, eq, newOrdering)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala: Set(simpleString, children, map, subqueries, asInstanceOf, SparkPlan, nodeName, isInstanceOf, child, <init>, apply, ==, metrics, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(unapply, find, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, ==, toString, metrics, !=, logWarning, output, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(map, asInstanceOf, SparkPlan, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, LeafExecNode, ==, sparkContext, outputPartitioning, toString, metrics, longMetric, output, transform, eq, productElement)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(find, execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, sqlContext, newPredicate, foreach, sparkContext, outputPartitioning, toString, metrics, !=, getClass, longMetric, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(unapply, map, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, foreach, sparkContext, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, sparkContext, toString, !=, collect, logWarning, output, transformAllExpressions, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, canonicalized, outputSet, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, sparkContext, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, outputOrdering, apply, ==, outputPartitioning, toString, executeTake, !=, output, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(find, execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, sparkContext, toString, logError, !=, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(find, execute, map, asInstanceOf, SparkPlan, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, flatMap, LeafExecNode, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, getClass, longMetric, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala: Set(newNaturalAscendingOrdering, unapply, execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, conf, isInstanceOf, child, references, <init>, outputOrdering, apply, ==, sqlContext, newPredicate, sparkContext, outputPartitioning, toString, metrics, !=, getClass, longMetric, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, ==, sqlContext, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, ==, foreach, sparkContext, toString, metrics, longMetric, output, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, schema, apply, flatMap, ==, sqlContext, toString, metrics, !=, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(map, asInstanceOf, synchronized, SparkPlan, conf, isInstanceOf, collectLeaves, <init>, apply, flatMap, ==, sqlContext, toString, metrics, collect, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala: Set(unapply, execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, isInstanceOf, <init>, apply, flatMap, ==, newPredicate, foreach, sparkContext, toString, metrics, !=, longMetric, output, executeBroadcast, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala: Set(find, execute, map, asInstanceOf, SparkPlan, expressions, canonicalized, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, sameResult, outputPartitioning, toString, output, executeBroadcast, transform, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, SparkPlan, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, references, <init>, apply, flatMap, ==, sparkContext, collect)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, simpleString, children, execute, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, SparkPlan, nodeName, expressions, conf, isInstanceOf, child, references, <init>, outputOrdering, schema, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, !=, longMetric, logWarning, output, ne, UnaryExecNode, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, foreach, p, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, find, simpleString, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, references, <init>, apply, ==, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, simpleString, execute, map, treeString, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, left, SparkPlan, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(find, execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, foreach, outputPartitioning, toString, metrics, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(find, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, p, toString, !=, collect, getClass, logWarning, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, LeafExecNode, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala: Set(children, execute, map, asInstanceOf, SparkPlan, expressions, canonicalized, outputSet, isInstanceOf, child, references, <init>, outputOrdering, apply, flatMap, executeCollect, LeafExecNode, ==, sqlContext, newPredicate, subexpressionEliminationEnabled, sparkContext, outputPartitioning, toString, metrics, !=, collect, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(unapply, execute, asInstanceOf, SparkPlan, expressions, isInstanceOf, <init>, schema, apply, ==, foreach, sparkContext, toString, logError, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, canonicalized, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, flatMap, LeafExecNode, ==, sqlContext, newPredicate, foreach, p, sparkContext, outputPartitioning, toString, innerChildren, longMetric, output, ne, transform, eq, right, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, isInstanceOf, <init>, apply, flatMap, ==, newPredicate, foreach, outputPartitioning, !=, output, ne, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(execute, map, asInstanceOf, synchronized, SparkPlan, expressions, isInstanceOf, <init>, schema, apply, ==, sparkContext, toString, !=, collect, output, transformAllExpressions, ne, transform, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala: Set(find, simpleString, map, subqueries, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, apply, executeCollect, ==, sameResult, toString, transformAllExpressions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala: Set(simpleString, map, expressions, <init>, apply, sparkContext, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala: Set(SparkPlan, <init>, ==, sparkContext, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(find, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, apply, ==, sameResult, foreach, toString, logWarning, output, transformDown, transformAllExpressions, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala: Set(map, SparkPlan, expressions, conf, <init>, ==, metrics, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, p, sparkContext, toString, metrics, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, outputSet, isInstanceOf, child, <init>, apply, flatMap, ==, sparkContext, outputPartitioning, toString, metrics, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, apply, flatMap, ==, outputPartitioning, toString, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, apply, ==, sparkContext, toString, output, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala: Set(find, map, asInstanceOf, left, SparkPlan, expressions, outputSet, collectFirst, isInstanceOf, references, <init>, apply, flatMap, ==, sparkContext, toString, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala: Set(execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, outputSet, isInstanceOf, child, <init>, outputOrdering, schema, apply, flatMap, ==, outputPartitioning, toString, collect, output, ne, UnaryExecNode, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala: Set(executeCollectIterator, map, asInstanceOf, SparkPlan, expressions, canonicalized, conf, isInstanceOf, child, <init>, apply, ==, sqlContext, sparkContext, toString, metrics, getClass, longMetric, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, <init>, schema, apply, LeafExecNode, ==, sqlContext, sparkContext, outputPartitioning, toString, longMetric, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala: Set(simpleString, execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, apply, ==, foreach, sparkContext, outputPartitioning, toString, !=, getClass, output, ne, transform, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, left, SparkPlan, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, SparkPlan, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(asInstanceOf, SparkPlan, isInstanceOf, <init>, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, UnaryExecNode, withNewChildren, right, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, references, <init>, apply, flatMap, ==, sparkContext, collect)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, find, simpleString, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, references, <init>, apply, ==, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/package.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, schema, apply, flatMap, ==, foreach, toString, !=, collect, getClass, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(find, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, apply, ==, foreach, toString, executeTake, !=, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, foreach, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, find, simpleString, children, execute, resetMetrics, map, treeString, asInstanceOf, left, SparkPlan, expressions, outputSet, transformUp, conf, isInstanceOf, <init>, outputOrdering, mapChildren, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, outputPartitioning, toString, !=, collect, output, ne, transform, executeToIterator, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(unapply, simpleString, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, getClass, output, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(unapply, map, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, foreach, sparkContext, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, apply, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, p, toString, collect, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, ==, sqlContext, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, sqlContext, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala: Set(map, equals, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, toString, !=, logWarning, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, simpleString, execute, map, treeString, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, flatMap, ==, sparkContext, toString, collect, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(unapply, map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, foreach, sparkContext, toString, !=, collect, getClass, logWarning, output, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, ==, toString, !=, collect, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala: Set(children, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, toString, transformDown, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, simpleString, children, execute, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, SparkPlan, nodeName, expressions, conf, isInstanceOf, child, references, <init>, outputOrdering, schema, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, !=, longMetric, logWarning, output, ne, UnaryExecNode, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(conf, <init>, apply, ==, foreach, sparkContext, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(unapply, map, schemaString, asInstanceOf, synchronized, expressions, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, clone, foreach, sparkContext, toString, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, references, <init>, apply, flatMap, ==, sparkContext, collect)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(execute, map, asInstanceOf, synchronized, SparkPlan, expressions, isInstanceOf, <init>, schema, apply, ==, sparkContext, toString, !=, collect, output, transformAllExpressions, ne, transform, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(unapply, map, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, foreach, sparkContext, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(simpleString, map, asInstanceOf, SparkPlan, nodeName, expressions, isInstanceOf, child, <init>, schema, apply, LeafExecNode, ==, sqlContext, sparkContext, toString, metrics, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, sparkContext, toString, !=, collect, logWarning, output, transformAllExpressions, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(find, execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, sparkContext, toString, logError, !=, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, left, SparkPlan, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(map, expressions, conf, <init>, schema, apply, sqlContext, foreach, sparkContext, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, simpleString, execute, map, treeString, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, foreach, sparkContext, toString, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, find, simpleString, children, execute, resetMetrics, map, treeString, asInstanceOf, left, SparkPlan, expressions, outputSet, transformUp, conf, isInstanceOf, <init>, outputOrdering, mapChildren, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, outputPartitioning, toString, !=, collect, output, ne, transform, executeToIterator, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(unapply, map, schemaString, asInstanceOf, synchronized, expressions, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, clone, foreach, sparkContext, toString, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, simpleString, children, execute, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, SparkPlan, nodeName, expressions, conf, isInstanceOf, child, references, <init>, outputOrdering, schema, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, !=, longMetric, logWarning, output, ne, UnaryExecNode, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, UnaryExecNode, withNewChildren, right, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, foreach, toString, metrics, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(map, conf, <init>, schema, apply, flatMap, foreach, sparkContext, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, schema, apply, flatMap, ==, foreach, toString, !=, collect, getClass, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(map, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, p, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala: Set(<init>, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(find, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, apply, ==, foreach, toString, executeTake, !=, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, map, asInstanceOf, synchronized, notifyAll, conf, isInstanceOf, <init>, apply, ==, foreach, sparkContext, toString, !=, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala: Set(conf, <init>, apply, flatMap)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala: Set(simpleString, map, asInstanceOf, conf, <init>, schema, apply, flatMap, ==, toString, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, find, simpleString, children, execute, resetMetrics, map, treeString, asInstanceOf, left, SparkPlan, expressions, outputSet, transformUp, conf, isInstanceOf, <init>, outputOrdering, mapChildren, schema, apply, flatMap, executeCollect, ==, sqlContext, foreach, outputPartitioning, toString, !=, collect, output, ne, transform, executeToIterator, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(simpleString, map, asInstanceOf, SparkPlan, nodeName, expressions, isInstanceOf, child, <init>, schema, apply, LeafExecNode, ==, sqlContext, sparkContext, toString, metrics, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(map, conf, <init>, apply, clone, foreach, sparkContext, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, UnaryExecNode, withNewChildren, right, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(map, expressions, conf, <init>, schema, apply, sqlContext, foreach, sparkContext, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala: Set(simpleString, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, schema, apply, ==, toString, collect, getClass, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(unapply, simpleString, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, getClass, output, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(map, asInstanceOf, isInstanceOf, <init>, ==, toString, logError, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(unapply, find, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, ==, toString, metrics, !=, logWarning, output, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(simpleString, map, asInstanceOf, left, expressions, <init>, schema, apply, ==, toString, !=, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(map, asInstanceOf, SparkPlan, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, LeafExecNode, ==, sparkContext, outputPartitioning, toString, metrics, longMetric, output, transform, eq, productElement)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(unapply, map, asInstanceOf, conf, <init>, schema, apply, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(find, execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, sqlContext, newPredicate, foreach, sparkContext, outputPartitioning, toString, metrics, !=, getClass, longMetric, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(unapply, map, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, foreach, sparkContext, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(find, simpleString, children, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, <init>, schema, apply, ==, transformExpressionsUp, foreach, toString, !=, collect, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala: Set(map, logTrace, conf, <init>, apply, ==, foreach, toString, !=, logWarning, output, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, sparkContext, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(find, execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, sparkContext, toString, logError, !=, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(find, execute, map, asInstanceOf, SparkPlan, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, flatMap, LeafExecNode, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, getClass, longMetric, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, apply, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(schemaString, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, p, toString, collect, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, ==, sqlContext, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala: Set(map, asInstanceOf, logTrace, expressions, conf, isInstanceOf, references, <init>, apply, flatMap, ==, toString, ne, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala: Set(map, expressions, conf, isInstanceOf, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, ==, p)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, schema, apply, flatMap, ==, sqlContext, toString, metrics, !=, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(map, asInstanceOf, synchronized, SparkPlan, conf, isInstanceOf, collectLeaves, <init>, apply, flatMap, ==, sqlContext, toString, metrics, collect, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(conf, isInstanceOf, <init>, apply, ==, foreach, sparkContext, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(unapply, map, schemaString, asInstanceOf, synchronized, expressions, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, clone, foreach, sparkContext, toString, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(conf, isInstanceOf, <init>, apply, ==, foreach, sparkContext, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(map, conf, <init>, apply, ==, foreach, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala: Set(unapply, map, conf, <init>, apply, flatMap, p, toString, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(unapply, map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, p, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(find, map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, sparkContext, toString, !=, getClass, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, foreach, p, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, toString, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala: Set(map, equals, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, toString, !=, logWarning, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, find, simpleString, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, references, <init>, apply, ==, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, simpleString, execute, map, treeString, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(find, execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, foreach, outputPartitioning, toString, metrics, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(find, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, p, toString, !=, collect, getClass, logWarning, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, LeafExecNode, ==, toString, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(map, treeString, schemaString, asInstanceOf, left, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, sparkContext, toString, collect, getClass, logWarning, ne, eq, right, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(map, asInstanceOf, logTrace, conf, isInstanceOf, <init>, apply, flatMap, ==, foreach, sparkContext, toString, metrics, !=, collect, getClass, logWarning, output, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, foreach, sparkContext, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(map, schemaString, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(<init>, apply, sqlContext, sparkContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(find, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, apply, ==, sameResult, foreach, toString, logWarning, output, transformDown, transformAllExpressions, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, foreach, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, flatMap, ==, sparkContext, toString, collect, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala: Set(map, conf, <init>, apply, ==, foreach, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(unapply, map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, foreach, sparkContext, toString, !=, collect, getClass, logWarning, output, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(unapply, find, simpleString, map, schemaString, executeQuery, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, <init>, schema, apply, ==, foreach, p, toString, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(unapply, find, map, asInstanceOf, expressions, isInstanceOf, references, <init>, apply, output, transformDown, transform)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, apply, ==, foreach, sparkContext, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala: Set(find, map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, ==, foreach, sparkContext, toString, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, sparkContext, toString, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, schema, apply, ==, toString, !=, collect, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, simpleString, children, execute, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, SparkPlan, nodeName, expressions, conf, isInstanceOf, child, references, <init>, outputOrdering, schema, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, !=, longMetric, logWarning, output, ne, UnaryExecNode, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala: Set(simpleString, children, map, subqueries, asInstanceOf, SparkPlan, nodeName, isInstanceOf, child, <init>, apply, ==, metrics, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, find, simpleString, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, references, <init>, apply, ==, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, left, SparkPlan, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, simpleString, children, execute, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, SparkPlan, nodeName, expressions, conf, isInstanceOf, child, references, <init>, outputOrdering, schema, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, !=, longMetric, logWarning, output, ne, UnaryExecNode, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala: Set(map, asInstanceOf, synchronized, SparkPlan, isInstanceOf, <init>, apply, ==, sqlContext, sparkContext, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, UnaryExecNode, withNewChildren, right, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, outputOrdering, apply, ==, outputPartitioning, toString, executeTake, !=, output, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, left, SparkPlan, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, UnaryExecNode, withNewChildren, right, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, simpleString, execute, map, treeString, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, ==, foreach, sparkContext, toString, metrics, longMetric, output, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala: Set(executeCollectIterator, map, asInstanceOf, SparkPlan, expressions, canonicalized, conf, isInstanceOf, child, <init>, apply, ==, sqlContext, sparkContext, toString, metrics, getClass, longMetric, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala: Set(simpleString, children, map, subqueries, asInstanceOf, SparkPlan, nodeName, isInstanceOf, child, <init>, apply, ==, metrics, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(map, conf, <init>, apply, clone, foreach, sparkContext, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, UnaryExecNode, withNewChildren, right, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(conf, <init>, apply, ==, foreach, sparkContext, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, simpleString, execute, map, treeString, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala: Set(simpleString, children, map, asInstanceOf, nodeName, isInstanceOf, <init>, apply, flatMap, ==, foreach, toString, metrics, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala: Set(execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, isInstanceOf, references, <init>, apply, ==, sparkContext, toString, metrics, getClass, longMetric, output, executeBroadcast, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(execute, map, equals, asInstanceOf, SparkPlan, allAttributes, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, newMutableProjection, subexpressionEliminationEnabled, sparkContext, outputPartitioning, inputSet, toString, metrics, !=, longMetric, logWarning, output, ne, UnaryExecNode, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala: Set(execute, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, apply, ==, sqlContext, sparkContext, outputPartitioning, toString, metrics, longMetric, output, UnaryExecNode, eq, newOrdering)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, outputOrdering, apply, ==, outputPartitioning, toString, executeTake, !=, output, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(find, execute, map, asInstanceOf, SparkPlan, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, flatMap, LeafExecNode, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, getClass, longMetric, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala: Set(newNaturalAscendingOrdering, unapply, execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, conf, isInstanceOf, child, references, <init>, outputOrdering, apply, ==, sqlContext, newPredicate, sparkContext, outputPartitioning, toString, metrics, !=, getClass, longMetric, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, find, simpleString, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, references, <init>, apply, ==, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, simpleString, execute, map, treeString, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala: Set(children, execute, map, asInstanceOf, SparkPlan, expressions, canonicalized, outputSet, isInstanceOf, child, references, <init>, outputOrdering, apply, flatMap, executeCollect, LeafExecNode, ==, sqlContext, newPredicate, subexpressionEliminationEnabled, sparkContext, outputPartitioning, toString, metrics, !=, collect, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, canonicalized, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, flatMap, LeafExecNode, ==, sqlContext, newPredicate, foreach, p, sparkContext, outputPartitioning, toString, innerChildren, longMetric, output, ne, transform, eq, right, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala: Set(simpleString, map, expressions, <init>, apply, sparkContext, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExpandExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, p, sparkContext, toString, metrics, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/GenerateExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, outputSet, isInstanceOf, child, <init>, apply, flatMap, ==, sparkContext, outputPartitioning, toString, metrics, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala: Set(execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, outputSet, isInstanceOf, child, <init>, outputOrdering, schema, apply, flatMap, ==, outputPartitioning, toString, collect, output, ne, UnaryExecNode, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, <init>, schema, apply, LeafExecNode, ==, sqlContext, sparkContext, outputPartitioning, toString, longMetric, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala: Set(simpleString, execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, apply, ==, foreach, sparkContext, outputPartitioning, toString, !=, getClass, output, ne, transform, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(find, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, p, toString, !=, collect, getClass, logWarning, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, references, <init>, apply, flatMap, ==, sparkContext, collect)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, foreach, toString, metrics, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala: Set(children, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, sqlContext, toString, transformDown, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, UnaryExecNode, withNewChildren, right, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(find, execute, map, asInstanceOf, left, BinaryExecNode, SparkPlan, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, sqlContext, newPredicate, foreach, sparkContext, outputPartitioning, toString, metrics, !=, getClass, longMetric, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, schema, apply, flatMap, ==, sqlContext, toString, metrics, !=, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(map, asInstanceOf, synchronized, SparkPlan, conf, isInstanceOf, collectLeaves, <init>, apply, flatMap, ==, sqlContext, toString, metrics, collect, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(<init>, apply, sqlContext, sparkContext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala: Set(unapply, map, asInstanceOf, synchronized, expressions, conf, isInstanceOf, <init>, flatMap, ==, foreach, toString, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala: Set(find, map, asInstanceOf, left, SparkPlan, expressions, outputSet, collectFirst, isInstanceOf, references, <init>, apply, flatMap, ==, sparkContext, toString, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, sparkContext, toString, !=, collect, logWarning, output, transformAllExpressions, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala: Set(unapply, map, asInstanceOf, synchronized, expressions, isInstanceOf, <init>, schema, apply, flatMap, ==, sqlContext, sparkContext, toString, collect, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(schemaString, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(execute, map, asInstanceOf, synchronized, SparkPlan, expressions, isInstanceOf, <init>, schema, apply, ==, sparkContext, toString, !=, collect, output, transformAllExpressions, ne, transform, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala: Set(children, execute, map, subqueryExecutionContext, executeQuery, asInstanceOf, doExecute, prepare, synchronized, left, BinaryExecNode, SparkPlan, nodeName, doPrepare, expressions, outputSet, makeCopy, conf, isInstanceOf, child, <init>, schema, apply, flatMap, executeCollect, LeafExecNode, ==, sqlContext, foreach, p, subexpressionEliminationEnabled, sparkContext, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, !=, collect, logWarning, ne, UnaryExecNode, waitForSubqueries, log, doExecuteBroadcast, right, newOrdering)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, SparkPlan, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, references, <init>, apply, flatMap, ==, sparkContext, collect)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, find, simpleString, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, references, <init>, apply, ==, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala: Set(find, simpleString, map, subqueries, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, apply, executeCollect, ==, sameResult, toString, transformAllExpressions, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(execute, map, asInstanceOf, synchronized, SparkPlan, expressions, isInstanceOf, <init>, schema, apply, ==, sparkContext, toString, !=, collect, output, transformAllExpressions, ne, transform, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, toString, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, sparkContext, toString, !=, collect, logWarning, output, transformAllExpressions, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(asInstanceOf, SparkPlan, isInstanceOf, <init>, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(unapply, execute, map, asInstanceOf, left, SparkPlan, expressions, canonicalized, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, flatMap, LeafExecNode, ==, sqlContext, newPredicate, foreach, p, sparkContext, outputPartitioning, toString, innerChildren, longMetric, output, ne, transform, eq, right, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(find, execute, map, asInstanceOf, SparkPlan, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, flatMap, LeafExecNode, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, metrics, getClass, longMetric, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, <init>, schema, apply, LeafExecNode, ==, sqlContext, sparkContext, outputPartitioning, toString, longMetric, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, SparkPlan, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(unapply, simpleString, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, getClass, output, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(unapply, map, asInstanceOf, conf, <init>, schema, apply, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(find, simpleString, children, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, <init>, schema, apply, ==, transformExpressionsUp, foreach, toString, !=, collect, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(schemaString, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, schema, ==, sqlContext, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(find, map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, sqlContext, sparkContext, toString, !=, getClass, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, child, <init>, apply, flatMap, ==, foreach, p, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, toString, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(map, schemaString, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(map, asInstanceOf, expressions, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(map, asInstanceOf, synchronized, SparkPlan, conf, isInstanceOf, collectLeaves, <init>, apply, flatMap, ==, sqlContext, toString, metrics, collect, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, sparkContext, toString, !=, collect, logWarning, output, transformAllExpressions, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(execute, map, asInstanceOf, SparkPlan, expressions, isInstanceOf, child, <init>, schema, apply, flatMap, ==, sqlContext, toString, metrics, !=, longMetric, output, ne, UnaryExecNode, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, left, SparkPlan, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, LeafExecNode, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(map, asInstanceOf, SparkPlan, nodeName, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, LeafExecNode, ==, sparkContext, outputPartitioning, toString, metrics, longMetric, output, transform, eq, productElement)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, asInstanceOf, left, SparkPlan, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, map, asInstanceOf, left, SparkPlan, expressions, conf, collectFirst, isInstanceOf, child, references, <init>, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(unapply, map, asInstanceOf, conf, <init>, schema, apply, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(find, simpleString, children, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, <init>, schema, apply, ==, transformExpressionsUp, foreach, toString, !=, collect, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, toString, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, ==, sqlContext, foreach, toString, !=, getClass, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(asInstanceOf, SparkPlan, isInstanceOf, <init>, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(simpleString, map, asInstanceOf, SparkPlan, nodeName, expressions, isInstanceOf, child, <init>, schema, apply, LeafExecNode, ==, sqlContext, sparkContext, toString, metrics, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, simpleString, execute, map, treeString, asInstanceOf, SparkPlan, conf, isInstanceOf, <init>, apply, executeCollect, ==, executeCollectPublic, toString, !=, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	agg, notify, count, wait, $asInstanceOf, equals, keyAs, flatMapGroups, asInstanceOf, synchronized, KeyValueGroupedDataset, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, mapGroups, flatMapGroupsWithState, toString, !=, getClass, mapValues, queryExecution, ne, aggUntyped, reduceGroups, eq, mapGroupsWithState, ##, finalize, keys, hashCode, cogroup.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(agg, count, asInstanceOf, KeyValueGroupedDataset, isInstanceOf, <init>, ==, toString, !=, queryExecution, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala: Set(KeyValueGroupedDataset)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, setStoreMetrics, treeString$default$2, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, nullLeft, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, copy$default$5, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, getStateInfo, copy$default$9, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, left, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, copy$default$8, getProgress, logTrace, asCode, canEqual, expressions, canonicalized, stateInfo, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, leftKeys, rightKeys, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, condition, schema, transformExpressionsDown, prettyJson, apply, flatMap, timeTakenMs, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, copy$default$7, subexpressionEliminationEnabled, StreamingSymmetricHashJoinExec, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, joinType, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, copy$default$6, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, eventTimeWatermark, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, nullRight, executeToIterator, right, ##, containsChild, stateWatermarkPredicates, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(execute, map, asInstanceOf, left, expressions, stateInfo, conf, isInstanceOf, leftKeys, rightKeys, <init>, condition, apply, ==, StreamingSymmetricHashJoinExec, sparkContext, output, transformAllExpressions, transform, withNewChildren, eventTimeWatermark, right, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, asInstanceOf, left, expressions, conf, isInstanceOf, leftKeys, rightKeys, <init>, outputOrdering, condition, schema, apply, ==, p, StreamingSymmetricHashJoinExec, sparkContext, outputPartitioning, copy, toString, joinType, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala: Set(map, asInstanceOf, expressions, stateInfo, conf, isInstanceOf, <init>, apply, flatMap, ==, foreach, StreamingSymmetricHashJoinExec, toString, metrics, !=, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/filters.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/filters.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/filters.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/filters.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/filters.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, GreaterThan, wait, copy$default$2, $asInstanceOf, In, productArity, equals, EqualNullSafe, asInstanceOf, StringStartsWith, synchronized, left, StringEndsWith, And, $isInstanceOf, canEqual, productPrefix, notifyAll, GreaterThanOrEqual, isInstanceOf, child, LessThan, references, StringContains, <init>, ==, clone, LessThanOrEqual, $init$, findReferences, Not, copy, values, toString, !=, attribute, getClass, copy$default$1, EqualTo, IsNotNull, Filter, ne, IsNull, value, eq, productIterator, right, ##, finalize, productElement, hashCode, Or.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala: Set(GreaterThan, In, equals, EqualNullSafe, asInstanceOf, left, And, GreaterThanOrEqual, isInstanceOf, child, LessThan, <init>, ==, LessThanOrEqual, Not, values, attribute, EqualTo, IsNotNull, Filter, IsNull, value, right, Or)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(GreaterThan, In, EqualNullSafe, asInstanceOf, StringStartsWith, left, StringEndsWith, And, GreaterThanOrEqual, isInstanceOf, child, LessThan, references, StringContains, <init>, ==, LessThanOrEqual, Not, values, toString, !=, EqualTo, IsNotNull, Filter, ne, IsNull, value, eq, right, Or)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(asInstanceOf, And, isInstanceOf, child, references, <init>, ==, values, Filter, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala: Set(GreaterThan, In, EqualNullSafe, asInstanceOf, StringStartsWith, left, StringEndsWith, And, GreaterThanOrEqual, isInstanceOf, child, LessThan, StringContains, <init>, ==, LessThanOrEqual, Not, values, toString, !=, attribute, EqualTo, IsNotNull, Filter, IsNull, value, right, Or)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, values, toString, attribute, getClass, Filter, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala: Set(GreaterThan, EqualNullSafe, asInstanceOf, left, And, GreaterThanOrEqual, isInstanceOf, child, LessThan, <init>, ==, LessThanOrEqual, Not, attribute, EqualTo, IsNotNull, Filter, IsNull, value, eq, right, Or)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala: Set(isInstanceOf, <init>, ==, Filter)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownFilters.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(isInstanceOf, <init>, ==, !=, getClass, Filter, value, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(isInstanceOf, <init>, ==, !=, getClass, Filter, value, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala: Set(<init>, !=, Filter, value)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, left, isInstanceOf, <init>, ==, toString, getClass, Filter, ne, value, eq, right, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceReaderHolder.scala: Set(asInstanceOf, canEqual, isInstanceOf, <init>, ==, getClass, Filter, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, getClass, Filter, value, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala: Set(<init>, Filter)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, Filter, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/SerializedOffset.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, productArity, equals, json, asInstanceOf, synchronized, SerializedOffset, $isInstanceOf, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, ==, clone, $init$, copy, toString, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(json, SerializedOffset, <init>, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceOffset.scala: Set(json, asInstanceOf, SerializedOffset, isInstanceOf, <init>, ==, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala: Set(json, asInstanceOf, SerializedOffset, isInstanceOf, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeqLog.scala: Set(json, asInstanceOf, SerializedOffset, isInstanceOf, <init>, ==, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/UnsafeRowSerializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/UnsafeRowSerializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/UnsafeRowSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/UnsafeRowSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/UnsafeRowSerializer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, defaultClassLoader, <init>, supportsRelocationOfSerializedObjects, setDefaultClassLoader, ==, clone, UnsafeRowSerializer, newInstance, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, UnsafeRowSerializer, toString, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, UnsafeRowSerializer, toString, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, readSchema, unapply, curried, supportBatch, isSplitable, wait, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, initializeLogIfNecessary, data, readParquetFootersInParallel, synchronized, $isInstanceOf, tupled, buildReaderWithPartitionValues, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, inferSchema, productPrefix, FileTypes, logName, notifyAll, isInstanceOf, prepareWrite, <init>, apply, ==, clone, $init$, copy$default$3, copy, metadata, ParquetFileFormat, mergeSchemasInParallel, toString, logError, !=, commonMetadata, getClass, logWarning, copy$default$1, shortName, readSchemaFromFooter, ne, vectorTypes, buildReader, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, data, inferSchema, isInstanceOf, <init>, apply, ==, ParquetFileFormat, toString, !=, getClass, logWarning, shortName, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(unapply, asInstanceOf, isInstanceOf, <init>, apply, ==, metadata, ParquetFileFormat, toString, !=, getClass, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(supportBatch, isSplitable, asInstanceOf, buildReaderWithPartitionValues, isInstanceOf, <init>, apply, ==, metadata, ParquetFileFormat, toString, getClass, ne, vectorTypes, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, tableIdent, verboseString, semanticHash, noscan, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy, inputSet, AnalyzeTableCommand, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, tableIdent, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, AnalyzeTableCommand, !=, collect, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, UserDefinedPythonFunction, name, wait, copy$default$2, $asInstanceOf, copy$default$5, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, builder, canEqual, copy$default$4, productPrefix, notifyAll, isInstanceOf, <init>, apply, ==, clone, $init$, copy$default$3, func, copy, udfDeterministic, toString, !=, getClass, copy$default$1, dataType, pythonEvalType, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(UserDefinedPythonFunction, name, asInstanceOf, builder, isInstanceOf, <init>, apply, ==, func, udfDeterministic, toString, dataType, pythonEvalType, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, decoder, wait, $asInstanceOf, equals, columnHeaderSize, all, gatherCompressibilityStats, asInstanceOf, synchronized, $isInstanceOf, typeId, Encoder, notifyAll, schemes, WithCompressionSchemes, AllCompressionSchemes, isInstanceOf, compressedSize, uncompressedSize, encoder, apply, supports, ==, compressionRatio, clone, compress, $init$, next, Decoder, toString, !=, getClass, decompress, ne, hasNext, eq, CompressionScheme, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(asInstanceOf, AllCompressionSchemes, isInstanceOf, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(gatherCompressibilityStats, Encoder, schemes, WithCompressionSchemes, compressedSize, encoder, apply, supports, ==, compressionRatio, compress, CompressionScheme)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala: Set(all, typeId, Encoder, schemes, WithCompressionSchemes, AllCompressionSchemes, compressedSize, uncompressedSize, apply, Decoder, CompressionScheme)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(gatherCompressibilityStats, Encoder, schemes, WithCompressionSchemes, compressedSize, encoder, apply, supports, ==, compressionRatio, compress, CompressionScheme)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(asInstanceOf, AllCompressionSchemes, isInstanceOf, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(gatherCompressibilityStats, Encoder, schemes, WithCompressionSchemes, compressedSize, encoder, apply, supports, ==, compressionRatio, compress, CompressionScheme)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala: Set(asInstanceOf, typeId, Encoder, isInstanceOf, apply, ==, clone, Decoder, toString, !=, CompressionScheme)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala: Set(decoder, apply, next, Decoder, decompress, hasNext, CompressionScheme)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(asInstanceOf, AllCompressionSchemes, isInstanceOf, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryRelation.scala: Set(asInstanceOf, isInstanceOf, apply, ==, next, toString, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(gatherCompressibilityStats, Encoder, schemes, WithCompressionSchemes, compressedSize, encoder, apply, supports, ==, compressionRatio, compress, CompressionScheme)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnBuilder.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, BuildRight, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, canEqual, BuildSide, productPrefix, notifyAll, isInstanceOf, BuildLeft, <init>, ==, clone, $init$, toString, !=, getClass, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala: Set(package, BuildRight, asInstanceOf, BuildSide, isInstanceOf, BuildLeft, <init>, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala: Set(package, asInstanceOf, BuildSide, isInstanceOf, <init>, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(package, BuildRight, asInstanceOf, BuildSide, isInstanceOf, BuildLeft, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala: Set(package, BuildRight, asInstanceOf, BuildSide, isInstanceOf, BuildLeft, <init>, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(package, asInstanceOf, BuildSide, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala: Set(package, BuildRight, asInstanceOf, BuildSide, isInstanceOf, BuildLeft, <init>, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcSerializer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, OrcSerializer, ==, clone, toString, !=, getClass, ne, serialize, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala: Set(<init>, OrcSerializer, serialize)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, truncateTable, $asInstanceOf, savePartition, equals, schemaString, asInstanceOf, initializeLogIfNecessary, getSchema, synchronized, $isInstanceOf, getSchema$default$3, getCommonJDBCType, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, tableExists, logName, notifyAll, isInstanceOf, createTable, ==, clone, getInsertStatement, $init$, schemaString$default$3, createConnectionFactory, toString, logError, !=, JdbcUtils, getClass, logWarning, resultSetToSparkInternalRows, ne, isCascadingTruncateTable, eq, resultSetToRows, log, saveTable, getCustomSchema, ##, finalize, hashCode, getSchemaOption, logDebug, logInfo, dropTable.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, JdbcUtils, logWarning, eq, getCustomSchema)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala: Set(truncateTable, tableExists, createTable, ==, createConnectionFactory, JdbcUtils, isCascadingTruncateTable, saveTable, getSchemaOption, dropTable)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala: Set(equals, asInstanceOf, getCommonJDBCType, isInstanceOf, ==, !=, JdbcUtils)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala: Set(asInstanceOf, getSchema, isInstanceOf, ==, createConnectionFactory, toString, !=, JdbcUtils, logWarning, resultSetToSparkInternalRows, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF1.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF1.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF1.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF1.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF1.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF1.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(call, UDF1)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF1)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF1.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/AggregateProcessor.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, AggregateProcessor, evaluate, synchronized, $isInstanceOf, notifyAll, initialize, isInstanceOf, <init>, apply, ==, clone, toString, !=, getClass, update, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala: Set(asInstanceOf, AggregateProcessor, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala: Set(AggregateProcessor, evaluate, initialize, <init>, apply, ==, !=, update)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, curried, wait, copy$default$2, $asInstanceOf, path, timestamp, size, commit, getBatch, productArity, equals, asInstanceOf, initializeLogIfNecessary, SeenFilesMap, synchronized, $isInstanceOf, FileStreamSource, tupled, logTrace, canEqual, FileEntry, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, stop, logName, notifyAll, purge, isInstanceOf, withBatchingLocked, <init>, schema, isNewFile, apply, ==, clone, Timestamp, $init$, copy$default$3, copy, toString, getOffset, logError, !=, getClass, logWarning, copy$default$1, seenFiles, batchId, sourceHasMetadata, ne, currentLogOffset, add, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(path, size, asInstanceOf, FileStreamSource, isInstanceOf, <init>, schema, apply, ==, toString, !=, getClass, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(path, size, FileStreamSource, FileEntry, <init>, apply, ==, logWarning, batchId, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/GroupState.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, setTimeoutDuration, asInstanceOf, synchronized, hasTimedOut, $isInstanceOf, getOption, notifyAll, isInstanceOf, GroupState, setTimeoutTimestamp, getCurrentProcessingTimeMs, remove, ==, clone, exists, getCurrentWatermarkMs, toString, !=, get, getClass, update, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/GroupStateImpl.scala: Set(setTimeoutDuration, asInstanceOf, hasTimedOut, getOption, GroupState, setTimeoutTimestamp, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/MapGroupsWithStateFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(asInstanceOf, GroupState, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/api/java/function/FlatMapGroupsWithStateFunction.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(asInstanceOf, hasTimedOut, isInstanceOf, setTimeoutTimestamp, remove, ==, exists, toString, !=, get, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala: Set(asInstanceOf, isInstanceOf, ==, exists, toString, get, update, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, SQLHistoryServerPlugin, <init>, createListeners, ==, clone, setupUI, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	readSchema, needsReconfiguration, setStartOffset, commit, mergeOffsets, createDataReaderFactories, stop, getStartOffset, ContinuousReader, deserializeOffset.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(ContinuousReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(ContinuousReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala: Set(commit, ContinuousReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(ContinuousReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(readSchema, ContinuousReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(readSchema, needsReconfiguration, setStartOffset, commit, mergeOffsets, stop, ContinuousReader, deserializeOffset)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(readSchema, createDataReaderFactories, ContinuousReader)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/catalog/Catalog.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	clearCache, notify, databaseExists, listTables, wait, $asInstanceOf, recoverPartitions, equals, createExternalTable, refreshByPath, getDatabase, asInstanceOf, synchronized, $isInstanceOf, tableExists, notifyAll, isInstanceOf, createTable, setCurrentDatabase, refreshTable, <init>, dropTempView, functionExists, uncacheTable, cacheTable, ==, currentDatabase, clone, dropGlobalTempView, getTable, isCached, toString, !=, getFunction, getClass, listFunctions, listColumns, ne, listDatabases, eq, Catalog, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, ne, Catalog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala: Set(clearCache, asInstanceOf, tableExists, isInstanceOf, <init>, uncacheTable, cacheTable, ==, toString, eq, Catalog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(clearCache, listTables, createTable, <init>, dropTempView, uncacheTable, cacheTable, currentDatabase, isCached, toString, ne, Catalog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(listTables, asInstanceOf, isInstanceOf, createTable, refreshTable, <init>, uncacheTable, cacheTable, ==, isCached, toString, !=, getClass, ne, eq, Catalog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(clearCache, databaseExists, listTables, asInstanceOf, tableExists, createTable, setCurrentDatabase, refreshTable, <init>, dropTempView, functionExists, ==, currentDatabase, dropGlobalTempView, getTable, isCached, getFunction, listFunctions, listColumns, ne, listDatabases, Catalog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, ne, Catalog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(refreshByPath, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, ne, eq, Catalog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(listTables, asInstanceOf, isInstanceOf, <init>, ==, currentDatabase, toString, !=, ne, Catalog)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(refreshByPath, asInstanceOf, isInstanceOf, refreshTable, <init>, ==, toString, eq, Catalog)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, count, wait, $asInstanceOf, equals, asInstanceOf, synchronized, avg, $isInstanceOf, notifyAll, typed, isInstanceOf, sumLong, ==, clone, toString, !=, getClass, ne, eq, sum, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	stop, BaseStreamingSource.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamProgress.scala: Set(BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(stop, BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/OffsetSeq.scala: Set(BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/BaseStreamingSource.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(stop, BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala: Set(BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(stop, BaseStreamingSource)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	resultBatch, notify, wait, equals, enableReturningBatches, getProgress, initBatch, listDirectory, notifyAll, initialize, getCurrentValue, <init>, nextKeyValue, nextBatch, VectorizedParquetRecordReader, toString, getCurrentKey, getClass, close, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(enableReturningBatches, initBatch, initialize, <init>, VectorizedParquetRecordReader, toString, getClass, close, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, SlidingWindowFunctionFrame, equals, asInstanceOf, prepare, synchronized, $isInstanceOf, notifyAll, isInstanceOf, UnboundedWindowFunctionFrame, UnboundedFollowingWindowFunctionFrame, <init>, ==, clone, OffsetWindowFunctionFrame, WindowFunctionFrame, toString, !=, getClass, ne, getNextOrNull, UnboundedPrecedingWindowFunctionFrame, eq, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala: Set(SlidingWindowFunctionFrame, asInstanceOf, prepare, isInstanceOf, UnboundedWindowFunctionFrame, UnboundedFollowingWindowFunctionFrame, <init>, ==, OffsetWindowFunctionFrame, WindowFunctionFrame, toString, !=, ne, UnboundedPrecedingWindowFunctionFrame, eq, write)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, read, withPadding, find, span, toBuffer, count, reduceOption, resume, wait, isAlive, foldRight, threadLocalRandomSeed, takeWhile, $asInstanceOf, setName, minBy, size, zip, toSet, getContextClassLoader, join, corresponds, :\, handleEndOfDataSection, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, writeIteratorToStream, setPriority, handleException, parkBlocker, toList, threadLocalRandomProbe, isTraversableAgain, asInstanceOf, sameElements, initializeLogIfNecessary, run, getPriority, shutdownOnTaskCompletion, reduceLeftOption, synchronized, sliding, partition, blockedOn, aggregate, $isInstanceOf, exception, forall, compute, mkString, checkAccess, min, newWriterThread, scanRight, envVars, fold, suspend, logTrace, nonEmpty, pythonVer, isTraceEnabled, initializeLogIfNecessary$default$2, getUncaughtExceptionHandler, getThreadGroup, stop, logName, notifyAll, /:, toIterator, getName, addString, to, collectFirst, isInterrupted, drop, isInstanceOf, getState, filter, getStackTrace, handlePythonException, GroupedIterator, <init>, toStream, destroy, pythonExec, max, buffered, ++, grouped, flatMap, take, WriterThread, reduceRight, ==, maxBy, indexWhere, accumulator, clone, setDaemon, slice, foreach, writeCommand, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, inheritableThreadLocals, reduce, padTo, $init$, toSeq, ReaderIterator, next, zipWithIndex, setContextClassLoader, threadLocals, ArrowPythonRunner, toString, copyToArray, length, seq, isDaemon, logError, !=, collect, handleTimingData, getClass, logWarning, hasDefiniteSize, setUncaughtExceptionHandler, countStackFrames, patch, start, foldLeft, contains, isEmpty, MonitorThread, ne, withPartial, getId, reversed, hasNext, indexOf, threadLocalRandomSecondarySeed, reduceLeft, eq, interrupt, newReaderIterator, sum, log, ##, scanLeft, finalize, hashCode, zipAll, logDebug, product, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala: Set(count, map, asInstanceOf, compute, mkString, drop, isInstanceOf, <init>, ==, next, ArrowPythonRunner, toString, length, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala: Set(map, asInstanceOf, compute, drop, isInstanceOf, GroupedIterator, <init>, grouped, flatMap, ==, toArray, ArrowPythonRunner, toString, length, isEmpty, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileStatusCache.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	invalidateAll, notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, FileStatusCache, notifyAll, isInstanceOf, <init>, ==, getOrCreate, clone, putLeafFiles, toString, getLeafFiles, NoopCache, !=, resetForTesting, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, FileStatusCache, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala: Set(asInstanceOf, FileStatusCache, isInstanceOf, <init>, ==, toString, NoopCache, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(invalidateAll, asInstanceOf, FileStatusCache, isInstanceOf, <init>, ==, getOrCreate)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, FileStatusCache, isInstanceOf, <init>, ==, getOrCreate, toString, NoopCache, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(invalidateAll, asInstanceOf, FileStatusCache, isInstanceOf, <init>, ==, putLeafFiles, toString, getLeafFiles, NoopCache, !=, getClass, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(FileStatusCache, <init>, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/RecordReaderIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/RecordReaderIterator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/RecordReaderIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/RecordReaderIterator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/RecordReaderIterator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, fold, nonEmpty, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, RecordReaderIterator, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, toString, copyToArray, length, seq, !=, collect, getClass, hasDefiniteSize, patch, close, foldLeft, contains, isEmpty, ne, withPartial, reversed, hasNext, indexOf, reduceLeft, eq, sum, ##, scanLeft, finalize, hashCode, zipAll, product.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileWholeTextReader.scala: Set(RecordReaderIterator, <init>, next, length, close, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(reduceOption, size, map, toMap, filterNot, asInstanceOf, forall, min, isInstanceOf, filter, RecordReaderIterator, <init>, max, ++, flatMap, ==, foreach, toArray, toSeq, toString, length, seq, collect, getClass, close, contains, isEmpty, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(map, asInstanceOf, forall, mkString, isInstanceOf, filter, RecordReaderIterator, <init>, ++, ==, foreach, length, !=, getClass, close, isEmpty, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala: Set(RecordReaderIterator, <init>, next, length, close, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF11.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF11.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF11.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF11.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF11.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF11.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF11)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF11.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createDataReaderFactories, readSchema, SupportsPushDownRequiredColumns, pruneColumns.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsPushDownRequiredColumns.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(readSchema, SupportsPushDownRequiredColumns, pruneColumns)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, UnivocityParser, parseStream, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, makeConverter, options, <init>, parseIterator, parse, ==, clone, $init$, toString, logError, !=, getClass, logWarning, tokenizeStream, ne, eq, makeConverter$default$3, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(UnivocityParser, isInstanceOf, options, <init>, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(UnivocityParser, parseStream, asInstanceOf, isInstanceOf, options, <init>, parseIterator, ==, toString, !=, tokenizeStream, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(UnivocityParser, asInstanceOf, isInstanceOf, options, <init>, parse, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	used, notify, readExternal, read, estimatedSize, wait, apply$default$4, writeExternal, $asInstanceOf, optimize, productArity, UnsafeHashedRelation, equals, taskMemoryManager, resultRow, asInstanceOf, HashedRelationBroadcastMode, HashedRelation, synchronized, allocateArray, $isInstanceOf, apply$default$3, asReadOnlyCopy, canEqual, LongToUnsafeRowMap, canonicalized, keyIsUnique, productPrefix, notifyAll, allocatePage, LongHashedRelation, key, isInstanceOf, freePage, getTotalMemoryConsumption, <init>, apply, ==, clone, acquireMemory, $init$, getAverageProbesPerLookup, freeArray, copy, mm, toString, !=, freeMemory, get, spill, getClass, getValue, copy$default$1, close, getMode, ne, transform, eq, productIterator, write, free, ##, finalize, productElement, hashCode, getUsed, append.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala: Set(estimatedSize, taskMemoryManager, asInstanceOf, HashedRelation, isInstanceOf, <init>, apply, ==, toString, get, close, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala: Set(estimatedSize, asInstanceOf, HashedRelationBroadcastMode, HashedRelation, asReadOnlyCopy, keyIsUnique, isInstanceOf, <init>, apply, ==, toString, get, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashJoin.scala: Set(asInstanceOf, HashedRelation, key, isInstanceOf, <init>, apply, ==, getAverageProbesPerLookup, !=, get, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala: Set(estimatedSize, asInstanceOf, HashedRelation, canonicalized, key, isInstanceOf, <init>, apply, ==, toString, getClass, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ColumnarBatchScan.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, produce, parent, simpleString, children, doProduce, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, $asInstanceOf, numberedTreeString, doConsume, evaluateVariables, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, usedInputs, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, needsUnsafeRowConversion, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, inputRDDs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, supportsBatch, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, ColumnarBatchScan, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, executeCollectPublic, inputSet, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, consume, innerChildren, collect, getClass, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, vectorTypes, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(map, asInstanceOf, expressions, conf, isInstanceOf, apply, ==, sqlContext, foreach, toString, !=, getClass, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(asInstanceOf, isInstanceOf, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(execute, map, asInstanceOf, expressions, canonicalized, conf, isInstanceOf, outputOrdering, schema, apply, flatMap, ==, supportsBatch, sqlContext, newPredicate, ColumnarBatchScan, foreach, p, sparkContext, outputPartitioning, toString, innerChildren, longMetric, output, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(find, execute, map, asInstanceOf, metricTerm, nodeName, expressions, conf, needsUnsafeRowConversion, isInstanceOf, outputOrdering, schema, apply, flatMap, ==, supportsBatch, sqlContext, ColumnarBatchScan, foreach, p, sparkContext, outputPartitioning, toString, metrics, consume, getClass, longMetric, output, ne, vectorTypes, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(execute, map, asInstanceOf, expressions, isInstanceOf, schema, apply, ==, supportsBatch, sqlContext, ColumnarBatchScan, sparkContext, outputPartitioning, toString, longMetric, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, nodeName, usedInputs, expressions, conf, isInstanceOf, inputRDDs, references, outputOrdering, schema, apply, ==, supportsBatch, sqlContext, foreach, p, sparkContext, outputPartitioning, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala: Set(simpleString, children, map, subqueries, asInstanceOf, nodeName, isInstanceOf, apply, ==, metrics, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(find, simpleString, map, asInstanceOf, expressions, isInstanceOf, references, apply, ==, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, references, schema, apply, flatMap, ==, foreach, p, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, calculateTotalSize, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, compareAndGetNewStats, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, calculateLocationSize, ==, clone, $init$, toString, logError, !=, CommandUtils, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo, updateTableStats.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala: Set(calculateTotalSize, asInstanceOf, compareAndGetNewStats, isInstanceOf, ==, toString, CommandUtils, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(calculateTotalSize, asInstanceOf, isInstanceOf, ==, toString, !=, CommandUtils, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, CommandUtils, getClass, ne, eq, log, updateTableStats)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(asInstanceOf, compareAndGetNewStats, isInstanceOf, calculateLocationSize, ==, toString, CommandUtils, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, CommandUtils, ne, eq, logInfo, updateTableStats)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, isInstanceOf, calculateLocationSize, ==, toString, !=, CommandUtils, getClass, logWarning, ne, eq, log, logDebug, logInfo, updateTableStats)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, size, equals, getAggregationBuffer, putAggregationBuffer, clear, asInstanceOf, synchronized, $isInstanceOf, AggregationBufferEntry, iterator, notifyAll, isInstanceOf, <init>, ==, clone, groupingKey, toString, !=, getClass, dumpToExternalSorter, ne, ObjectAggregationMap, aggregationBuffer, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator.scala: Set(size, getAggregationBuffer, putAggregationBuffer, asInstanceOf, AggregationBufferEntry, iterator, isInstanceOf, <init>, ==, groupingKey, !=, dumpToExternalSorter, ne, ObjectAggregationMap, aggregationBuffer)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, JsonUtils, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, sample, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(JsonUtils, sample, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/LongOffset.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, wait, $asInstanceOf, productArity, equals, json, asInstanceOf, synchronized, $isInstanceOf, canEqual, productPrefix, notifyAll, -, isInstanceOf, <init>, offset, apply, ==, clone, $init$, LongOffset, copy, toString, +, !=, getClass, copy$default$1, ne, eq, convert, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(json, -, <init>, offset, apply, ==, LongOffset, toString, +, !=, convert)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/memory.scala: Set(unapply, asInstanceOf, synchronized, -, isInstanceOf, <init>, offset, apply, ==, LongOffset, copy, toString, +, ne, eq, convert)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/socket.scala: Set(asInstanceOf, synchronized, -, isInstanceOf, <init>, offset, apply, ==, LongOffset, +, !=, ne, convert)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	partitionSchema, notify, InMemoryFileIndex, refresh, partitionSpec, wait, <init>$default$5, $asInstanceOf, inferPartitioning, listFiles, equals, listLeafFiles, inputFiles, asInstanceOf, initializeLogIfNecessary, metadataOpsTimeNs, synchronized, $isInstanceOf, shouldFilterOut, logTrace, hadoopConf, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, rootPaths, leafFiles, <init>, allFiles, ==, clone, sizeInBytes, $init$, leafDirToChildrenFiles, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, InMemoryFileIndex, asInstanceOf, hadoopConf, isInstanceOf, <init>, allFiles, ==, sizeInBytes, toString, !=, getClass, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, InMemoryFileIndex, asInstanceOf, isInstanceOf, <init>, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(partitionSchema, InMemoryFileIndex, asInstanceOf, hadoopConf, isInstanceOf, <init>, allFiles, ==, sizeInBytes, toString, !=, getClass, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(partitionSchema, InMemoryFileIndex, asInstanceOf, isInstanceOf, <init>, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(InMemoryFileIndex, asInstanceOf, synchronized, logTrace, hadoopConf, isInstanceOf, <init>, allFiles, ==, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(partitionSchema, InMemoryFileIndex, partitionSpec, listFiles, inputFiles, asInstanceOf, metadataOpsTimeNs, hadoopConf, isInstanceOf, rootPaths, <init>, ==, sizeInBytes)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	stringWithStats, notify, simpleString, wait, $asInstanceOf, equals, analyzed, asInstanceOf, assertAnalyzed, assertSupported, synchronized, $isInstanceOf, codegen, codegenToSeq, withCachedData, notifyAll, debug, isInstanceOf, <init>, prepareForExecution, stringOrError, toRdd, ==, logical, sparkPlan, clone, sparkSession, optimizedPlan, planner, toString, !=, getClass, ne, hiveResultString, QueryExecution, eq, executedPlan, preparations, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, toRdd, ==, logical, sparkSession, toString, !=, ne, QueryExecution, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, logical, sparkSession, toString, !=, ne, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(stringWithStats, simpleString, asInstanceOf, codegen, debug, isInstanceOf, <init>, ==, logical, sparkSession, toString, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(analyzed, asInstanceOf, isInstanceOf, <init>, ==, logical, sparkSession, toString, !=, ne, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(<init>, toRdd, sparkSession, toString, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala: Set(analyzed, asInstanceOf, assertAnalyzed, isInstanceOf, <init>, ==, logical, sparkSession, toString, !=, getClass, ne, QueryExecution, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala: Set(<init>, toRdd, sparkSession, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, logical, sparkSession, toString, !=, ne, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, analyzed, asInstanceOf, assertAnalyzed, synchronized, notifyAll, isInstanceOf, <init>, ==, sparkSession, toString, !=, ne, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(simpleString, analyzed, asInstanceOf, assertAnalyzed, codegen, isInstanceOf, <init>, toRdd, ==, logical, sparkSession, optimizedPlan, toString, !=, ne, QueryExecution, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(stringWithStats, simpleString, asInstanceOf, codegen, debug, isInstanceOf, <init>, ==, logical, sparkSession, toString, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(<init>, logical, clone, planner, ne, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(asInstanceOf, withCachedData, isInstanceOf, <init>, ==, logical, sparkSession, optimizedPlan, planner, QueryExecution, preparations)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(simpleString, analyzed, asInstanceOf, isInstanceOf, <init>, ==, logical, sparkSession, toString, !=, getClass, ne, QueryExecution, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, toRdd, ==, logical, sparkSession, toString, QueryExecution, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(analyzed, asInstanceOf, isInstanceOf, <init>, ==, sparkSession, toString, !=, QueryExecution, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(simpleString, analyzed, asInstanceOf, <init>, ==, logical, sparkSession, toString, !=, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(analyzed, asInstanceOf, <init>, toRdd, ==, logical, sparkSession, ne, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, logical, sparkSession, toString, !=, ne, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, toRdd, ==, logical, sparkSession, toString, getClass, ne, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(analyzed, asInstanceOf, isInstanceOf, <init>, ==, logical, sparkSession, toString, !=, ne, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(<init>, ==, logical, planner, !=, ne, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(asInstanceOf, isInstanceOf, <init>, toRdd, ==, logical, sparkSession, toString, !=, QueryExecution, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, logical, sparkSession, toString, ne, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(analyzed, asInstanceOf, isInstanceOf, <init>, toRdd, ==, logical, sparkSession, toString, ne, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(analyzed, asInstanceOf, isInstanceOf, <init>, ==, logical, sparkSession, toString, !=, getClass, ne, QueryExecution, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala: Set(asInstanceOf, isInstanceOf, <init>, toRdd, ==, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, toRdd, ==, logical, sparkSession, toString, !=, ne, QueryExecution, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala: Set(simpleString, asInstanceOf, codegen, isInstanceOf, <init>, ==, sparkSession, toString, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala: Set(<init>, ==, sparkSession, toString, QueryExecution, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, logical, sparkSession, toString, ne, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala: Set(<init>, QueryExecution)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala: Set(simpleString, asInstanceOf, codegen, debug, isInstanceOf, <init>, ==, toString, !=, getClass, ne, QueryExecution, eq, executedPlan)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newTaskTempFile, notify, wait, $asInstanceOf, setupJob, equals, newTaskTempFileAbsPath, setupCommitter, abortJob, abortTask, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, commitJob, <init>, commitTask, ==, clone, $init$, deleteWithJob, toString, logError, !=, getClass, logWarning, SQLHadoopMapReduceCommitProtocol, setupTask, ne, eq, log, onTaskCommit, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createDataReaderFactories, readSchema, outputPartitioning, SupportsReportPartitioning.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(createDataReaderFactories, readSchema, outputPartitioning, SupportsReportPartitioning)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportPartitioning.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, decoder, IntDelta, wait, $asInstanceOf, BooleanBitSet, MAX_DICT_SIZE, productArity, equals, gatherCompressibilityStats, PassThrough, asInstanceOf, RunLengthEncoding, synchronized, $isInstanceOf, canEqual, typeId, Encoder, elementNum, productPrefix, LongDelta, notifyAll, isInstanceOf, compressedSize, uncompressedSize, <init>, encoder, BITS_PER_LONG, supports, _uncompressedSize, ==, compressionRatio, clone, compress, $init$, next, Decoder, toString, !=, getClass, decompress, ne, hasNext, DictionaryEncoding, eq, productIterator, _compressedSize, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(gatherCompressibilityStats, PassThrough, Encoder, compressedSize, <init>, encoder, supports, ==, compressionRatio, compress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala: Set(IntDelta, BooleanBitSet, PassThrough, RunLengthEncoding, typeId, Encoder, LongDelta, compressedSize, uncompressedSize, <init>, Decoder, DictionaryEncoding)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, register, wait, $asInstanceOf, equals, clear, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, onSuccess, <init>, unregister, ==, QueryExecutionListener, clone, $init$, toString, logError, !=, getClass, logWarning, onFailure, ne, eq, log, ##, finalize, ExecutionListenerManager, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(asInstanceOf, isInstanceOf, onSuccess, <init>, ==, toString, !=, onFailure, ne, ExecutionListenerManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(clear, <init>, clone, ne, ExecutionListenerManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(<init>, toString, ne, ExecutionListenerManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(<init>, ==, QueryExecutionListener, !=, ne, ExecutionListenerManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, isInstanceOf, <init>, ==, clone, toString, logWarning, ne, ExecutionListenerManager)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(asInstanceOf, isInstanceOf, onSuccess, <init>, ==, toString, onFailure, ne, ExecutionListenerManager)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	dictionaryIds, getInterval, notify, reserveDictionaryIds, putShort, arrayData, appendNulls, appendStruct, putShorts, getShort, isArray, putDecimal, wait, getUTF8String, reserve, $assertionsDisabled, reserveNewColumn, getBooleans, getArrayOffset, appendLong, appendShort, putNull, equals, putLongsLittleEndian, putLong, hasNull, appendShorts, isNullAt, putDoubles, dictionary, getDictId, putNulls, getDouble, putFloats, putBoolean, appendFloats, childColumns, appendNotNulls, getStruct, appendFloat, getFloats, getBytesAsUTF8String, putInt, putDouble, putIntsLittleEndian, appendDouble, appendBooleans, reserveInternal, setDictionary, getMap, putBytes, getDictionaryIds, notifyAll, getByte, appendByteArray, isConstant, appendBoolean, WritableColumnVector, appendByte, putArray, getArray, putLongs, <init>, DEFAULT_ARRAY_LENGTH, getInts, elementsAppended, putNotNull, appendArray, MAX_CAPACITY, getDoubles, appendBytes, getFloat, getDecimal, appendInt, appendNull, putBooleans, hasDictionary, getInt, getBytes, appendDoubles, getShorts, putNotNulls, appendInts, getLongs, reset, putByteArray, putFloat, toString, getBoolean, numNulls, getClass, setIsConstant, dataType, close, getArrayLength, appendLongs, getChild, getElementsAppended, getLong, hashCode, appendNotNull, getBinary, putInts, putByte, capacity.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala: Set(<init>, dataType)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(<init>, toString, getBoolean, getClass, dataType, close, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(WritableColumnVector, <init>, toString, dataType, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala: Set(WritableColumnVector, <init>, dataType)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala: Set(dictionaryIds, reserveDictionaryIds, putShort, putShorts, getShort, putNull, putLong, isNullAt, putDoubles, dictionary, putFloats, putBoolean, putInt, setDictionary, putBytes, getByte, WritableColumnVector, putLongs, <init>, putBooleans, getInt, toString, dataType, getLong, putInts, putByte, capacity)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala: Set(WritableColumnVector, getInt, capacity)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(WritableColumnVector, <init>, toString, dataType, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala: Set(WritableColumnVector, <init>, getInt)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(<init>, toString, getBoolean, getClass, dataType, close, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(WritableColumnVector, <init>, toString, dataType, close)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/cache.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, UncacheTableCommand, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, tableIdent, verboseString, semanticHash, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, CacheTableCommand, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, ifExists, $init$, copy$default$3, copy, inputSet, toString, ClearCacheCommand, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, isLazy, log, plan, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(UncacheTableCommand, children, tableIdent, map, asInstanceOf, expressions, CacheTableCommand, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, ClearCacheCommand, !=, collect, logWarning, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/progress.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, endOffset, name, wait, $asInstanceOf, numRowsTotal, timestamp, equals, json, description, numInputRows, StreamingQueryProgress, asInstanceOf, synchronized, $isInstanceOf, notifyAll, StateOperatorProgress, isInstanceOf, SinkProgress, inputRowsPerSecond, sink, <init>, id, prettyJson, durationMs, stateOperators, ==, clone, numRowsUpdated, eventTime, copy, startOffset, toString, !=, getClass, batchId, ne, sources, memoryUsedBytes, runId, jsonValue, eq, processedRowsPerSecond, SourceProgress, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala: Set(StreamingQueryProgress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(name, json, numInputRows, StreamingQueryProgress, asInstanceOf, synchronized, StateOperatorProgress, isInstanceOf, SinkProgress, sink, <init>, id, stateOperators, ==, copy, toString, ne, sources, runId, eq, SourceProgress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala: Set(name, StreamingQueryProgress, <init>, id, ==, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(name, asInstanceOf, StateOperatorProgress, isInstanceOf, <init>, ==, toString, ne, memoryUsedBytes, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala: Set(name, StreamingQueryProgress, synchronized, inputRowsPerSecond, <init>, durationMs, processedRowsPerSecond)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala: Set(name, StreamingQueryProgress, <init>, id, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala: Set(StreamingQueryProgress, asInstanceOf, synchronized, isInstanceOf, <init>, runId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, tableIdent, verboseString, semanticHash, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, columnNames, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy, inputSet, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, AnalyzeColumnCommand, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, tableIdent, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, !=, collect, logWarning, ne, AnalyzeColumnCommand)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	WriteSupport, createWriter.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(WriteSupport, createWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastHashJoinExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, produce, parent, simpleString, children, doProduce, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, doConsume, evaluateVariables, copy$default$5, resetMetrics, printSchema, buildSideKeyGenerator, join, map, productArity, verboseStringWithSuffix, equals, treeString, buildSide, schemaString, argString, streamedPlan, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, left, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, usedInputs, buildPlan, logTrace, asCode, canEqual, expressions, canonicalized, createResultProjection, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, leftKeys, inputRDDs, rightKeys, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, condition, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, buildKeys, p, jsonFields, copy$default$7, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, BroadcastHashJoinExec, copy$default$3, copy, executeCollectPublic, inputSet, streamSideKeyGenerator, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, consume, innerChildren, collect, joinType, getClass, supportCodegen, streamedKeys, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, copy$default$6, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, executeToIterator, right, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, buildSide, asInstanceOf, left, expressions, conf, isInstanceOf, leftKeys, rightKeys, <init>, outputOrdering, condition, schema, apply, ==, p, sparkContext, outputPartitioning, BroadcastHashJoinExec, copy, toString, joinType, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, buildSide, asInstanceOf, left, expressions, transformUp, conf, isInstanceOf, leftKeys, rightKeys, <init>, outputOrdering, condition, apply, flatMap, ==, foreach, outputPartitioning, BroadcastHashJoinExec, toString, joinType, ne, requiredChildOrdering, withNewChildren, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(find, produce, parent, simpleString, children, doProduce, execute, doConsume, evaluateVariables, map, treeString, executeQuery, asInstanceOf, doExecute, generateTreeString, nodeName, usedInputs, expressions, conf, isInstanceOf, inputRDDs, references, <init>, outputOrdering, schema, apply, ==, sqlContext, foreach, p, sparkContext, outputPartitioning, BroadcastHashJoinExec, toString, evaluateRequiredVariables, metrics, !=, consume, supportCodegen, longMetric, logWarning, output, needStopCheck, needCopyResult, ne, withNewChildren, eq, doExecuteBroadcast, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	customOperatorOptimizationRules, resourceLoader, notify, customCheckRules, listenerManager, loadResource, optimizer, catalog, wait, newHadoopConfWithOptions, analyzer, $asInstanceOf, executePlan, SessionStateBuilder, addJar, equals, parentState, asInstanceOf, synchronized, newHadoopConf, customPostHocResolutionRules, $isInstanceOf, newBuilder, build, customPlanningStrategies, udfRegistration, notifyAll, conf, isInstanceOf, extensions, refreshTable, mergeSparkConf, sqlParser, <init>, ==, clone, createQueryExecution, SessionResourceLoader, session, functionRegistry, planner, toString, !=, NewBuilder, getClass, customResolutionRules, streamingQueryManager, SessionState, ne, eq, experimentalMethods, createClone, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala: Set(catalog, asInstanceOf, isInstanceOf, <init>, ==, toString, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(newHadoopConfWithOptions, conf, <init>, toString, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala: Set(catalog, executePlan, asInstanceOf, build, conf, isInstanceOf, <init>, ==, session, toString, !=, getClass, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, newHadoopConf, isInstanceOf, <init>, ==, toString, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala: Set(analyzer, <init>, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, asInstanceOf, synchronized, newHadoopConf, notifyAll, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(catalog, analyzer, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala: Set(newHadoopConf, conf, <init>, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala: Set(asInstanceOf, newHadoopConf, conf, <init>, ==, toString, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(listenerManager, optimizer, catalog, analyzer, executePlan, asInstanceOf, conf, isInstanceOf, sqlParser, <init>, ==, toString, !=, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(executePlan, asInstanceOf, isInstanceOf, <init>, ==, toString, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(customOperatorOptimizationRules, resourceLoader, customCheckRules, listenerManager, optimizer, catalog, analyzer, parentState, newHadoopConf, customPostHocResolutionRules, newBuilder, build, customPlanningStrategies, udfRegistration, conf, extensions, mergeSparkConf, sqlParser, <init>, clone, createQueryExecution, SessionResourceLoader, session, functionRegistry, planner, NewBuilder, customResolutionRules, streamingQueryManager, SessionState, ne, experimentalMethods, createClone)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(optimizer, asInstanceOf, conf, isInstanceOf, <init>, ==, planner, SessionState, experimentalMethods)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(listenerManager, catalog, conf, <init>, toString, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, getClass, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(catalog, asInstanceOf, newHadoopConf, newBuilder, build, conf, isInstanceOf, refreshTable, <init>, ==, toString, !=, getClass, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(catalog, executePlan, asInstanceOf, isInstanceOf, <init>, ==, toString, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, newHadoopConf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(executePlan, asInstanceOf, <init>, ==, toString, !=, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, session, toString, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(catalog, executePlan, asInstanceOf, conf, refreshTable, sqlParser, <init>, ==, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(asInstanceOf, newHadoopConf, conf, isInstanceOf, <init>, ==, toString, !=, getClass, streamingQueryManager, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(executePlan, asInstanceOf, newHadoopConf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala: Set(newHadoopConf, conf, <init>, ==, toString, !=, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(newHadoopConfWithOptions, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(catalog, newHadoopConfWithOptions, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, getClass, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala: Set(catalog, asInstanceOf, isInstanceOf, <init>, ==, toString, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(catalog, executePlan, asInstanceOf, conf, isInstanceOf, <init>, ==, session, toString, !=, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala: Set(newHadoopConfWithOptions, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala: Set(newHadoopConfWithOptions, conf, isInstanceOf, <init>, ==, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(catalog, asInstanceOf, newHadoopConf, conf, isInstanceOf, <init>, ==, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(asInstanceOf, synchronized, conf, isInstanceOf, <init>, ==, toString, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(conf, isInstanceOf, <init>, ==, !=, getClass, SessionState, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(listenerManager, catalog, asInstanceOf, synchronized, build, udfRegistration, conf, isInstanceOf, extensions, sqlParser, <init>, ==, clone, session, toString, streamingQueryManager, SessionState, ne, experimentalMethods)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(conf, isInstanceOf, <init>, ==, !=, getClass, SessionState, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(conf, <init>, ==, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala: Set(catalog, newHadoopConf, conf, <init>, toString, !=, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(asInstanceOf, isInstanceOf, sqlParser, <init>, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(analyzer, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, getClass, streamingQueryManager, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(catalog, newHadoopConfWithOptions, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(listenerManager, catalog, executePlan, asInstanceOf, conf, isInstanceOf, refreshTable, sqlParser, <init>, ==, session, toString, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala: Set(equals, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(analyzer, asInstanceOf, isInstanceOf, <init>, ==, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(optimizer, analyzer, asInstanceOf, conf, isInstanceOf, <init>, ==, planner, toString, !=, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(catalog, asInstanceOf, newHadoopConf, conf, isInstanceOf, <init>, ==, toString, !=, getClass, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, session, toString, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, newHadoopConf, conf, isInstanceOf, <init>, ==, toString, getClass, SessionState, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, session, toString, !=, getClass, SessionState, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, !=, getClass, SessionState, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(<init>, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(executePlan, asInstanceOf, newHadoopConf, conf, isInstanceOf, <init>, ==, toString, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala: Set(resourceLoader, addJar, asInstanceOf, isInstanceOf, <init>, ==, SessionResourceLoader, toString, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala: Set(conf, <init>, ==, toString, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, refreshTable, <init>, ==, toString, !=, getClass, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, getClass, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(catalog, analyzer, asInstanceOf, isInstanceOf, <init>, SessionState)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala: Set(analyzer, asInstanceOf, isInstanceOf, <init>, ==, getClass, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(catalog, asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(catalog, asInstanceOf, isInstanceOf, refreshTable, <init>, ==, toString, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(asInstanceOf, conf, isInstanceOf, <init>, ==, toString, !=, SessionState, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala: Set(catalog, asInstanceOf, isInstanceOf, <init>, ==, toString, !=, SessionState, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getInterval, notify, reserveDictionaryIds, putShort, arrayData, appendNulls, appendStruct, putShorts, getShort, putDecimal, wait, getUTF8String, reserve, $assertionsDisabled, reserveNewColumn, getBooleans, getArrayOffset, appendLong, appendShort, putNull, equals, putLongsLittleEndian, putLong, hasNull, appendShorts, isNullAt, putDoubles, allocateColumns, getDictId, putNulls, getDouble, putFloats, putBoolean, appendFloats, appendNotNulls, getStruct, appendFloat, getFloats, getBytesAsUTF8String, putInt, putDouble, putIntsLittleEndian, OnHeapColumnVector, appendDouble, appendBooleans, reserveInternal, setDictionary, getMap, putBytes, getDictionaryIds, notifyAll, getByte, appendByteArray, appendBoolean, appendByte, putArray, getArray, putLongs, <init>, getInts, putNotNull, appendArray, getDoubles, appendBytes, getFloat, getDecimal, appendInt, appendNull, putBooleans, hasDictionary, getInt, getBytes, appendDoubles, getShorts, putNotNulls, appendInts, getLongs, reset, putByteArray, putFloat, toString, getBoolean, numNulls, getClass, setIsConstant, dataType, close, getArrayLength, appendLongs, getChild, getElementsAppended, getLong, hashCode, appendNotNull, getBinary, putInts, putByte.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala: Set(OnHeapColumnVector, <init>, dataType)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(OnHeapColumnVector, <init>, toString, getBoolean, getClass, dataType, close, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(allocateColumns, OnHeapColumnVector, <init>, toString, dataType, close)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, columnPartition, wait, copy$default$2, $asInstanceOf, lowerBound, insert, productArity, equals, JDBCRelation, buildScan, JDBCPartitioningInfo, asInstanceOf, initializeLogIfNecessary, unhandledFilters, synchronized, $isInstanceOf, logTrace, canEqual, copy$default$4, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, isInstanceOf, upperBound, <init>, schema, apply, ==, sqlContext, clone, sparkSession, sizeInBytes, $init$, copy$default$3, copy, column, toString, logError, !=, getClass, logWarning, copy$default$1, ne, jdbcOptions, numPartitions, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo, parts, needConversion.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala: Set(columnPartition, lowerBound, JDBCRelation, JDBCPartitioningInfo, upperBound, <init>, schema, apply, ==, sqlContext, sparkSession, jdbcOptions, numPartitions, parts)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(lowerBound, JDBCRelation, asInstanceOf, isInstanceOf, upperBound, <init>, schema, apply, ==, sparkSession, toString, !=, ne, numPartitions, parts)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, notifyAll, valueArray, <init>, keyArray, ColumnarMap, numElements, foreach, copy, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarArray.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, fileCleanupDelayMs, metadataPath, isCompactionBatch, parseVersion, wait, $asInstanceOf, getBatchIdFromFileName, equals, isDeletingExpiredLog, asInstanceOf, initializeLogIfNecessary, getAllValidBatches, minBatchesToRetain, CompactibleFileStreamLog, synchronized, $isInstanceOf, getValidBatchesBeforeCompactionBatch, nextCompactionBatchId, logTrace, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, logName, notifyAll, purge, isInstanceOf, COMPACT_FILE_SUFFIX, getLatest, <init>, compactLogs, allFiles, defaultCompactInterval, isBatchFile, ==, clone, purgeAfter, batchIdToPath, compactInterval, $init$, fileManager, toString, logError, !=, get, getClass, logWarning, batchFilesFilter, deriveCompactInterval, ne, serialize, pathToBatchId, add, getOrderedBatchFiles, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala: Set(asInstanceOf, <init>, toString, !=, add, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(<init>, allFiles, ==, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(asInstanceOf, isInstanceOf, getLatest, <init>, ==, toString, !=, get, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(metadataPath, asInstanceOf, synchronized, logTrace, purge, isInstanceOf, getLatest, <init>, allFiles, ==, toString, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(metadataPath, asInstanceOf, synchronized, logTrace, purge, isInstanceOf, getLatest, <init>, allFiles, ==, toString, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala: Set(fileCleanupDelayMs, isDeletingExpiredLog, asInstanceOf, CompactibleFileStreamLog, isInstanceOf, <init>, defaultCompactInterval, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(fileCleanupDelayMs, isCompactionBatch, isDeletingExpiredLog, getAllValidBatches, CompactibleFileStreamLog, getLatest, <init>, defaultCompactInterval, ==, compactInterval, get, logWarning, ne, add)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol.scala: Set(asInstanceOf, <init>, toString, !=, add, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetadataLogFileIndex.scala: Set(<init>, allFiles, ==, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, toJavaIntMap, ColumnVectorUtils, wait, $assertionsDisabled, equals, notifyAll, <init>, toBatch, toString, getClass, toJavaIntArray, hashCode, populate.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	DDL_TIME, notify, propKeys, serdeClassName, treeString$default$2, find, isView, simpleString, children, partitionSpecsAndLocs, refresh, maxRowsPerPartition, checkDataColNames, numFiles, verboseString, partitionSpec, semanticHash, TOTAL_SIZE, wait, stats, AlterTableRecoverPartitionsCommand, copy$default$2, $asInstanceOf, location, path, numberedTreeString, extended, copy$default$5, AlterTableSetPropertiesCommand, printSchema, verifyPartitionProviderIsHive, map, productArity, verboseStringWithSuffix, columnName, equals, newPartition, treeString, schemaString, argString, AlterTableAddPartitionCommand, AlterTableSerDePropertiesCommand, verifyAlterTableType, subqueries, AlterTableDropPartitionCommand, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, oldPartition, totalSize, synchronized, generateTreeString$default$6, AlterTableChangeColumnCommand, allAttributes, nodeName, $isInstanceOf, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, ifNotExists, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, NUM_FILES, logName, notifyAll, conf, mapProductIterator, cmd, collectFirst, newColumn, purge, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, properties, apply, flatMap, resolved, tableName, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, partSpec, clone, cascade, comment, constraints, sameResult, foreach, DropDatabaseCommand, serdeProperties, p, jsonFields, resolve, HIVE_PROVIDER, ifExists, $init$, DDLUtils, copy$default$3, copy, inputSet, toString, AlterDatabasePropertiesCommand, isDatasourceTable, isCanonicalizedPlan, metrics, retainData, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, databaseName, AlterTableUnsetPropertiesCommand, logWarning, output, copy$default$1, transformDown, transformAllExpressions, props, mapExpressions, ne, DropTableCommand, transform, isHiveTable, withNewChildren, resolveQuoted, statePrefix, specs, eq, productIterator, toJSON, PartitionStatistics, AlterTableRenamePartitionCommand, log, AlterTableSetLocationCommand, CreateDatabaseCommand, ##, containsChild, finalize, verifyNotReadPath, productElement, hashCode, DescribeDatabaseCommand, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, partitionSpec, AlterTableRecoverPartitionsCommand, location, path, AlterTableSetPropertiesCommand, map, AlterTableAddPartitionCommand, AlterTableSerDePropertiesCommand, AlterTableDropPartitionCommand, asInstanceOf, AlterTableChangeColumnCommand, expressions, ifNotExists, conf, isInstanceOf, <init>, schema, properties, apply, flatMap, ==, comment, foreach, DropDatabaseCommand, HIVE_PROVIDER, DDLUtils, AlterDatabasePropertiesCommand, !=, collect, AlterTableUnsetPropertiesCommand, logWarning, props, ne, DropTableCommand, AlterTableRenamePartitionCommand, AlterTableSetLocationCommand, CreateDatabaseCommand, DescribeDatabaseCommand)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(checkDataColNames, location, map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, references, <init>, schema, properties, apply, flatMap, resolved, ==, foreach, p, resolve, HIVE_PROVIDER, DDLUtils, toString, isDatasourceTable, !=, output, ne, transform, eq, verifyNotReadPath)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(isView, simpleString, checkDataColNames, partitionSpec, stats, location, path, verifyPartitionProviderIsHive, map, verifyAlterTableType, asInstanceOf, expressions, ifNotExists, conf, isInstanceOf, <init>, schema, properties, apply, flatMap, tableName, ==, comment, foreach, p, resolve, DDLUtils, toString, isDatasourceTable, !=, getClass, databaseName, output, props, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(refresh, AlterTableRecoverPartitionsCommand, path, map, asInstanceOf, conf, <init>, schema, apply, tableName, ==, comment, copy, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(find, simpleString, children, location, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, <init>, schema, apply, resolved, tableName, ==, transformExpressionsUp, foreach, resolve, HIVE_PROVIDER, DDLUtils, toString, !=, collect, output, ne, transform, isHiveTable, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(path, schemaString, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, HIVE_PROVIDER, DDLUtils, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(AlterTableRecoverPartitionsCommand, map, asInstanceOf, run, expressions, conf, isInstanceOf, <init>, schema, properties, tableName, ==, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(find, path, map, columnName, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, HIVE_PROVIDER, DDLUtils, toString, !=, getClass, output, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(refresh, location, path, map, AlterTableAddPartitionCommand, AlterTableDropPartitionCommand, asInstanceOf, run, expressions, conf, isInstanceOf, <init>, apply, flatMap, ==, foreach, p, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(path, map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, tableName, ==, HIVE_PROVIDER, DDLUtils, toString, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(path, map, columnName, schemaString, asInstanceOf, conf, isInstanceOf, isStreaming, <init>, schema, properties, apply, flatMap, tableName, ==, foreach, HIVE_PROVIDER, DDLUtils, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(path, map, asInstanceOf, expressions, isInstanceOf, <init>, apply, resolved, ==, HIVE_PROVIDER, DDLUtils, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ContinuousDataReader, next, getOffset, get, close.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(ContinuousDataReader, next, get)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousDataReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(ContinuousDataReader, next, getOffset, get, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(get)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	DataWriter, write, abort, commit.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(commit)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(commit)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala: Set(DataWriter)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriterFactory.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/DataWriter.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(DataWriter, write, abort, commit)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(DataWriter, write)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFileLinesReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toIterable, withFilter, toTraversable, notify, withPadding, find, span, toBuffer, count, reduceOption, wait, foldRight, takeWhile, $asInstanceOf, minBy, size, zip, toSet, corresponds, :\, duplicate, map, sliding$default$2, dropWhile, toMap, filterNot, equals, toList, isTraversableAgain, asInstanceOf, sameElements, reduceLeftOption, synchronized, sliding, partition, aggregate, $isInstanceOf, forall, mkString, min, scanRight, fold, nonEmpty, notifyAll, /:, toIterator, addString, to, collectFirst, drop, isInstanceOf, filter, GroupedIterator, <init>, toStream, max, buffered, ++, grouped, flatMap, take, reduceRight, ==, maxBy, indexWhere, clone, slice, foreach, exists, reduceRightOption, toVector, toIndexedSeq, copyToBuffer, toArray, reduce, padTo, $init$, toSeq, next, zipWithIndex, toString, copyToArray, length, seq, !=, collect, getClass, hasDefiniteSize, patch, close, foldLeft, contains, isEmpty, ne, withPartial, reversed, hasNext, indexOf, reduceLeft, HadoopFileLinesReader, eq, sum, ##, scanLeft, finalize, hashCode, zipAll, product.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala: Set(size, map, <init>, foreach, length, !=, close, isEmpty, HadoopFileLinesReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(map, mkString, nonEmpty, <init>, flatMap, foreach, toString, close, HadoopFileLinesReader)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(map, asInstanceOf, mkString, nonEmpty, isInstanceOf, filter, <init>, flatMap, take, ==, foreach, zipWithIndex, toString, !=, close, contains, isEmpty, ne, HadoopFileLinesReader)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/LocalTableScanExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, LocalTableScanExec, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, rows, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, LocalTableScanExec, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/DB2Dialect.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, BooleanColumnAccessor, LongColumnAccessor, DoubleColumnAccessor, NullColumnAccessor, equals, asInstanceOf, synchronized, FloatColumnAccessor, $isInstanceOf, ShortColumnAccessor, CompactDecimalColumnAccessor, buffer, ColumnAccessor, NativeColumnAccessor, columnType, StringColumnAccessor, BinaryColumnAccessor, MapColumnAccessor, ByteColumnAccessor, notifyAll, initialize, isInstanceOf, <init>, apply, ==, clone, DecimalColumnAccessor, $init$, ArrayColumnAccessor, BasicColumnAccessor, toString, !=, getClass, extractTo, decompress, extractSingle, ne, StructColumnAccessor, hasNext, IntColumnAccessor, eq, underlyingBuffer, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala: Set(asInstanceOf, ColumnAccessor, initialize, isInstanceOf, <init>, apply, ==, toString, decompress, ne, hasNext, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala: Set(ColumnAccessor, ==, underlyingBuffer)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala: Set(buffer, ColumnAccessor, NativeColumnAccessor, columnType, apply, decompress, hasNext)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala: Set(BooleanColumnAccessor, LongColumnAccessor, DoubleColumnAccessor, NullColumnAccessor, asInstanceOf, FloatColumnAccessor, ShortColumnAccessor, CompactDecimalColumnAccessor, buffer, ColumnAccessor, NativeColumnAccessor, columnType, StringColumnAccessor, BinaryColumnAccessor, MapColumnAccessor, ByteColumnAccessor, initialize, isInstanceOf, <init>, apply, ==, DecimalColumnAccessor, ArrayColumnAccessor, BasicColumnAccessor, decompress, extractSingle, StructColumnAccessor, IntColumnAccessor)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala: Set(BooleanColumnAccessor, LongColumnAccessor, DoubleColumnAccessor, NullColumnAccessor, asInstanceOf, FloatColumnAccessor, ShortColumnAccessor, CompactDecimalColumnAccessor, buffer, ColumnAccessor, NativeColumnAccessor, columnType, StringColumnAccessor, BinaryColumnAccessor, MapColumnAccessor, ByteColumnAccessor, initialize, isInstanceOf, <init>, apply, ==, DecimalColumnAccessor, ArrayColumnAccessor, BasicColumnAccessor, decompress, extractSingle, StructColumnAccessor, IntColumnAccessor)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, EventTimeWatermarkExec, notify, register, register$default$2, treeString$default$2, unapply, find, simpleString, children, name, count, delay, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, countFailedValues, verboseStringWithSuffix, equals, currentStats, treeString, isZero, schemaString, argString, subqueries, executeQuery, zero, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, register$default$3, synchronized, avg, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, min, logTrace, asCode, copyAndReset, canEqual, expressions, toInfo, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, merge, outputOrdering, generateTreeString$default$5, id, foreachUp, mapChildren, delayMs, schema, max, transformExpressionsDown, prettyJson, eventTimeStats, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, eventTime, isAtDriverSide, writeReplace, copy$default$3, copy, executeCollectPublic, inputSet, reset, metadata, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, isRegistered, innerChildren, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, EventTimeStatsAccum, executeBroadcast, ne, requiredChildOrdering, add, transform, withNewChildren, value, statePrefix, EventTimeStats, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(EventTimeWatermarkExec, unapply, children, delay, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, eventTime, copy, toString, output, ne, value, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(EventTimeWatermarkExec, name, count, map, asInstanceOf, synchronized, avg, min, conf, isInstanceOf, collectLeaves, <init>, id, max, eventTimeStats, apply, flatMap, ==, sqlContext, copy, toString, metrics, collect, logWarning, EventTimeStatsAccum, ne, value, EventTimeStats, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(EventTimeWatermarkExec, name, count, execute, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, delayMs, schema, max, eventTimeStats, apply, flatMap, ==, foreach, sparkContext, copy, metadata, toString, !=, collect, logWarning, output, transformAllExpressions, EventTimeStatsAccum, ne, add, transform, value, EventTimeStats, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetricInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetricInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetricInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetricInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetricInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, $asInstanceOf, equals, metricType, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, SQLMetricInfo, clone, toString, !=, getClass, ne, accumulatorId, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala: Set(name, metricType, asInstanceOf, isInstanceOf, <init>, ==, SQLMetricInfo, ne, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala: Set(name, metricType, asInstanceOf, isInstanceOf, <init>, ==, SQLMetricInfo, toString, !=, accumulatorId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/MySQLDialect.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, jars, verboseString, semanticHash, wait, stats, $asInstanceOf, path, numberedTreeString, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, ListFilesCommand, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, files, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, ListJarsCommand, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, AddJarCommand, copy, inputSet, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, AddFileCommand, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, path, map, ListFilesCommand, asInstanceOf, expressions, conf, isInstanceOf, ListJarsCommand, <init>, schema, apply, flatMap, ==, foreach, AddJarCommand, !=, collect, logWarning, AddFileCommand, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourcePartitioning.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, satisfies, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, <init>, ==, clone, $init$, toString, !=, getClass, ne, numPartitions, satisfies0, eq, ##, DataSourcePartitioning, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, DataSourcePartitioning)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, notifyBatchFallingBehind, execute, wait, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, canEqual, nextBatchTime, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, isInstanceOf, <init>, clock, processingTime, TriggerExecutor, ==, clone, $init$, copy, ProcessingTimeExecutor, toString, OneTimeExecutor, logError, !=, getClass, logWarning, copy$default$1, ne, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(execute, asInstanceOf, synchronized, isInstanceOf, <init>, ==, ProcessingTimeExecutor, toString, !=, ne, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(execute, asInstanceOf, isInstanceOf, <init>, TriggerExecutor, ==, copy, ProcessingTimeExecutor, toString, OneTimeExecutor, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQuery.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, $asInstanceOf, explain, equals, asInstanceOf, synchronized, $isInstanceOf, exception, stop, notifyAll, isInstanceOf, StreamingQuery, id, lastProgress, processAllAvailable, ==, clone, status, sparkSession, isActive, recentProgress, toString, awaitTermination, !=, getClass, ne, runId, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, asInstanceOf, synchronized, exception, notifyAll, isInstanceOf, StreamingQuery, id, ==, sparkSession, toString, !=, ne, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(asInstanceOf, isInstanceOf, StreamingQuery, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(name, explain, asInstanceOf, exception, stop, isInstanceOf, StreamingQuery, id, processAllAvailable, ==, status, sparkSession, isActive, toString, !=, ne, runId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala: Set(name, explain, exception, stop, StreamingQuery, id, lastProgress, processAllAvailable, ==, status, sparkSession, isActive, recentProgress, awaitTermination, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(name, asInstanceOf, isInstanceOf, StreamingQuery, ==, sparkSession, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala: Set(asInstanceOf, isInstanceOf, StreamingQuery, id, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, asInstanceOf, synchronized, exception, notifyAll, isInstanceOf, StreamingQuery, id, ==, sparkSession, toString, !=, ne, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(name, asInstanceOf, isInstanceOf, ==, sparkSession, isActive, toString, !=, ne, runId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala: Set(name, explain, exception, stop, StreamingQuery, id, lastProgress, processAllAvailable, ==, status, sparkSession, isActive, recentProgress, awaitTermination, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala: Set(name, synchronized, lastProgress)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(name, asInstanceOf, synchronized, stop, isInstanceOf, ==, sparkSession, isActive, toString, !=, ne, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, asInstanceOf, synchronized, exception, notifyAll, isInstanceOf, StreamingQuery, id, ==, sparkSession, toString, !=, ne, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, asInstanceOf, synchronized, exception, notifyAll, isInstanceOf, StreamingQuery, id, ==, sparkSession, toString, !=, ne, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, asInstanceOf, synchronized, exception, notifyAll, isInstanceOf, StreamingQuery, id, ==, sparkSession, toString, !=, ne, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/EpochCoordinator.scala: Set(asInstanceOf, synchronized, isInstanceOf, id, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(asInstanceOf, isInstanceOf, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(asInstanceOf, isInstanceOf, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(name, asInstanceOf, isInstanceOf, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, json, notifyAll, <init>, Offset, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(json, <init>, Offset, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateStreamOffset.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(json, <init>, Offset, toString, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/Offset.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/MicroBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(json, <init>, Offset, toString)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/streaming/ContinuousReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, wait, stats, outputColumnNames, copy$default$2, $asInstanceOf, numberedTreeString, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, basicWriteJobStatsTracker, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, outputColumns, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, ignoreIfExists, query, CreateDataSourceTableAsSelectCommand, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy$default$3, copy, mode, inputSet, toString, isCanonicalizedPlan, metrics, CreateDataSourceTableCommand, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, table, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(map, asInstanceOf, query, CreateDataSourceTableAsSelectCommand, expressions, conf, collectFirst, isInstanceOf, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, mode, toString, CreateDataSourceTableCommand, !=, output, ne, transform, eq, table)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryWrapper.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, wait, $asInstanceOf, explain, equals, asInstanceOf, synchronized, $isInstanceOf, exception, stop, notifyAll, isInstanceOf, <init>, id, StreamingQueryWrapper, lastProgress, processAllAvailable, ==, clone, status, sparkSession, isActive, recentProgress, toString, awaitTermination, !=, getClass, ne, runId, eq, streamingQuery, ##, finalize, hashCode, explainInternal.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, asInstanceOf, synchronized, exception, notifyAll, isInstanceOf, <init>, id, StreamingQueryWrapper, ==, sparkSession, toString, !=, ne, runId, streamingQuery)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, StatFunctions, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, multipleApproxQuantiles, isInstanceOf, pearsonCorrelation, ==, clone, $init$, calculateCov, toString, logError, !=, getClass, logWarning, ne, eq, crossTabulate, summary, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(StatFunctions, asInstanceOf, isInstanceOf, ==, toString, !=, ne, summary)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala: Set(StatFunctions, asInstanceOf, multipleApproxQuantiles, isInstanceOf, pearsonCorrelation, ==, calculateCov, crossTabulate)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	readSchema, SupportsScanColumnarBatch, createBatchDataReaderFactories, createDataReaderFactories, enableBatchRead.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsScanColumnarBatch.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2ScanExec.scala: Set(readSchema, SupportsScanColumnarBatch, createBatchDataReaderFactories, createDataReaderFactories, enableBatchRead)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getInterval, notify, anyNull, copyValue, getShort, wait, getUTF8String, empty, equals, isNullAt, getDouble, numFields, getString, getStruct, setBoolean, getMap, setByte, notifyAll, getByte, setInt, getArray, <init>, fromSeq, apply, getFloat, getDecimal, getInt, toSeq, MutableColumnarRow, copy, toString, getBoolean, setDecimal, get, setShort, getClass, setNullAt, update, setLong, rowId, getLong, setFloat, hashCode, getBinary, setDouble.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala: Set(<init>, apply, MutableColumnarRow)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(empty, equals, <init>, apply, MutableColumnarRow, copy, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ColumnarBatch.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/AggregateHashMap.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/Statistics.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/Statistics.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/Statistics.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/Statistics.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/Statistics.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	numRows, sizeInBytes, Statistics.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/Statistics.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/reader/SupportsReportStatistics.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala: Set(sizeInBytes, Statistics)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	StateStoreAwareZipPartitionsHelper.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinHelper.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowUtils.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowUtils.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowUtils.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	fromArrowField, notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, ArrowUtils, notifyAll, isInstanceOf, toArrowField, fromArrowType, ==, clone, fromArrowSchema, toString, rootAllocator, !=, getClass, toArrowSchema, ne, eq, toArrowType, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowWriter.scala: Set(fromArrowField, asInstanceOf, ArrowUtils, isInstanceOf, ==, rootAllocator, toArrowSchema, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala: Set(asInstanceOf, ArrowUtils, fromArrowSchema, rootAllocator, !=, toArrowSchema)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowPythonRunner.scala: Set(ArrowUtils, ==, fromArrowSchema, rootAllocator, !=, toArrowSchema)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/ReduceAggregator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, zero, asInstanceOf, synchronized, $isInstanceOf, finish, notifyAll, isInstanceOf, <init>, merge, toColumn, outputEncoder, ==, clone, ReduceAggregator, reduce, toString, !=, getClass, bufferEncoder, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(asInstanceOf, <init>, toColumn, outputEncoder, ==, ReduceAggregator, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, equals, unsafeRow, BufferedRowIterator, notifyAll, shouldStop, currentRows, <init>, partitionIndex, processNext, durationMs, stopEarly, next, incPeakExecutionMemory, toString, getClass, init, hasNext, hashCode, append.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/BufferedRowIterator.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala: Set(BufferedRowIterator, <init>, durationMs, next, toString, init, hasNext, append)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanInfo.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, simpleString, children, wait, $asInstanceOf, equals, asInstanceOf, synchronized, nodeName, $isInstanceOf, notifyAll, isInstanceOf, fromSparkPlan, <init>, SparkPlanInfo, ==, clone, metadata, toString, metrics, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkPlanInfo, ==, metrics, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala: Set(fromSparkPlan, <init>, SparkPlanInfo, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala: Set(simpleString, children, asInstanceOf, nodeName, isInstanceOf, <init>, SparkPlanInfo, ==, toString, metrics, !=, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala: Set(asInstanceOf, isInstanceOf, <init>, SparkPlanInfo, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	genCode, notify, treeString$default$2, ruleName, find, nullable, sql, simpleString, children, updateResult, verboseString, semanticHash, flatArguments, wait, InSubquery, copy$default$2, $asInstanceOf, semanticEquals, numberedTreeString, copy$default$5, map, productArity, verboseStringWithSuffix, equals, PlanSubqueries, treeString, argString, asInstanceOf, initializeLogIfNecessary, ExecSubqueryExpression, generateTreeString, childrenResolved, synchronized, ScalarSubquery, updated$1, generateTreeString$default$6, nodeName, $isInstanceOf, logTrace, asCode, canEqual, canonicalized, copy$default$4, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, prettyName, collectFirst, otherCopyArgs, isInstanceOf, eval, stringArgs, ReuseSubquery, child, withNewPlan, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, prettyJson, apply, flatMap, resolved, ==, fastEquals, result$1, origin, checkInputDataTypes, clone, foreach, p, jsonFields, sparkSession, $init$, conditionString, copy$default$3, copy, toString, logError, !=, deterministic, doGenCode, innerChildren, collect, getClass, logWarning, copy$default$1, dataType, foldable, transformDown, ne, exprId, eval$default$1, transform, withNewChildren, eq, productIterator, toJSON, log, plan, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala: Set(sql, children, updateResult, map, asInstanceOf, ExecSubqueryExpression, synchronized, nodeName, makeCopy, conf, isInstanceOf, child, <init>, apply, flatMap, ==, foreach, p, sparkSession, toString, !=, collect, logWarning, ne, log, plan)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(sql, simpleString, map, PlanSubqueries, treeString, asInstanceOf, conf, isInstanceOf, ReuseSubquery, <init>, apply, ==, sparkSession, toString, !=, dataType, ne, plan)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SortExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, produce, parent, simpleString, children, doProduce, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, doConsume, evaluateVariables, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, SortExec, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, usedInputs, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, inputRDDs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, global, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, testSpillFrequency, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, sortOrder, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, consume, innerChildren, collect, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, createSorter, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, SortExec, asInstanceOf, expressions, conf, isInstanceOf, child, global, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(find, execute, map, SortExec, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, sparkContext, copy, toString, logError, !=, output, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, children, requiredChildDistribution, map, SortExec, asInstanceOf, expressions, transformUp, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceOptions.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceOptions.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceOptions.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceOptions.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceOptions.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, empty, DataSourceOptions, equals, getDouble, notifyAll, <init>, asMap, getInt, toString, getBoolean, get, getClass, getLong, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/DataSourceOptions.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala: Set(DataSourceOptions, <init>, toString, get, getClass)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupportWithSchema.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/RateSourceProvider.scala: Set(DataSourceOptions, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(empty, DataSourceOptions, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/console.scala: Set(DataSourceOptions, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(DataSourceOptions, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(DataSourceOptions, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(empty, DataSourceOptions, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(empty, DataSourceOptions, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/StreamWriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/memoryV2.scala: Set(DataSourceOptions, <init>, toString, get)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(empty, DataSourceOptions, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala: Set(DataSourceOptions, <init>, getInt, getBoolean, get)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/ContinuousReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/WriteSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	readBooleans, notify, readLong, readFloats, wait, VectorizedPlainValuesReader, readBoolean, equals, readValueDictionaryId, readLongs, readFloat, readDoubles, skip, readInteger, notifyAll, initFromPage, readByte, <init>, readBytes, readDouble, toString, readBinary, getClass, getNextOffset, readIntegers, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, copy$default$5, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, buildSide, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, left, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, condition, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, joinType, getClass, BroadcastNestedLoopJoinExec, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, right, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, buildSide, asInstanceOf, left, expressions, conf, isInstanceOf, <init>, outputOrdering, condition, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, joinType, BroadcastNestedLoopJoinExec, output, ne, eq, right)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, unapply, find, coordinator, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, preparePostShuffleRDD$default$2, copy$default$2, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, preparePostShuffleRDD, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, newPartitioning, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, prepareShuffleDependency, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, ShuffleExchangeExec, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(unapply, children, map, asInstanceOf, expressions, ShuffleExchangeExec, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala: Set(map, preparePostShuffleRDD, asInstanceOf, synchronized, prepareShuffleDependency, ShuffleExchangeExec, isInstanceOf, <init>, apply, ==, sqlContext, sparkContext, !=, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, map, asInstanceOf, expressions, ShuffleExchangeExec, conf, isInstanceOf, child, <init>, apply, ==, sparkContext, output, transformAllExpressions, transform, withNewChildren, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala: Set(execute, map, asInstanceOf, prepareShuffleDependency, expressions, ShuffleExchangeExec, isInstanceOf, child, <init>, outputOrdering, apply, ==, outputPartitioning, copy, toString, executeTake, !=, output, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(find, coordinator, children, requiredChildDistribution, map, newPartitioning, asInstanceOf, expressions, transformUp, ShuffleExchangeExec, conf, isInstanceOf, child, <init>, outputOrdering, apply, flatMap, ==, foreach, outputPartitioning, toString, ne, requiredChildOrdering, withNewChildren, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ruleName, wait, $asInstanceOf, EnsureRequirements, productArity, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, conf, isInstanceOf, <init>, apply, ==, clone, $init$, copy, toString, logError, !=, getClass, logWarning, copy$default$1, ne, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(EnsureRequirements, asInstanceOf, conf, isInstanceOf, <init>, apply, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	ExecutionPage, notify, wait, $asInstanceOf, equals, prefix, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, renderJson, toString, logError, !=, getClass, logWarning, render, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala: Set(ExecutionPage, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, putShort, unapply, SHORT, copyMemory, getShort, BINARY, wait, precision, copy$default$2, $asInstanceOf, ByteArrayColumnType, setField, actualSize, productArity, equals, putLong, scale, asInstanceOf, getDouble, COMPACT_DECIMAL, synchronized, LARGE_DECIMAL, copyField, $isInstanceOf, putInt, canEqual, scalaTag, deserialize, productPrefix, notifyAll, isInstanceOf, FLOAT, getField, <init>, NULL, STRING, STRUCT, apply, getFloat, ==, clone, DOUBLE, INT, getInt, $init$, copy, defaultSize, BYTE, toString, !=, ARRAY, ColumnType, NativeColumnType, getClass, ByteBufferHelper, copy$default$1, dataType, BOOLEAN, ne, serialize, LONG, eq, productIterator, ##, MAP, finalize, extract, DirectCopyColumnType, getLong, productElement, hashCode, append.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder.scala: Set(putInt, <init>, apply, ==, ColumnType, NativeColumnType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnStats.scala: Set(SHORT, getShort, BINARY, precision, actualSize, scale, getDouble, FLOAT, <init>, STRING, apply, getFloat, ==, clone, DOUBLE, INT, getInt, defaultSize, BYTE, ColumnType, dataType, BOOLEAN, ne, LONG, getLong)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnAccessor.scala: Set(SHORT, BINARY, precision, asInstanceOf, COMPACT_DECIMAL, LARGE_DECIMAL, isInstanceOf, FLOAT, <init>, NULL, STRING, STRUCT, apply, ==, DOUBLE, INT, BYTE, ARRAY, ColumnType, NativeColumnType, dataType, BOOLEAN, LONG, MAP, extract)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/NullableColumnAccessor.scala: Set(==, getInt, ByteBufferHelper)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/compressionSchemes.scala: Set(putShort, SHORT, getShort, setField, actualSize, putLong, asInstanceOf, copyField, putInt, isInstanceOf, getField, <init>, STRING, apply, ==, clone, INT, getInt, defaultSize, BYTE, toString, !=, ColumnType, NativeColumnType, ByteBufferHelper, dataType, BOOLEAN, LONG, extract, getLong, append)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressibleColumnAccessor.scala: Set(apply, getInt, NativeColumnType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/compression/CompressionScheme.scala: Set(<init>, apply, getInt, ColumnType, NativeColumnType)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnBuilder.scala: Set(SHORT, BINARY, precision, actualSize, asInstanceOf, COMPACT_DECIMAL, LARGE_DECIMAL, isInstanceOf, FLOAT, <init>, NULL, STRING, STRUCT, apply, ==, DOUBLE, INT, defaultSize, BYTE, ARRAY, ColumnType, NativeColumnType, dataType, BOOLEAN, LONG, MAP, append)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	getInterval, notify, getShort, wait, getUTF8String, getBooleans, equals, hasNull, isNullAt, getDouble, getStruct, getFloats, OrcColumnVector, getMap, notifyAll, getByte, getArray, <init>, getInts, setBatchSize, getDoubles, getFloat, getDecimal, getInt, getBytes, getShorts, getLongs, toString, getBoolean, numNulls, getClass, dataType, close, getChild, getLong, hashCode, getBinary.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnVector.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, format, wait, $asInstanceOf, equals, json, jdbc, asInstanceOf, initializeLogIfNecessary, orc, parquet, csv, textFile, synchronized, option, $isInstanceOf, load, text, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, options, <init>, schema, ==, clone, $init$, toString, DataFrameReader, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, table, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(format, json, jdbc, parquet, option, load, options, <init>, schema, toString, DataFrameReader, ne, table)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(asInstanceOf, synchronized, isInstanceOf, options, <init>, schema, ==, clone, toString, DataFrameReader, logWarning, ne, table)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, name, onQueryTerminated, wait, $asInstanceOf, equals, Event, onQueryProgress, asInstanceOf, synchronized, $isInstanceOf, exception, logEvent, QueryStartedEvent, notifyAll, isInstanceOf, <init>, id, progress, ==, StreamingQueryListener, clone, $init$, QueryTerminatedEvent, toString, !=, QueryProgressEvent, getClass, onQueryStarted, ne, runId, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(name, Event, asInstanceOf, exception, QueryStartedEvent, isInstanceOf, <init>, id, ==, StreamingQueryListener, QueryTerminatedEvent, toString, !=, ne, runId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, Event, asInstanceOf, synchronized, exception, notifyAll, isInstanceOf, <init>, id, ==, StreamingQueryListener, toString, !=, ne, runId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(name, Event, asInstanceOf, synchronized, isInstanceOf, <init>, id, progress, ==, StreamingQueryListener, toString, QueryProgressEvent, ne, runId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingQueryListenerBus.scala: Set(onQueryTerminated, Event, onQueryProgress, asInstanceOf, synchronized, QueryStartedEvent, isInstanceOf, <init>, progress, StreamingQueryListener, QueryTerminatedEvent, QueryProgressEvent, onQueryStarted, runId)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF5.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF5.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF5.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF5.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF5.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF5.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(call, UDF5)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF5)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF5.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ruleName, wait, $asInstanceOf, HiveOnlyCheck, compose, productArity, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, andThen, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, conf, isInstanceOf, <init>, PreWriteCheck, castAndRenameQueryOutput, apply, PreReadCheck, ==, clone, sparkSession, $init$, copy, toString, PreprocessTableCreation, ResolveSQLOnFile, logError, !=, getClass, logWarning, copy$default$1, ne, failAnalysis, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, DDLPreprocessingUtils, PreprocessTableInsertion, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(HiveOnlyCheck, conf, <init>, PreWriteCheck, apply, PreReadCheck, clone, PreprocessTableCreation, ResolveSQLOnFile, ne, PreprocessTableInsertion)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	PruneFileSourcePartitions, notify, ruleName, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, apply, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala: Set(PruneFileSourcePartitions)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, produce, enableAccumulatorsForTest, parent, simpleString, children, doProduce, predicates, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, doConsume, relation, evaluateVariables, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, partitionFilters, usedInputs, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, attributes, conf, mapProductIterator, needsUnsafeRowConversion, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, inputRDDs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, supportsBatch, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, InMemoryTableScanExec, $init$, copy$default$3, readBatches, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, consume, innerChildren, collect, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, vectorTypes, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, readPartitions, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, buildFilter, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, InMemoryTableScanExec, copy, toString, output, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, unapplySeq, ruleName, curried, Once, name, batches, execute, wait, $asInstanceOf, preOptimizationBatches, compose, productArity, equals, extendedOperatorOptimizationRules, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, andThen, tupled, maxIterations, OptimizeSubqueries, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, Strategy, FixedPoint, fixedPoint, isInstanceOf, <init>, isPlanIntegral, apply, ==, clone, SparkOptimizer, strategy, postHocOptimizationBatches, $init$, copy, toString, logError, !=, rules, getClass, logWarning, Batch, copy$default$1, ne, eq, productIterator, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala: Set(<init>, apply, ==, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(unapply, name, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, clone, copy, toString, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeTableCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(name, <init>, apply, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/views.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, apply, ==, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala: Set(<init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(name, wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, apply, ==, toString, !=, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(unapply, name, asInstanceOf, Strategy, isInstanceOf, <init>, apply, ==, toString, !=, rules, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreRDD.scala: Set(<init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala: Set(name, asInstanceOf, <init>, apply, ==, toString, logWarning, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, unapplySeq, name, execute, asInstanceOf, andThen, isInstanceOf, <init>, apply, ==, copy, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(extendedOperatorOptimizationRules, Strategy, <init>, apply, clone, SparkOptimizer, rules, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/IncrementalExecution.scala: Set(unapply, execute, asInstanceOf, Strategy, isInstanceOf, <init>, apply, ==, rules, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLContext.scala: Set(<init>, apply, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(unapply, name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, ne, eq, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, logError, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(unapply, unapplySeq, name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(name, asInstanceOf, <init>, apply, ==, toString, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ExistingRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq, productElement)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSinkLog.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(unapply, name, asInstanceOf, <init>, apply, ==, copy, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingSymmetricHashJoinExec.scala: Set(name, execute, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(unapply, name, asInstanceOf, isInstanceOf, <init>, apply, ==, copy, toString, logError, !=, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, rules, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/HDFSMetadataLog.scala: Set(logTrace, <init>, apply, ==, toString, !=, logWarning, ne, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(name, execute, asInstanceOf, isInstanceOf, <init>, apply, ==, copy, toString, logError, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(name, execute, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, getClass, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/databases.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, logError, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala: Set(name, asInstanceOf, logTrace, isInstanceOf, <init>, apply, ==, toString, ne, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala: Set(isInstanceOf, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/CatalogFileIndex.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala: Set(execute, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ProgressReporter.scala: Set(name, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, copy, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(name, isInstanceOf, <init>, apply, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(unapply, name, asInstanceOf, synchronized, isInstanceOf, <init>, apply, ==, clone, copy, toString, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(name, isInstanceOf, <init>, apply, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSourceLog.scala: Set(batches, <init>, apply, ==, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala: Set(unapply, <init>, apply, toString, !=, logWarning, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(unapply, name, asInstanceOf, isInstanceOf, <init>, apply, ne, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/SetCommand.scala: Set(unapplySeq, name, equals, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, logWarning, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(unapply, name, asInstanceOf, Strategy, isInstanceOf, <init>, apply, ==, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala: Set(unapply, unapplySeq, name, execute, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, rules, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/statefulOperators.scala: Set(name, execute, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, logWarning, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamingRelation.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, getClass, logWarning, ne, eq, hashCode, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala: Set(asInstanceOf, logTrace, isInstanceOf, <init>, apply, ==, toString, !=, getClass, logWarning, ne, eq, hashCode, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, !=, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/package.scala: Set(<init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, logWarning, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/resources.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog.scala: Set(name, <init>, apply, ==, toString, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(unapply, name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, logWarning, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala: Set(unapply, name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, getClass, logWarning)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(unapply, name, asInstanceOf, isInstanceOf, <init>, apply, rules)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, logWarning, ne, eq, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, logWarning, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/functions.scala: Set(name, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, !=, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala: Set(extendedOperatorOptimizationRules, Strategy, <init>, apply, clone, SparkOptimizer, rules, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashMapGenerator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, curried, generateRowIterator, name, wait, copy$default$2, $asInstanceOf, productArity, equals, generate, asInstanceOf, Buffer, HashMapGenerator, bufferValues, synchronized, $isInstanceOf, genComputeHash, initializeAggregateHashMap, tupled, groupingKeySignature, canEqual, generateEquals, productPrefix, notifyAll, isInstanceOf, generateClose, <init>, apply, groupingKeys, buffVars, generateFindOrInsert, ==, clone, $init$, copy, toString, !=, getClass, copy$default$1, dataType, generateHashFunction, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(equals, generate, asInstanceOf, isInstanceOf, <init>, apply, ==, copy, toString, !=, dataType, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(equals, generate, asInstanceOf, isInstanceOf, <init>, apply, ==, copy, toString, !=, dataType, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator.scala: Set(name, asInstanceOf, Buffer, HashMapGenerator, groupingKeySignature, isInstanceOf, <init>, groupingKeys, toString, dataType, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator.scala: Set(name, asInstanceOf, Buffer, HashMapGenerator, bufferValues, groupingKeySignature, isInstanceOf, <init>, apply, groupingKeys, buffVars, dataType, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala: Set(equals, generate, asInstanceOf, isInstanceOf, <init>, apply, ==, copy, toString, !=, dataType, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, copy$default$2, $asInstanceOf, copy$default$5, productArity, equals, physicalPlanDescription, description, asInstanceOf, synchronized, $isInstanceOf, logEvent, canEqual, copy$default$4, productPrefix, notifyAll, isInstanceOf, <init>, ==, clone, $init$, accumUpdates, copy$default$3, details, copy, toString, !=, time, getClass, sparkPlanInfo, copy$default$1, copy$default$6, ne, executionId, SparkListenerDriverAccumUpdates, SparkListenerSQLExecutionStart, eq, productIterator, SparkListenerSQLExecutionEnd, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala: Set(physicalPlanDescription, description, asInstanceOf, isInstanceOf, <init>, ==, accumUpdates, details, !=, time, getClass, sparkPlanInfo, ne, executionId, SparkListenerDriverAccumUpdates, SparkListenerSQLExecutionStart, SparkListenerSQLExecutionEnd)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/SQLMetrics.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, !=, getClass, executionId, SparkListenerDriverAccumUpdates)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SQLExecution.scala: Set(<init>, ==, toString, executionId, SparkListenerSQLExecutionStart, SparkListenerSQLExecutionEnd)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetLogRedirector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetLogRedirector.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetLogRedirector.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetLogRedirector.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetLogRedirector.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, ParquetLogRedirector, wait, equals, INSTANCE, notifyAll, toString, getClass, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(ParquetLogRedirector, INSTANCE, toString, getClass, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetLogRedirector.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, Once, wait, ProcessingTime, equals, notifyAll, <init>, toString, getClass, Trigger, hashCode, Continuous.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(ProcessingTime, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(ProcessingTime, <init>, toString, getClass, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, ProcessingTime, notifyAll, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(ProcessingTime, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, ProcessingTime, notifyAll, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(ProcessingTime, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala: Set(ProcessingTime, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala: Set(ProcessingTime, <init>, toString)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(ProcessingTime, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, ProcessingTime, notifyAll, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(<init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(ProcessingTime, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala: Set(ProcessingTime, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(ProcessingTime, <init>, toString, getClass, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Triggers.scala: Set(<init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(ProcessingTime, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/ProcessingTime.scala: Set(ProcessingTime, <init>, toString, Trigger)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, unapply, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, copy$default$5, resetMetrics, printSchema, map, ObjectHashAggregateExec, productArity, resultExpressions, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, requiredChildDistributionExpressions, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, supportsAggregate, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, groupingExpressions, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, initialInputBufferOffset, p, jsonFields, copy$default$7, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, aggregateExpressions, innerChildren, aggregateAttributes, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, copy$default$6, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/AggUtils.scala: Set(children, map, ObjectHashAggregateExec, resultExpressions, requiredChildDistributionExpressions, asInstanceOf, expressions, conf, isInstanceOf, child, supportsAggregate, <init>, groupingExpressions, apply, flatMap, ==, sqlContext, initialInputBufferOffset, toString, aggregateExpressions, aggregateAttributes, transformDown, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, curried, wait, stats, copy$default$2, $asInstanceOf, productArity, equals, asInstanceOf, initializeLogIfNecessary, outputColumns, synchronized, $isInstanceOf, tupled, logTrace, canEqual, ExecutedWriteSummary, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, isInstanceOf, <init>, apply, ==, outputPath, customPartitionLocations, clone, FileFormatWriter, $init$, updatedPartitions, OutputSpec, copy$default$3, copy, toString, logError, !=, getClass, logWarning, copy$default$1, ne, eq, productIterator, write, log, ##, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/DataWritingCommand.scala: Set(<init>, ==, FileFormatWriter, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala: Set(asInstanceOf, outputColumns, isInstanceOf, <init>, apply, ==, outputPath, customPartitionLocations, FileFormatWriter, updatedPartitions, OutputSpec, toString, !=, ne, eq, write, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSink.scala: Set(unapply, asInstanceOf, isInstanceOf, <init>, apply, ==, FileFormatWriter, OutputSpec, toString, !=, logWarning, write, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	DebugQuery.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/debug/package.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/OutputWriter.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, getFileExtension, OutputWriter, notifyAll, isInstanceOf, <init>, ==, clone, newInstance, toString, !=, getClass, OutputWriterFactory, close, ne, eq, write, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetOutputWriter.scala: Set(OutputWriter, <init>, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcOutputWriter.scala: Set(OutputWriter, <init>, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala: Set(asInstanceOf, getFileExtension, OutputWriter, isInstanceOf, <init>, ==, newInstance, toString, !=, OutputWriterFactory, close, ne, eq, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala: Set(isInstanceOf, <init>, ==, OutputWriterFactory)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonFileFormat.scala: Set(OutputWriter, isInstanceOf, <init>, ==, !=, getClass, OutputWriterFactory, close, write, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala: Set(OutputWriter, isInstanceOf, <init>, ==, !=, getClass, OutputWriterFactory, close, write, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala: Set(OutputWriter, <init>, !=, OutputWriterFactory, close, write)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, OutputWriter, isInstanceOf, <init>, ==, toString, getClass, OutputWriterFactory, close, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(asInstanceOf, OutputWriter, isInstanceOf, <init>, ==, !=, getClass, OutputWriterFactory, close, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala: Set(asInstanceOf, OutputWriter, isInstanceOf, <init>, ==, toString, getClass, OutputWriterFactory, close, ne, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFileFormat.scala: Set(asInstanceOf, OutputWriter, isInstanceOf, <init>, ==, !=, getClass, OutputWriterFactory, close, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, newInstance, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, newInstance, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, newInstance, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, newInstance, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/json/JsonDataSource.scala: Set(<init>, toString, close)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, close, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, newInstance, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, newInstance, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, newInstance, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(asInstanceOf, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/FlatMapGroupsInPandasExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, verboseString, semanticHash, execute, executeCollectIterator, wait, groupingAttributes, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, func, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, FlatMapGroupsInPandasExec, executeBroadcast, ne, requiredChildOrdering, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala: Set(find, children, map, asInstanceOf, transformExpressions, expressions, outputSet, transformUp, isInstanceOf, child, references, <init>, apply, flatMap, ==, foreach, inputSet, !=, output, transformDown, FlatMapGroupsInPandasExec, ne, withNewChildren)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, groupingAttributes, map, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, func, copy, toString, output, FlatMapGroupsInPandasExec, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTrigger.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, wait, intervalMs, $asInstanceOf, productArity, equals, asInstanceOf, synchronized, $isInstanceOf, create, canEqual, productPrefix, notifyAll, isInstanceOf, <init>, apply, ==, clone, ContinuousTrigger, $init$, copy, toString, !=, getClass, copy$default$1, ne, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(intervalMs, asInstanceOf, synchronized, create, isInstanceOf, <init>, apply, ==, ContinuousTrigger, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, apply, ==, ContinuousTrigger, toString, !=, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala: Set(wait, asInstanceOf, synchronized, notifyAll, isInstanceOf, <init>, apply, ==, ContinuousTrigger, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/streaming/Trigger.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	catalogTable, notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, staticPartitions, verboseString, semanticHash, wait, stats, outputColumnNames, copy$default$2, $asInstanceOf, ifPartitionNotExists, numberedTreeString, copy$default$5, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, copy$default$9, basicWriteJobStatsTracker, copy$default$12, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, outputColumns, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, fileIndex, query, copy$default$8, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, InsertIntoHadoopFsRelationCommand, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, options, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, partitionColumns, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, bucketSpec, fastEquals, outputPath, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, copy$default$7, resolve, copy$default$10, $init$, copy$default$3, copy, mode, inputSet, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, copy$default$6, ne, transform, withNewChildren, copy$default$11, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, fileFormat, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(catalogTable, find, stats, outputColumnNames, map, asInstanceOf, run, outputColumns, fileIndex, expressions, InsertIntoHadoopFsRelationCommand, conf, isInstanceOf, options, <init>, partitionColumns, schema, apply, flatMap, resolved, ==, bucketSpec, outputPath, p, mode, toString, !=, collect, getClass, logWarning, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(catalogTable, staticPartitions, ifPartitionNotExists, map, asInstanceOf, query, expressions, InsertIntoHadoopFsRelationCommand, conf, collectFirst, isInstanceOf, options, references, <init>, schema, apply, flatMap, resolved, ==, bucketSpec, outputPath, foreach, p, resolve, mode, toString, !=, output, ne, transform, eq, fileFormat)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	catalogTable, notify, treeString$default$2, unapply, find, simpleString, children, refresh, maxRowsPerPartition, LogicalRelation, verboseString, semanticHash, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, relation, attributeMap, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, computeStats, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, apply$default$2, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, newInstance, copy$default$3, copy, inputSet, toString, isCanonicalizedPlan, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala: Set(LogicalRelation, map, asInstanceOf, synchronized, logTrace, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, p, toString, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(catalogTable, unapply, LogicalRelation, relation, attributeMap, map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, find, simpleString, children, LogicalRelation, relation, map, treeString, asInstanceOf, expressions, outputSet, transformUp, conf, isInstanceOf, isStreaming, <init>, mapChildren, schema, apply, flatMap, ==, foreach, resolve, copy, toString, !=, collect, output, ne, transform, resolveQuoted)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery.scala: Set(unapply, LogicalRelation, relation, map, asInstanceOf, expressions, outputSet, isInstanceOf, isStreaming, references, <init>, apply, flatMap, ==, p, toString, collect, output, ne, transform, withNewChildren, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(catalogTable, find, simpleString, children, LogicalRelation, relation, map, asInstanceOf, expressions, transformUp, conf, isInstanceOf, <init>, schema, apply, resolved, ==, transformExpressionsUp, foreach, resolve, toString, !=, collect, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala: Set(unapply, LogicalRelation, map, schemaString, asInstanceOf, synchronized, expressions, conf, isInstanceOf, isStreaming, <init>, schema, apply, ==, clone, foreach, newInstance, copy, toString, logWarning, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(LogicalRelation, relation, map, asInstanceOf, conf, isInstanceOf, <init>, schema, apply, ==, newInstance, toString, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala: Set(catalogTable, unapply, find, simpleString, LogicalRelation, relation, map, asInstanceOf, expressions, isInstanceOf, references, <init>, apply, ==, resolve, output, transform, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala: Set(catalogTable, find, LogicalRelation, stats, relation, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, schema, apply, flatMap, resolved, ==, p, newInstance, toString, !=, collect, getClass, logWarning, output, ne, eq, logDebug)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala: Set(find, refresh, LogicalRelation, stats, relation, map, asInstanceOf, expressions, conf, isInstanceOf, <init>, apply, ==, sameResult, foreach, toString, logWarning, output, transformDown, transformAllExpressions, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala: Set(unapply, LogicalRelation, stats, relation, map, asInstanceOf, expressions, conf, collectFirst, isInstanceOf, <init>, schema, apply, flatMap, ==, foreach, toString, !=, collect, getClass, logWarning, output, ne, eq, log, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoDataSourceCommand.scala: Set(LogicalRelation, relation, asInstanceOf, isInstanceOf, <init>, apply, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions.scala: Set(catalogTable, unapply, find, LogicalRelation, relation, map, asInstanceOf, expressions, isInstanceOf, references, <init>, apply, resolve, output, transformDown, transform)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala: Set(LogicalRelation, map, asInstanceOf, expressions, isInstanceOf, <init>, apply, resolved, ==, toString, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/CommitLog.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, metadataPath, parseVersion, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, isTraceEnabled, deserialize, initializeLogIfNecessary$default$2, logName, notifyAll, purge, isInstanceOf, getLatest, <init>, CommitLog, isBatchFile, ==, clone, purgeAfter, batchIdToPath, $init$, fileManager, toString, logError, !=, get, getClass, logWarning, batchFilesFilter, ne, serialize, pathToBatchId, add, getOrderedBatchFiles, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(metadataPath, asInstanceOf, synchronized, purge, isInstanceOf, getLatest, <init>, CommitLog, ==, toString, !=, get, ne, add, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala: Set(metadataPath, asInstanceOf, isInstanceOf, <init>, CommitLog, ==, toString, logError, !=, get, logWarning, ne, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(metadataPath, asInstanceOf, purge, isInstanceOf, getLatest, <init>, CommitLog, ==, toString, !=, get, logWarning, ne, add, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF18.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF18.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF18.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF18.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF18.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF18.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF18.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF18)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/package.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, package, wait, $asInstanceOf, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala source file has the following implicit definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	format.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following member ref dependencies of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamMetadata.scala are invalidated:[0m
[0m[[0mdebug[0m] [0m[naha] 	/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusStore.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	toSparkPlanGraph, execution, notify, desc, stages, name, executionsCount, submissionTime, toSparkPlanGraphCluster, SparkPlanGraphClusterWrapper, wait, copy$default$2, $asInstanceOf, listener, productArity, equals, physicalPlanDescription, description, metricType, asInstanceOf, synchronized, $isInstanceOf, toSparkPlanGraphNode, canEqual, SQLPlanMetric, productPrefix, executionsList, notifyAll, isInstanceOf, SparkPlanGraphNodeWrapper, cluster, completionTime, <init>, metricValues, id, SQLAppStatusStore, nodes, ==, clone, planGraphCount, executionMetrics, planGraph, $init$, jobs, copy$default$3, details, copy, toString, SQLExecutionUIData, metrics, node, !=, edges, getClass, copy$default$1, ne, executionId, SparkPlanGraphWrapper, accumulatorId, eq, productIterator, ##, finalize, productElement, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SparkPlanGraph.scala: Set(execution, desc, name, metricType, asInstanceOf, SQLPlanMetric, isInstanceOf, cluster, <init>, id, nodes, ==, toString, metrics, node, !=, edges, accumulatorId, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala: Set(execution, listener, asInstanceOf, <init>, SQLAppStatusStore, ==, !=)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLAppStatusListener.scala: Set(execution, desc, stages, name, submissionTime, SparkPlanGraphClusterWrapper, physicalPlanDescription, description, metricType, asInstanceOf, SQLPlanMetric, isInstanceOf, SparkPlanGraphNodeWrapper, completionTime, <init>, id, nodes, ==, planGraph, jobs, details, SQLExecutionUIData, metrics, node, !=, edges, getClass, ne, executionId, SparkPlanGraphWrapper, accumulatorId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLHistoryServerPlugin.scala: Set(execution, executionsCount, <init>, SQLAppStatusStore)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/AllExecutionsPage.scala: Set(execution, desc, submissionTime, description, executionsList, completionTime, <init>, SQLAppStatusStore, ==, jobs, details, toString, SQLExecutionUIData, !=, ne, executionId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/ExecutionPage.scala: Set(execution, desc, submissionTime, physicalPlanDescription, completionTime, <init>, id, SQLAppStatusStore, ==, executionMetrics, planGraph, jobs, toString, SQLExecutionUIData, metrics, node, !=, ne, executionId)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLTab.scala: Set(execution, <init>, SQLAppStatusStore)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/PackedRowWriterFactory.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, commit, createDataWriter, PackedRowDataWriter, productArity, equals, asInstanceOf, initializeLogIfNecessary, synchronized, $isInstanceOf, logTrace, canEqual, isTraceEnabled, initializeLogIfNecessary$default$2, productPrefix, logName, notifyAll, isInstanceOf, <init>, ==, clone, $init$, copy, toString, logError, !=, getClass, logWarning, copy$default$1, ne, rows, PackedRowCommitMessage, eq, productIterator, write, log, PackedRowWriterFactory, ##, finalize, productElement, hashCode, abort, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/ConsoleWriter.scala: Set(asInstanceOf, isInstanceOf, <init>, rows, PackedRowCommitMessage, PackedRowWriterFactory)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF3.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF3.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF3.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF3.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF3.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF3.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF3.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(call, UDF3)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF3)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Column.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, unapply, leq, desc, name, rlike, wait, *, <=, Column, %, $asInstanceOf, minus, withInputType, timestamp, mod, explain, desc_nulls_last, asc_nulls_last, binary, map, plus, equals, float, isNull, startsWith, isin, short, asInstanceOf, <, initializeLogIfNecessary, unary_-, synchronized, unary_!, TypedColumn, $isInstanceOf, ||, >=, as, expr, logTrace, divide, string, or, isTraceEnabled, initializeLogIfNecessary$default$2, !==, ColumnName, logName, asc, notifyAll, named, decimal, bitwiseOR, -, isInstanceOf, <=>, double, array, alias, =!=, long, endsWith, getField, <init>, boolean, over, cast, encoder, apply, date, ==, getItem, desc_nulls_first, clone, int, substr, eqNullSafe, $init$, multiply, struct, isNaN, ===, toString, equalTo, +, logError, bitwiseAND, !=, &&, getClass, logWarning, isNotNull, notEqual, between, geq, contains, ne, lt, byte, asc_nulls_first, gt, when, generateAlias, eq, like, log, and, /, >, ##, finalize, otherwise, hashCode, logDebug, logInfo, bitwiseXOR.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(name, Column, map, startsWith, asInstanceOf, unary_!, expr, isInstanceOf, <init>, apply, ==, toString, +, &&, ne, log, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala: Set(unapply, name, *, Column, withInputType, explain, map, asInstanceOf, <, unary_!, TypedColumn, ||, >=, as, expr, named, -, isInstanceOf, alias, <init>, cast, encoder, apply, ==, toString, +, !=, &&, contains, ne, /, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala: Set(name, asInstanceOf, ColumnName, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala: Set(name, <=, Column, map, <, >=, -, <init>, apply, +, &&, contains, ne, /, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/WindowSpec.scala: Set(<=, Column, map, asInstanceOf, expr, isInstanceOf, <init>, apply, ==, &&, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala: Set(name, Column, withInputType, map, asInstanceOf, unary_!, TypedColumn, ||, expr, -, isInstanceOf, alias, <init>, apply, ==, toString, +, &&, getClass, ne, generateAlias, eq, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/KeyValueGroupedDataset.scala: Set(name, withInputType, map, asInstanceOf, TypedColumn, as, named, -, <init>, encoder, apply, ==, toString, +, !=, &&, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/StatFunctions.scala: Set(name, *, <=, Column, map, asInstanceOf, <, unary_!, TypedColumn, ||, >=, as, expr, -, isInstanceOf, endsWith, <init>, apply, ==, isNaN, toString, +, &&, getClass, logWarning, ne, /, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzePartitionCommand.scala: Set(Column, map, asInstanceOf, ||, isInstanceOf, <init>, apply, ==, toString, +, &&, contains, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Window.scala: Set(Column, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/udaf.scala: Set(Column, map, expr, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala: Set(TypedColumn, <init>)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(unapply, desc, name, Column, timestamp, desc_nulls_last, asc_nulls_last, map, asInstanceOf, unary_-, unary_!, TypedColumn, as, expr, ColumnName, asc, isInstanceOf, array, <init>, apply, date, desc_nulls_first, substr, struct, contains, ne, asc_nulls_first, log)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/typedaggregators.scala: Set(asInstanceOf, TypedColumn, <init>, apply, ==, +, /)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala: Set(<=, Column, map, asInstanceOf, <, ||, >=, isInstanceOf, <init>, apply, ==, +, &&)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/Aggregator.scala: Set(TypedColumn, expr, <init>, apply)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/expressions/javalang/typed.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/python/UserDefinedPythonFunction.scala: Set(name, Column, map, asInstanceOf, ||, expr, isInstanceOf, <init>, apply, ==, toString, &&, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala: Set(name, Column, map, asInstanceOf, unary_!, ||, expr, isInstanceOf, <init>, apply, ==, toString, &&, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVUtils.scala: Set(Column, startsWith, asInstanceOf, unary_!, ||, ColumnName, isInstanceOf, <init>, ==, toString, &&, >)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala: Set(name, Column, map, asInstanceOf, ||, as, expr, isInstanceOf, <init>, cast, apply, ==, +, &&, getClass, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Relation.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, unapply, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, printSchema, map, productArity, reader, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, StreamingDataSourceV2Relation, asInstanceOf, transformExpressions, initializeLogIfNecessary, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, computeStats, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, DataSourceV2Relation, $init$, newInstance, copy, inputSet, toString, isCanonicalizedPlan, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownOperatorsToDataSource.scala: Set(unapply, map, reader, asInstanceOf, expressions, outputSet, transformUp, isInstanceOf, references, <init>, mapChildren, apply, flatMap, ==, p, DataSourceV2Relation, output, ne, transform)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala: Set(reader, asInstanceOf, isInstanceOf, <init>, DataSourceV2Relation, output)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(map, reader, StreamingDataSourceV2Relation, asInstanceOf, expressions, conf, isInstanceOf, isStreaming, <init>, schema, apply, flatMap, ==, foreach, copy, toString, !=, collect, logWarning, output, transformAllExpressions, ne, transform, eq, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousExecution.scala: Set(map, reader, StreamingDataSourceV2Relation, asInstanceOf, synchronized, expressions, isInstanceOf, <init>, schema, apply, ==, DataSourceV2Relation, toString, !=, collect, output, transformAllExpressions, ne, transform, logDebug, logInfo)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala: Set(map, reader, schemaString, asInstanceOf, conf, isInstanceOf, isStreaming, <init>, schema, apply, flatMap, ==, foreach, DataSourceV2Relation, newInstance, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVInferSchema.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, infer, equals, CSVInferSchema, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, ==, clone, toString, !=, inferField, getClass, mergeRowTypes, ne, eq, findTightestCommonType, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVDataSource.scala: Set(infer, CSVInferSchema, asInstanceOf, isInstanceOf, ==, toString, !=, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/stat/FrequentItems.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, FrequentItems, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, singlePassFreqItems, synchronized, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, notifyAll, isInstanceOf, ==, clone, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameStatFunctions.scala: Set(FrequentItems, asInstanceOf, singlePassFreqItems, isInstanceOf, ==)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ExchangeCoordinator.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, estimatePartitionStartIndices, registerExchange, wait, $asInstanceOf, equals, asInstanceOf, initializeLogIfNecessary, synchronized, ExchangeCoordinator, $isInstanceOf, logTrace, isTraceEnabled, initializeLogIfNecessary$default$2, logName, isEstimated, notifyAll, isInstanceOf, <init>, ==, clone, postShuffleRDD, $init$, toString, logError, !=, getClass, logWarning, ne, eq, log, ##, finalize, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/EnsureRequirements.scala: Set(asInstanceOf, ExchangeCoordinator, isInstanceOf, <init>, ==, toString, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchangeExec.scala: Set(registerExchange, asInstanceOf, ExchangeCoordinator, isInstanceOf, <init>, ==, postShuffleRDD, toString, eq, hashCode)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/ddl.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, tableIdent, verboseString, semanticHash, wait, stats, replace, copy$default$2, $asInstanceOf, path, numberedTreeString, provider, copy$default$5, printSchema, map, productArity, verboseStringWithSuffix, equals, tableDesc, treeString, schemaString, argString, userSpecifiedSchema, RefreshResource, subqueries, CreateTable, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, query, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, CreateTempViewUsing, otherCopyArgs, missingInput, isInstanceOf, stringArgs, options, isStreaming, doCanonicalize, collectLeaves, references, global, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy$default$3, copy, mode, inputSet, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, RefreshTable, copy$default$6, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkSqlParser.scala: Set(children, tableIdent, path, provider, map, tableDesc, RefreshResource, CreateTable, asInstanceOf, query, expressions, conf, CreateTempViewUsing, isInstanceOf, options, <init>, schema, apply, flatMap, ==, foreach, mode, !=, collect, logWarning, RefreshTable, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(provider, map, tableDesc, CreateTable, asInstanceOf, query, expressions, conf, collectFirst, isInstanceOf, options, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, mode, toString, !=, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala: Set(refresh, tableIdent, path, map, tableDesc, CreateTable, asInstanceOf, conf, options, <init>, schema, apply, ==, copy, ne)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/rules.scala: Set(find, simpleString, children, provider, map, tableDesc, CreateTable, asInstanceOf, query, expressions, transformUp, conf, isInstanceOf, <init>, schema, apply, resolved, ==, transformExpressionsUp, foreach, resolve, mode, toString, !=, collect, output, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala: Set(tableIdent, path, map, tableDesc, CreateTable, asInstanceOf, conf, isInstanceOf, options, <init>, schema, apply, ==, mode, toString, collect, ne)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	commit, SupportsWriteInternalRow, createInternalRowWriterFactory, createWriterFactory, abort.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2.scala: Set(commit, SupportsWriteInternalRow, createInternalRowWriterFactory, createWriterFactory, abort)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/MicroBatchWriter.scala: Set(commit, SupportsWriteInternalRow, createInternalRowWriterFactory, createWriterFactory, abort)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(commit, SupportsWriteInternalRow)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/writer/SupportsWriteInternalRow.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(commit, SupportsWriteInternalRow)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SourceOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SourceOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SourceOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SourceOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SourceOptions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, respectSparkSchema, equals, SourceOptions, asInstanceOf, SKIP_HIVE_METADATA, synchronized, $isInstanceOf, DEFAULT_SKIP_HIVE_METADATA, notifyAll, isInstanceOf, DEFAULT_RESPECT_SPARK_SCHEMA, <init>, ==, clone, toString, RESPECT_SPARK_SCHEMA, !=, getClass, ne, skipHiveMetadata, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java)[0m
[0m[[0mdebug[0m] [0m[naha] Including /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala by /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java, /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	createMicroBatchReader, MicroBatchReadSupport.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/sources/RateStreamSourceV2.scala: Set(MicroBatchReadSupport)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala: Set(createMicroBatchReader, MicroBatchReadSupport)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/sources/v2/MicroBatchReadSupport.java[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala: Set(createMicroBatchReader, MicroBatchReadSupport)[0m
[0m[[0mdebug[0m] [0m[naha] None of the modified names appears in /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousRateStreamSource.scala. This dependency is not being considered for invalidation.[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowExec.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, simpleString, children, verboseString, partitionSpec, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, WindowExec, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, doPrepare, windowExpression, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, isCanonicalizedPlan, metrics, executeTake, logError, !=, innerChildren, collect, getClass, longMetric, logWarning, output, copy$default$1, transformDown, transformAllExpressions, mapExpressions, executeBroadcast, ne, requiredChildOrdering, orderSpec, transform, withNewChildren, statePrefix, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(children, partitionSpec, map, WindowExec, asInstanceOf, expressions, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, ne, orderSpec, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/command/InsertIntoDataSourceDirCommand.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, treeString$default$2, find, simpleString, children, refresh, maxRowsPerPartition, verboseString, semanticHash, wait, stats, copy$default$2, $asInstanceOf, numberedTreeString, provider, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, asInstanceOf, transformExpressions, initializeLogIfNecessary, run, generateTreeString, childrenResolved, synchronized, generateTreeString$default$6, allAttributes, nodeName, InsertIntoDataSourceDirCommand, $isInstanceOf, query, validConstraints, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, resolveChildren, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, isStreaming, doCanonicalize, collectLeaves, references, <init>, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, resolved, ==, producedAttributes, fastEquals, origin, transformExpressionsUp, clone, constraints, sameResult, foreach, p, jsonFields, resolve, $init$, copy$default$3, copy, inputSet, storage, toString, isCanonicalizedPlan, metrics, logError, !=, statsCache, maxRows, innerChildren, collect, invalidateStatsCache, getClass, logWarning, output, copy$default$1, overwrite, transformDown, transformAllExpressions, mapExpressions, ne, transform, withNewChildren, resolveQuoted, statePrefix, eq, productIterator, toJSON, log, ##, containsChild, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala: Set(provider, map, asInstanceOf, InsertIntoDataSourceDirCommand, query, expressions, conf, collectFirst, isInstanceOf, references, <init>, schema, apply, flatMap, resolved, ==, foreach, p, resolve, storage, toString, !=, output, overwrite, ne, transform, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTaskRetryException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTaskRetryException.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTaskRetryException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTaskRetryException.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousTaskRetryException.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, printStackTrace, getLocalizedMessage, wait, $asInstanceOf, equals, fillInStackTrace, initCause, asInstanceOf, synchronized, $isInstanceOf, getCause, ContinuousTaskRetryException, notifyAll, isInstanceOf, getStackTrace, getStackTraceElement, <init>, getMessage, setStackTrace, getSuppressed, ==, getStackTraceDepth, clone, addSuppressed, toString, !=, getClass, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/continuous/ContinuousDataSourceRDDIter.scala: Set(asInstanceOf, ContinuousTaskRetryException, isInstanceOf, <init>, ==, toString, !=, getClass, ne, eq)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextOptions.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextOptions.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextOptions.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	notify, wait, $asInstanceOf, wholeText, TextOptions, equals, asInstanceOf, synchronized, $isInstanceOf, notifyAll, isInstanceOf, compressionCodec, COMPRESSION, <init>, ==, clone, toString, !=, getClass, WHOLETEXT, ne, eq, ##, finalize, hashCode.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/text/TextFileFormat.scala: Set(wholeText, TextOptions, compressionCodec, <init>, !=)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF7.java...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF7.java...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF7.java)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF7.java)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF7.java source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	call, UDF7.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/functions.scala: Set(call, UDF7)[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/UDFRegistration.scala: Set(call, UDF7)[0m
[0m[[0mdebug[0m] [0m[naha] Name hashing optimization doesn't apply to non-Scala dependency: /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/java/org/apache/spark/sql/api/java/UDF7.java[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Invalidating (transitively) by inheritance from /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala...[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated by transitive inheritance dependency: Set(/usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala)[0m
[0m[[0mdebug[0m] [0m[naha] The /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala source file has the following regular definitions changed:[0m
[0m[[0mdebug[0m] [0m[naha] 	newNaturalAscendingOrdering, notify, treeString$default$2, find, produce, parent, CollectLimitExec, simpleString, children, doProduce, verboseString, semanticHash, execute, executeCollectIterator, wait, requiredChildDistribution, copy$default$2, $asInstanceOf, numberedTreeString, doConsume, evaluateVariables, resetMetrics, printSchema, map, productArity, verboseStringWithSuffix, equals, treeString, schemaString, argString, subqueries, executeQuery, asInstanceOf, transformExpressions, initializeLogIfNecessary, doExecute, generateTreeString, prepare, metricTerm, synchronized, generateTreeString$default$6, allAttributes, nodeName, $isInstanceOf, LocalLimitExec, doPrepare, usedInputs, logTrace, asCode, canEqual, expressions, canonicalized, copy$default$4, GlobalLimitExec, outputSet, isTraceEnabled, makeCopy, initializeLogIfNecessary$default$2, transformUp, productPrefix, logName, notifyAll, conf, mapProductIterator, collectFirst, otherCopyArgs, missingInput, isInstanceOf, stringArgs, child, inputRDDs, doCanonicalize, collectLeaves, references, newMutableProjection$default$3, <init>, outputOrdering, generateTreeString$default$5, foreachUp, mapChildren, schema, transformExpressionsDown, prettyJson, apply, flatMap, executeCollect, ==, producedAttributes, fastEquals, sqlContext, origin, transformExpressionsUp, clone, newMutableProjection, newPredicate, sameResult, foreach, sortOrder, p, jsonFields, subexpressionEliminationEnabled, sparkContext, outputPartitioning, $init$, copy$default$3, copy, executeCollectPublic, inputSet, prepareSubqueries, toString, evaluateRequiredVariables, isCanonicalizedPlan, metrics, executeTake, logError, !=, BaseLimitExec, consume, innerChildren, collect, getClass, supportCodegen, longMetric, logWarning, output, copy$default$1, needStopCheck, needCopyResult, transformDown, transformAllExpressions, mapExpressions, projectList, executeBroadcast, ne, requiredChildOrdering, limit, transform, withNewChildren, statePrefix, TakeOrderedAndProjectExec, eq, waitForSubqueries, productIterator, toJSON, log, doExecuteBroadcast, consume$default$3, executeToIterator, ##, containsChild, newOrdering, finalize, productElement, hashCode, logDebug, logInfo.[0m
[0m[[0mdebug[0m] [0m[naha] All member reference dependencies will be considered within this context.[0m
[0m[[0mdebug[0m] [0m[naha] The following modified names cause invalidation of /usr/local/spark-2.3.2-bin-hadoop2.7/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala: Set(CollectLimitExec, children, map, asInstanceOf, LocalLimitExec, expressions, GlobalLimitExec, conf, isInstanceOf, child, <init>, outputOrdering, schema, apply, ==, p, sparkContext, outputPartitioning, copy, toString, output, projectList, ne, limit, TakeOrderedAndProjectExec, eq)[0m
[0m[[0mdebug[0m] [0m[naha] New invalidations:[0m
[0m[[0mdebug[0m] [0m[naha] 	Set()[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Previously invalidated, but (transitively) depend on new invalidations:[0m
[0m[[0mdebug[0m] [0m[naha] 	Set()[0m
[0m[[0mdebug[0m] [0m[naha] All newly invalidated sources after taking into account (previously) recompiled sources:Set()[0m
